{"doc_id": "arxiv:2511.18632", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.18632.pdf", "meta": {"doc_id": "arxiv:2511.18632", "source": "arxiv", "arxiv_id": "2511.18632", "title": "The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion", "authors": ["Jan Benedikt Ruhland", "Doguhan Bahcivan", "Jan-Peter Sowa", "Ali Canbay", "Dominik Heider"], "published": "2025-11-23T22:12:35Z", "updated": "2025-11-23T22:12:35Z", "summary": "Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.   In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.   Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.18632v1", "url_pdf": "https://arxiv.org/pdf/2511.18632.pdf", "meta_path": "data/raw/arxiv/meta/2511.18632.json", "sha256": "1193f48010e1a2ba029e81326051ae3474b0d51b8ff80f5d9566139bacdefe5b", "status": "ok", "fetched_at": "2026-02-18T02:26:26.337878+00:00"}, "pages": [{"page": 1, "text": " \n1 \nThe Locally Deployable Virtual Doctor: LLM \nBased Human Interface for Automated \nAnamnesis and Database Conversion \n \nJan Benedikt Ruhland1, Doguhan Bahcivan1, Jan-Peter Sowa2, Ali Canbay2, Dominik Heider3,4,* \n \n1: Heinrich Heine University Düsseldorf, Faculty of Mathematics and Natural Sciences, Düsseldorf, \nGermany \n2: University Hospital Knappschaftskrankenhaus Bochum, Department of Internal Medicine, \nBochum, Germany \n3: Philipps University of Marburg, Department of Mathematics and Computer Science, Marburg, \nGermany \n4: University of Münster, Institute of Medical Informatics, Münster, Germany \n \n*corresponding author: dominik.heider@uni-muenster.de \nAbstract \nRecent advances in large language models made it possible to achieve high conversational performance \nwith substantially reduced computational demands, enabling practical on-site deployment in clinical \nenvironments. Such progress allows for local integration of AI systems that uphold strict data protection \nand patient privacy requirements, yet their secure implementation in medicine necessitates careful \nconsideration of ethical, regulatory, and technical constraints. \nIn this study, we introduce MedChat, a locally deployable virtual physician framework that integrates \nan LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. \nThe chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, \nwhile model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database \ninterface was implemented to ensure complete separation between patient data and the model’s \ninference process. The avatar component was realized through a conditional diffusion model operating \nin latent space, trained on researcher video datasets and synchronized with mel-frequency audio features \nfor realistic speech and facial animation. \nUnlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally \ndeployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks \nexhibited smooth convergence (final L1 = 0.037, MSE = 0.016), and MedChat achieved stable fine-\ntuning with strong generalization to unseen data. The proposed system thus provides a privacy-\npreserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings. \nKeywords: Machine Learning, AI in medicine, LLM, Diffusion Model, Autoencoder \n \n"}, {"page": 2, "text": " \n2 \n1. Introduction \nArtificial intelligence (AI) has reached a pivotal moment in its development, leading to transformative \nchanges across nearly all sectors of human society [1-4]. The rapid evolution of AI technologies \npresents both significant opportunities and substantial challenges, particularly with respect to \nadaptation, regulation, and ethical implementation. Central to this advancement is the emergence and \ncontinuous improvement of Large Language Models (LLMs), which are not only driving innovation in \nnatural language processing and human-computer interaction but also in other fields [2, 5]. \nOne prominent example is ChatGPT [6], an easily accessible and highly versatile LLM that has \ndemonstrated utility across a wide range of applications. These include grammar correction, content \nrefinement, application development, and even educational support. Despite their flexibility and \nimpressive capabilities, such models possess significant risks. They may produce incorrect or \nmisleading outputs, especially when their responses are accepted without critical scrutiny [7]. \nNevertheless, the societal potential of LLMs remains vast, particularly when appropriately integrated \ninto domain-specific workflows. \nIn the field of medicine, AI occupies a uniquely sensitive and impactful position. The application of \nmachine learning to medical data requires rigorous considerations due to the inherently private, \ncomplex, and ethically sensitive nature of healthcare information [8, 9]. Ensuring data privacy, \nmaintaining transparency, and adhering to ethical research practices are essential prerequisites that \ncomplicate the implementation of AI in clinical settings. However, despite these challenges, AI has \nalready demonstrated its capacity to augment human expertise. For instance, convolutional neural \nnetworks have been reported to show superior performance in certain diagnostic tasks, particularly in \nmedical imaging and pattern recognition [10, 11]. \nOne area where AI has the potential to further improve healthcare delivery is in anamnesis, the medical \nhistory-taking process. Effective anamnesis is complicated by the multivariate nature of symptoms, \ndisease-specific manifestations, and the potential for asymptomatic progression, as seen in conditions \nsuch as COVID-19 [12]. Traditional approaches often rely on standardized questionnaires, which may \noverlook critical individual nuances. In contrast, LLMs offer the flexibility to adapt questions \ndynamically based on patient responses, allowing for a more personalized and comprehensive data \ncollection process. \nBy integrating LLMs into anamnesis workflows, clinicians could receive more structured, relevant, and \nindividualized information, potentially leading to earlier and more accurate diagnoses. Much like the \nrole of CNNs in medical image tasks, LLMs could serve as decision-support tools that assist healthcare \nprofessionals, reduce the risk of diagnostic errors, and contribute to the early identification of severe \ndiseases. Thus, with appropriate safeguards, AI holds promise as a transformative aid in the practice of \nmodern medicine. \nAs demonstrated in a recent study by Tu et al., LLMs have begun to outperform medical professionals \nin real-world clinical settings [13]. Their model, the Articulate Medical Intelligence Explorer (AMIE), \nwas evaluated in a controlled study where trained actors simulated patient interactions, allowing for \ndirect comparison between AMIE and practicing clinicians during the anamnesis process. The objective \nof the study was to assess the viability of deploying a domain-specific LLM for clinical use, particularly \nin patient history-taking. The results revealed that AMIE, even when used by non-experts, outperformed \nlicensed medical professionals across multiple diagnostic dimensions. This underscores the \n"}, {"page": 3, "text": " \n3 \ntransformative potential of LLMs in supporting or even enhancing the clinical decision-making process, \nespecially in initial diagnostic stages such as anamnesis. \nOur study aimed to develop a locally deployable LLM, named MedChat, optimized for use in real-\nworld clinical environments with constrained hardware capabilities. A key objective was to ensure \nsecure integration with patient data systems. To that end, MedChat was designed to access a patient's \nmedical history only through a pre-processing interface: the relevant data is extracted and securely fed \nto the model prior to the interaction, without granting the LLM direct access to the underlying database. \nThis approach balances the benefits of personalized context with stringent privacy and security \nstandards. In addition, safeguards were implemented to mitigate risks associated with misuse or \nunauthorized manipulation of the chatbot, addressing a common concern in deploying AI systems in \nsensitive domains such as healthcare. \nBeyond the development of the LLM itself, we also designed a human-machine interface to enhance \nthe interaction quality and user engagement. To achieve this, video recordings of two researchers were \nused to train a diffusion model operating on latent space transformations, capable of generating \nsynchronized speech and facial animations based on textual input. This enabled the creation of a virtual \nphysician avatar, effectively simulating a human-like presence without requiring real-time human \nsupervision. The resulting system offers a fully automated, responsive, and lifelike clinical interaction \nenvironment, potentially serving as a scalable solution for digital anamnesis. \nThe main contributions of this work are summarized as follows: \n1. We present MedChat, a locally deployable large language model tailored for clinical anamnesis, \ncapable of operating fully offline on consumer-grade hardware. \n2. We integrate a diffusion-based virtual physician avatar for naturalistic, multimodal interaction \ncombining speech and facial animation. \n3. We employ parameter-efficient fine-tuning (LoRA) to achieve medical adaptation while \nmaintaining the general-purpose capabilities of the base model. \n4. We propose a secure database interface that enables structured, privacy-preserving integration \nof patient information without granting direct database access to the language model. \n5. We demonstrate the complete end-to-end training and evaluation of all system components, \nlaying the groundwork for future clinical validation. \nResearch in Context \nThe integration of LLMs into healthcare has advanced rapidly, encompassing diverse applications \nranging from diagnostic assistance and clinical documentation support to systems that approach partial \nor near-full autonomy in tasks traditionally performed by physicians, such as patient consultation and \ntreatment planning [13, 14]. Current implementations include clinical decision support systems [14], \nsymptom-based triage agents [15], and automated virtual health advisors [16]. More experimental \nresearch has focused on developing fully autonomous “Virtual Doctor” systems capable of interacting \ndirectly with patients; however, these remain largely at the conceptual or prototype stage due to \npersistent ethical, regulatory, and safety challenges [17]. \n"}, {"page": 4, "text": " \n4 \nA major limitation in the deployment of LLMs and their embodied counterparts lies in their substantial \ncomputational requirements. High-performance models demand extensive memory resources and GPU \nprocessing capacity [18], often making cloud-based deployment necessary. While such infrastructure \nalleviates the need for on-premise hardware, it introduces significant concerns related to privacy, data \nprotection, and compliance with medical data governance frameworks, particularly when sensitive \ninformation such as patient records, imaging data, or personal health histories is processed. \nMost existing LLM-based healthcare systems rely on purely text-based chatbot interfaces that lack \nvisual embodiment or avatar representation [13-16]. Although these models provide robust functional \ncapabilities, their absence of visual and emotional expressiveness can reduce patient engagement and \ntrust, elements that are fundamental in clinical communication and adherence. To address this \nlimitation, recent research has investigated the integration of avatars and virtual agents capable of visual \nand nonverbal interaction [19, 20]. \nUnlike avatars developed through game-engine-based real-time rendering or pre-recorded video \nplayback, AI-driven avatar systems are typically implemented using generative approaches such as \nGenerative Adversarial Networks [21] or diffusion-based models [22]. These architectures enable the \nsynthesis of realistic facial expressions, gestures, and lip-synchronized speech, offering the potential \nfor more natural and immersive interactions. However, achieving real-time or near-real-time \nresponsiveness remains a considerable technical challenge. Full-scale diffusion models exhibit high \nlatency, making them unsuitable for dynamic, interactive clinical environments. Consequently, \nresearchers have explored lightweight alternatives, including optimized GANs and parameter-efficient \ndiffusion models, often referred to as “fast” or “turbo” diffusion variants, that aim to balance visual \nfidelity with computational efficiency [23, 24]. \nDespite these advancements, avatar-based or visually embodied LLM systems continue to face \nsubstantial computational barriers. These architectures require not only sufficient GPU memory to host \nthe LLM and associated generative modules, often exceeding 12-16 GB for high-resolution outputs, but \nalso high-throughput tensor processing to meet the latency constraints of real-time interaction. \nAccordingly, the hardware and performance demands of such systems remain a significant obstacle to \ntheir large-scale deployment and integration within clinical workflows. \nIn contrast to prior work, our approach uniquely combines local deployability, multimodal avatar-based \ncommunication, and strict data isolation mechanisms. Whereas systems such as AMIE or MedAssist \nrely on cloud infrastructure with extensive computational demands, MedChat demonstrates that \ncomparable performance can be achieved under resource constraints while maintaining full data \nsovereignty and GDPR (General Data Protection Regulation) compliance. Table 1 provides a \ncomparison of our model MedChat and other existing systems. \nTable 1: Comparison of existing LLM-based medical systems. \nSystem \nDeployment Avatar \nInterface \nPrivacy Model \nClinical Readiness \nAMIE [13] \nCloud \nNo \nLimited \nExperimental \nMedAssist [14] \nCloud \nNo \nPartial \nPrototype \nHealsHealthAI [16] \nCloud \nNo \nPartial \nResearch \nMedChat  \nLocal \nYes \nFull Isolation \nPreclinical Evaluation \nPlanned \n \n"}, {"page": 5, "text": " \n5 \n2. Materials and Methods \n2.1 Project Overview \nThe primary objective of this project was to design and implement a locally deployable virtual physician \nsystem, referred to as MedChat, capable of operating entirely offline within clinical environments. The \nsystem is specifically developed to function without reliance on external servers or internet \nconnectivity, thereby ensuring compliance with strict data protection regulations and maintaining the \nhighest possible standards of patient privacy and information security. \nThe MedChat system is conceptualized as an interactive virtual physician embedded within a controlled \nphysical environment. In a typical clinical workflow, the patient enters a private, designated \nconsultation room equipped with a high-resolution display, camera, and computing interface. Within \nthis space, the virtual physician is projected on the screen, enabling a naturalistic and confidential \ninteraction. Prior to initiating the consultation, the patient is registered by clinical staff, and relevant \nportions of the patient’s medical history are securely preloaded into the system’s local database. This \nallows the virtual physician to access contextually relevant information during the anamnesis process \nwhile preserving data locality and avoiding external data transmission. \nDuring the consultation, MedChat autonomously conducts a structured anamnesis, guiding the patient \nthrough a series of context-aware questions designed to capture essential clinical information. The \ncollected data are then processed, organized, and securely stored in a local database using encrypted \nstorage protocols. Following each session, the system automatically generates a structured summary \nreport that synthesizes the key findings of the interaction. This report can subsequently be accessed and \nreviewed by licensed healthcare professionals, providing an efficient and standardized foundation for \nfurther clinical assessment and decision-making. \nFigure 1 presents a schematic overview of the anamnesis workflow facilitated by the MedChat system. \nBy partially automating the patient intake procedure, MedChat contributes to the standardization of \nanamnesis across healthcare facilities, enhances the consistency of data collection, and optimizes the \nutilization of patient waiting times. Moreover, by reducing the administrative burden on medical \npersonnel, the system allows healthcare providers to allocate more time to direct patient care and \ncomplex diagnostic reasoning. In this way, MedChat serves as both a technological innovation and a \nworkflow enhancement tool, bridging artificial intelligence capabilities with the practical requirements \nof clinical practice. \n \n"}, {"page": 6, "text": " \n6 \n \nFigure 1: Overview of the anamnesis process facilitated by the MedChat system, illustrating its \nintegration within standard clinical workflows. \n2.2 The Virtual Doctor Avatar \n2.2.1. Network Design and Preprocessing \nWe developed a comprehensive preprocessing pipeline to extract both audio and video frames from \nrecorded materials. The audio signals were resampled to 22,050 Hz to enhance the fine-tuning \nperformance of the text-to-speech (TTS) model. Prior to transformation, the recordings were \ndownsampled to 16 kHz to ensure compatibility with the Wav2Vec model [25], which was employed \nfor automated transcription. Pretrained Wav2Vec models from [26] were utilized to generate bilingual \ntranscriptions in English and German. Subsequently, the audio data were converted into mel-frequency \nspectrograms using a window size of 1,024 and a hop length of 128, extracting 80 mel-frequency \ncomponents to capture perceptually salient acoustic features. \nThe speech synthesis component was implemented using the Piper TTS framework [27], which follows \nan adversarial variational design inspired by the HiFi-GAN [28] and VITS [29] architectures. The \ntraining objective combines several complementary loss terms: (i) an adversarial loss applied to a least-\nsquares discriminator-generator pair, (ii) a feature (perceptual) loss that minimizes the L1 distance \nbetween intermediate feature maps of real and generated audio, thereby enhancing perceptual fidelity \nin the frequency domain, and (iii) a Kullback-Leibler (KL) divergence term that regularizes the latent \nvariable distribution. Together, these losses encourage both realistic waveform synthesis and consistent \nlatent space structure, yielding speech output that closely resembles the target speaker’s tone and \nprosody. For this purpose, we adapted the pretrained Thorsten Voice model [30] for German and the \nAmy model [31] for English, training both on voice recordings provided by two researchers. The \nresulting customized voices ensured natural and consistent speech output in both languages, closely \nresembling the speaker’s tone and prosody. \n"}, {"page": 7, "text": " \n7 \nTo efficiently encode visual information, we developed a custom autoencoder that compresses image \ndata into a compact latent-space representation, thereby reducing the computational cost of subsequent \nvideo generation. The model architecture was inspired by the Stable Diffusion framework [32] and \ntrained on facial image datasets supplied by the two researchers with video recordings around 30 \nminutes yielding over 70,000 images each with corresponding audio signals. For initial experiments, \nwe additionally employed the “Visual Historical Record of American High School Yearbooks” dataset \n[33] to assess model generalization. The latent space was constrained to the range [-1, 1] using a \nhyperbolic tangent activation function to enhance numerical stability. Although this constraint may \ndistort direct decoding, it improves robustness to noise and supports high-quality image reconstruction. \nTo mitigate potential distortion, an adaptive normalization layer [34] was integrated into the decoder to \nmap the constrained latent space back into an unconstrained representation. Furthermore, noise \naugmentation was introduced into the latent space to simulate imperfections typical of diffusion \nprocesses, promoting resilience during image generation. The autoencoder was designed to operate \nindependently of input resolution, maintaining proportional compression relative to the image \ndimensions. Details of the model architecture and experimental results supporting these design choices \nare provided in appendix A. \nFor the video generation stage, a conditional diffusion model was implemented, conditioned jointly on \nthe input image and the mel-frequency representation of the corresponding audio frame. This \nmultimodal conditioning enables synchronized lip movement and facial dynamics consistent with the \nspoken content. The diffusion model adopts a U-Net backbone architecture augmented with cross-\nattention layers, conditional embeddings, and multi-head self-attention mechanisms, ensuring both \ntemporal coherence across sequential frames and semantic consistency between visual and auditory \nmodalities. \nDuring inference, the model diverges from conventional diffusion frameworks that typically initiate the \ngeneration process from random Gaussian noise. Instead, it begins from a pre-diffused default image, \nwhich serves as a structured prior, substantially reducing the number of denoising steps required for \nimage reconstruction. This modification results in a significant decrease in computational cost and \ninference latency while maintaining high visual fidelity in the synthesized video output. \nFurthermore, the system was evaluated for its ability to generate multiple subsequent frames, from the \nsame pre-diffused input. This approach enables parallel frame generation, potentially bridging the \nperformance gap between diffusion-based synthesis and real-time video rendering. The resulting \narchitecture demonstrates that, with appropriate conditioning and initialization strategies, diffusion \nmodels can achieve interactive responsiveness suitable for real-time avatar-based communication \nsystems. \nFigure 2 illustrates the training process of the diffusion model, highlighting the key modifications \nintroduced during inference. During training, the model receives as input the current frame, the mel-\nfrequency representation of the corresponding audio, and the previous frame, in addition to the timestep \nprovided by the cosine noise scheduler. The target frame is progressively diffused, and the network is \ntrained to predict the added noise, enabling it to learn the denoising dynamics. During inference, the \ntrained model generates a sequence of frames, corresponding to one second of video (30 frames), by \nconditioning on a fixed reference image and a weakly diffused version of the same image. The mel-\nfrequency inputs are updated continuously to reflect the evolving audio, allowing the system to produce \ntemporally coherent video frames that are synchronized with the speech signal. This approach ensures \nboth high-fidelity reconstruction and real-time responsiveness in the Virtual Doctor Avatar. \n"}, {"page": 8, "text": " \n8 \n \nFigure 2: Illustration of the diffusion model training and inference pipeline for the Virtual Doctor \nAvatar. During training, the model is conditioned on mel-frequency features, the previous frame, and \nthe current timestep from the cosine noise scheduler. The target frame is progressively diffused and \nused to train the network to predict the added noise. During inference, the trained model generates one \nsecond of video (30 frames) using a consistent conditioning image and a weakly diffused version of \nthat image, while the conditioning mel-frequency inputs vary over time to reflect the changing audio \nfeatures. \n \nTable 2: Overview of model architectures and hyperparameters. \nComponent \nArchitecture \nLoss Function \nEpochs \nLearning Rate \nAutoencoder \nEncoder-Decoder \nCNN  \nL1 \nReconstruction \n30 \n0.00001 \nDiffusion U-Net \nConditional U-\nNet with cross-\nattention \nMSE \n1,000 \n \n0.0001 \nPiper TTS \nVariational \nInference GAN \nAdversarial, L1, \nKL \n135 \n0.00001 \nMedChat (LLM) \nLLaMA 3.1-8B \nwith LoRA \nCross-Entropy \n4 \n \n0.000005 \n \n"}, {"page": 9, "text": " \n9 \n2.2.2. Training Procedure \nHaving established the preprocessing and network design, we next detail the fine-tuning procedures \napplied to each component, focusing on training stability, convergence behavior, and computational \nefficiency. The training process commenced with the fine-tuning of the audio data using the Piper TTS \nmodel. A reduced learning rate of 0.00001 was applied to both the generator and discriminator \ncomponents to promote stable convergence and prevent overfitting. Training was conducted over 135 \nepochs, with model checkpoints evaluated every 50 epochs to assess qualitative and quantitative \nimprovements. After 135 epochs, the synthesized voice demonstrated a high degree of perceptual \nsimilarity to the target speaker. However, additional training beyond this point led to a gradual \ndegradation in voice quality, suggesting the onset of overfitting and confirming the optimal stopping \npoint for fine-tuning. \nFollowing the completion of the speech synthesis stage, training proceeded to the video generation \ncomponent, utilizing the previously developed autoencoder as the foundational model for latent space \ncompression. After verifying the model’s robustness on the public image dataset [33] (see appendix A), \nthe network was fine-tuned on recordings of the target researcher for 30 epochs. Training employed the \nAdam optimizer with an initial learning rate of 0.00001 and an L1 reconstruction loss as the objective \nfunction. A batch size of 32 was used, with all input images standardized to a resolution of 512×512 \npixels. The autoencoder’s latent space was configured for a compression factor of 32. \nFor the diffusion-based generation stage, a cosine noise scheduling strategy [35] was employed to \nensure smooth and stable convergence across 600 diffusion time steps. The model was trained for 1,000 \nepochs using the Adam optimizer with an initial learning rate of 0.0001, modulated by a cosine \nannealing schedule to facilitate adaptive step-size decay. The mean squared error (MSE) between the \npredicted and actual noise served as the training objective, consistent with standard Denoising Diffusion \nProbabilistic Model formulations [32].  \n2.3 MedChat Model Development \n2.3.1 Datasets and Conversation Synthesis \nTo construct the primary dataset for large language model fine-tuning, we integrated three publicly \navailable datasets from the Kaggle machine learning platform [36, 37, 38]. These datasets collectively \ncomprised 10,080 samples, each containing a set of symptoms associated with a specific disease. \nTo generate realistic medical dialogues from these structured data samples, we employed a teacher-\nstudent knowledge distillation approach. Specifically, we fine-tuned the Meta-Llama-3.1-70B-Instruct \nmodel [39] to serve as a teacher model capable of generating high-quality, medically relevant \nconversations. The generated dialogues were then used to train a smaller, more deployment-friendly \nmodel, Meta-Llama-3.1-8B-Instruct, which forms the basis of our MedChat system. \nWe leveraged the capabilities of the larger, pretrained teacher model to reduce training time and benefit \nfrom its existing contextual understanding. However, due to the substantial hardware requirements \n(approximately 140 GB of VRAM in 16-bit precision) the 70B model is impractical for deployment in \ntypical medical facility settings. Hence, our final MedChat implementation is based on the 8B version, \nwhich is more feasible for real-world applications. \n"}, {"page": 10, "text": " \n10 \nTo ensure the generated dialogues were clinically meaningful and structurally consistent, we provided \nthe teacher model with detailed generation guidelines. These guidelines, described in the appendix B, \nincluded instructions on conversational tone, question structure, and appropriate responses to patient \ninput. The model was allowed to generate up to 1,500 tokens per dialogue, enabling the creation of rich \nand informative medical conversations. \nBy repeating the generation process for every entry in the symptom-disease dataset, we created a large \nsynthetic corpus of anamnesis dialogues. This synthetic dataset served as the foundation for fine-tuning \nour MedChat model, allowing it to effectively simulate medical interviews despite the absence of real \npatient interaction data. \nFollowing the generation of 10,080 medical dialogues, we applied preprocessing steps to prepare the \ndialogues for model training, including formatting the conversations into structured JSON files. One \ncommon issue observed in the generated text was the excessive use of filler or acknowledgment phrases, \ne.g., “I see.” or “Noted”. To improve quality and coherence, we filtered out chatbot responses that did \nnot contain a question and hence contained redundant information. \nFor training, 10% of the dataset was randomly withheld to serve as a validation set, enabling the \nassessment of overfitting and evaluation of model performance on an independent subset.  \n2.3.2 MedChat Training \nWe selected the Meta-Llama-3.1-8B-Instruct model [39] as the base model for fine-tuning, due to the \nimpracticality of training a large language model from scratch, which would require extensive \ncomputational resources. Released in July 2024, this model is among the first robust large language \nmodels to combine a 128,000-token context window with multilingual capabilities, all within a \nmanageable size of 8 billion parameters. Its relatively compact architecture allows it to be deployed and \nfine-tuned on consumer-grade or mid-range data center GPUs. \nFine-tuning was performed on a cluster of six NVIDIA A100 GPUs, each equipped with 40 GB of \nVRAM. We employed model parallelism by distributing instances of the model across all GPUs, \nsplitting the dataset accordingly, and synchronizing gradients across devices during training. This setup \nenabled efficient parallel training and supported rapid experimentation. The dataset was divided into a \n90:10 train-test split, with model evaluation conducted at the end of each epoch. \nWe used the AdamW optimizer [40] with a weight decay of 0.01 and an initial learning rate of 0.000005, \ncoupled with a linear learning rate decay schedule. Each training batch consisted of 4 samples, and \ngradient accumulation was performed every 16 steps before synchronization across GPUs. Each epoch \nincluded 420 steps per GPU, and training was conducted for 4 epochs, resulting in approximately 1,680 \ntraining steps per device. In aggregate, this yielded 10,080 steps across all devices, corresponding to \nfull coverage of our dataset. \nTo further enhance training efficiency and preserve the model’s broad capabilities, we adopted a \nparameter-efficient fine-tuning approach using Low-Rank Adaptation (LoRA) [41]. Since the model is \nrequired to retain general-purpose functionalities, such as open-domain conversation, classification, \nsummarization, and multilingual support, we aimed to avoid catastrophic forgetting. In line with the \nrecommendations from [41], we limited fine-tuning to lightweight adapter modules inserted into the \nKey and Value projection matrices of the transformer layers. This selective update approach provides \n"}, {"page": 11, "text": " \n11 \na favorable trade-off between computational cost and fine-tuning effectiveness while maintaining the \nintegrity of the pretrained model's diverse skill set. \n2.3.3 Database Integration \nTo manage and store data collected during interactions, we integrated an SQLite database into the \nMedChat system using SQLAlchemy, a Python-based Object-Relational Mapping framework. This \narchitecture supports efficient and structured data handling, enabling seamless communication between \nthe application and the underlying database layer. \n \nFollowing the conclusion of each anamnesis session, MedChat produces a structured summary and \nclassification of the dialogue in JSON format. This output is subsequently parsed, converted into \napplication-level data objects, and persisted in the SQLite database. These operations (parsing, \ntransformation, and data storage) are implemented outside the model itself and are managed by a \ndedicated service class, which functions as an intermediary between the model’s output and the \ndatabase. Although modern language models support direct database access via tool calls [39], we \nexplicitly exclude this functionality to mitigate potential security risks and maintain strict control over \ndata access. \n \nTo further safeguard the system against adversarial inputs, such as prompt injection and jailbreak attacks \n[42], we integrated Prompt Guard [43] into the application. Prompt Guard is a security-focused \nclassification model that evaluates incoming prompts and categorizes them as benign, prompt injection, \nor jailbreak attempts. By embedding Prompt Guard into the input validation pipeline, we establish a \nproactive security layer that filters malicious content before it reaches the language model. This \napproach enhances the safety, reliability, and integrity of MedChat, particularly in the context of \nhealthcare, where the consequences of model misuse can be significant. \n3. Results \n3.1 Training of the Autoencoder and Diffusion Model \nThe fine-tuning of the autoencoders, designed with a constrained latent space and augmented with \nlatent-space noise, demonstrated stable and consistent convergence throughout the training process. The \nautoencoder was personalized using recordings from a single researcher, enabling the network to adapt \nto specific facial and vocal characteristics. Figure 3 presents the L1 reconstruction loss over the course \nof 30 training epochs. At the beginning of training, the L1 loss started at approximately 0.382 and \nrapidly decreased during the first 10 epochs, reaching a value of 0.051. The subsequent epochs exhibited \na slower, steady decline, ultimately stabilizing around 0.037 by the final epoch. This smooth \nconvergence indicates that the network successfully learned to encode and reconstruct input images \nwhile maintaining the constraints imposed on the latent space. The latent-space noise augmentation \nfurther contributed to this stability by preventing overfitting and promoting robustness to perturbations \nin the latent representation. Notably, the short fine-tuning duration of 30 epochs was sufficient to \nachieve a low reconstruction error, demonstrating the efficiency of the proposed architecture and \ntraining regimen. \n \nThe U-Net model employed for diffusion-based video generation also exhibited clear convergence over \nits training regime. Figure 4 illustrates the MSE value between the predicted and applied noise across \n"}, {"page": 12, "text": " \n12 \n1,000 epochs. The training curve begins at an initial MSE of approximately 0.774 and shows a \nconsistent downward trend, reaching around 0.016 by the final epochs. While minor fluctuations of up \nto ±0.002 are observed throughout training, these are attributable to the stochastic nature of the diffusion \nprocess, where noise is randomly sampled at each timestep, and do not indicate instability. The \nconsistent overall decrease in MSE demonstrates that the network effectively learned to denoise latent \nrepresentations while conditioning on both image and audio features. These fluctuations also suggest \nthat the model maintains adaptability to varying noise patterns, which is beneficial for generalization \nduring inference. \n \nFigure 3: L1 reconstruction loss of the autoencoder during fine-tuning on recordings from a single \nresearcher. The network was trained for 30 epochs with latent-space noise augmentation and a \nconstrained latent representation. The curve demonstrates smooth and stable convergence, indicating \neffective learning of the input image reconstruction while maintaining latent-space regularity. \n \nFigure 4: MSE value of the U-Net diffusion model during training over 1,000 epochs. The model \npredicts the noise added to latent representations conditioned on image and audio features using a cosine \n"}, {"page": 13, "text": " \n13 \nnoise schedule. Minor fluctuations are observed due to the stochastic nature of the diffusion process, \nwhile the overall downward trend demonstrates stable convergence. \n3.2 Dialogue Generation and MedChat Fine-tuning \nFigure 5 presents the fine-tuning dynamics of the MedChat model. At the onset of training, both the \ntraining and evaluation losses are relatively high, reflecting the model’s initial lack of adaptation to the \nspecific task and substantial deviation from the desired output distributions. During the first 500 training \nsteps, a rapid decrease in both losses is observed, indicating that the model quickly begins to capture \nrelevant patterns from the training data. Beyond this point, the training loss continues to decrease \ngradually, while the evaluation loss stabilizes, demonstrating that the model maintains consistent \nperformance on unseen data. The close alignment between training and evaluation loss throughout the \nremainder of the fine-tuning process suggests minimal overfitting and highlights the model’s ability to \ngeneralize effectively. Overall, these results indicate that the fine-tuning procedure successfully adapts \nMedChat to the target task while preserving robustness to novel inputs. \n \n \n \nFigure 5: Training loss curves for the MedChat fine-tuning process. The model exhibits a rapid initial \ndecrease in loss, followed by convergence, indicating effective learning and stable generalization to \nunseen data. \n \nThe simulated interaction described in Appendix C involves an initial consultation with a patient who \nhas no prior history recorded in the MedChat system and for whom no external information from other \nmodels or sensor data is available. In this simulation, the patient is modeled as presenting symptoms \nconsistent with pneumonia. The scenario is designed to evaluate the ability of MedChat to \nautonomously conduct a structured anamnesis, elicit relevant clinical information, and generate an \ninitial patient summary entirely based on the interaction within the system. This setup provides a \ncontrolled environment for assessing the model’s performance in handling cases without any pre-\nexisting contextual knowledge or external guidance. \n \n"}, {"page": 14, "text": " \n14 \nFrom a deployment perspective, the full MedChat-Avatar system can operate on consumer-grade GPUs \nwith as little as 40 GB of VRAM for end-to-end interaction, or on 24 GB of VRAM and only CPUs \nwhen the audio-only mode is used. This configuration substantially reduces hardware costs and energy \nconsumption compared to cloud-based LLM solutions, which typically require 80-100 GB VRAM or \ndistributed clusters for inference. \nTable 3 provides a short summary of the models developed in the research as well as their training \nbehavior.  \n \nTable 3: Quantitative summary of training performance across components. \nModel \nInitial Loss \nFinal Loss \nEpochs \nConvergence Behavior \nAutoencoder \n0.382 \n0.037 \n30 \nSmooth convergence \nDiffusion Model \n0.774 \n0.016 \n1,000 \nStable with minor stochastic \nfluctuations \nMedChat \n2.45 \n0.23 \n4 \nStable convergence without \noverfitting \n \n4. Discussion \nThe developed medical chatbot, MedChat, is designed as a clinical decision support tool rather than a \nstand-alone diagnostic system. All interactions require supervision by qualified medical personnel, and \nno direct diagnostic or therapeutic functions are performed autonomously. Despite these constraints, \nthe system demonstrates robust performance under realistic hardware conditions, validating the \nfeasibility of offline deployment. \nFrom a deployment perspective, MedChat is computationally accessible, as the base model can be run \non consumer-grade GPUs equipped with at least 24 GB of VRAM, making it feasible for integration \ninto local healthcare infrastructure without high-end computational resources. In addition, the Virtual \nDoctor avatar interface can run on 16 GB of VRAM or a reduced version where only audio is available \ncan run without a graphic card and is even manageable by raspberry Pi 4b.  \nThe current implementation of the avatar operates under a Markovian assumption, wherein each \ngenerated frame is conditioned solely on the preceding frame and the most recent audio segment. While \nthis design choice significantly reduces computational complexity and enables efficient real-time \ngeneration, it also introduces limitations in temporal coherence. Specifically, the resulting motion \nbetween consecutive frames may appear subtle or slightly unnatural due to the absence of long-term \ntemporal dependencies. Future improvements could involve extending the conditioning context through \na 3D convolutional U-Net architecture [44], enabling the model to process and generate entire video \nsequences with awareness of multiple preceding and future frames. Such an approach would likely \nenhance temporal smoothness and realism in facial and gestural movements and thereby overcoming \nthe uncanny valley. However, these benefits would come at the expense of increased computational \nrequirements during both training and inference. Thus, the Markovian framework currently offers a \npragmatic trade-off between performance efficiency and visual naturalness, serving as a foundation for \nsubsequent advancements in temporally coherent avatar synthesis. \nIn terms of performance evaluation, the Virtual Doctor avatar, comprising the audio and image \ngeneration components, was qualitatively assessed using L1 loss to ensure fidelity to the expected latent \nspace representations. However, such quantitative evaluation could not be directly applied to the \n"}, {"page": 15, "text": " \n15 \nMedChat dialogue model, due to the inherent subjectivity and open-ended nature of conversational \ninteraction. Instead, the performance of MedChat will be systematically assessed through an upcoming \nstudy at the University Hospital Bochum, where its real-world applicability and usability will be \nevaluated under medical supervision. \nPreliminary results, included in the appendix C, illustrate natural and coherent dialogue between the \navatar and users, suggesting promising potential for interactive anamnesis. Nonetheless, several \nlimitations must be acknowledged. \nFirst, the training data for MedChat includes synthetically generated dialogues and relies heavily on the \npretrained knowledge of the base LLaMA model [39], which may embed inherent biases. These biases \ncould lead to unexpected behaviors or misinterpretations during the anamnesis process, particularly in \ncomplex or ambiguous clinical cases. It should always be used with care and medical supervision.  \nSecond, the model’s ability to generalize to rare symptoms or conditions is limited by the coverage and \ndiversity of the synthetic dataset. As a result, MedChat may underperform when faced with less \ncommon medical presentations not adequately represented in the training data. Thus, it has to be \nimproved in the future with corrected anamnesis results provided by physicians.  \nThird, the model’s medical knowledge base is restricted to the information contained within its training \ncorpus. This constraint implies that MedChat may lack awareness of emerging diseases, new clinical \nguidelines, or the latest evidence-based practices. This gap could compromise the accuracy and \ncompleteness of the anamnesis it conducts, highlighting the need for continued fine-tuning using large-\nscale, up-to-date medical corpora curated from verified sources. \nIn summary, MedChat represents a significant advancement in the development of AI-driven clinical \nsupport systems, combining natural language dialogue with a multimodal virtual avatar capable of \ngenerating realistic audio and visual outputs. This integration enhances the user experience by creating \na more engaging and human-like interaction during the anamnesis process. While the current system \nhas limitations related to training data coverage, potential biases, and the need for clinical validation, it \nalready demonstrates the ability to conduct structured, medically relevant conversations in a natural and \nintuitive manner. \nImportantly, MedChat is designed to run on accessible, consumer-grade hardware and follows a \nmodular architecture, making it adaptable for a range of clinical environments. The forthcoming clinical \nevaluation and further fine-tuning on large-scale, domain-specific medical corpora are expected to \nimprove its accuracy, generalizability, and safety. With continued development, MedChat, together \nwith its Virtual Doctor avatar, has the potential to become a valuable tool in supporting healthcare \nprofessionals, streamlining patient interviews, and improving access to consistent, high-quality \npreliminary assessments. This work lays a strong foundation for the next generation of intelligent, \nmultimodal healthcare assistants, capable of complementing human expertise and enriching patient \ncare. \nFuture work will extend the temporal coherence of the avatar via 3D convolutional architectures and \nexpand MedChat’s conversational robustness through reinforcement learning with human feedback. \nThe planned clinical evaluation at the University Hospital Bochum will further quantify usability, \npatient trust, and integration feasibility in real-world workflows. \n"}, {"page": 16, "text": " \n16 \n5. Conclusion \nIn this paper, we present a fully AI-driven anamnesis pipeline designed to autonomously conduct patient \ninterviews and generate concise, structured summaries to assist physicians by highlighting key medical \ninformation. The system, referred to as MedChat, integrates language models optimized for local \nexecution, enabling deployment on consumer-grade graphics hardware. This ensures accessibility and \npracticality for clinical environments with limited computational infrastructure. \nMedChat was developed using a combination of medical datasets and synthetically generated dialogues. \nHowever, it is important to note that the system has not yet undergone formal clinical validation. A \nprospective evaluation is planned at the University Hospital in Bochum, where the system's utility will \nbe assessed from two perspectives: the extent to which it supports medical professionals in their \ndiagnostic workflow, and the degree of acceptance and trust exhibited by patients toward an AI-guided \nanamnesis process. \nFuture development efforts will focus on integrating more advanced LLMs and enhancing the visual \nand interactive quality of the system’s virtual avatar. A key ongoing challenge is maintaining a balance \nbetween system performance and practical feasibility. Many medical institutions operate under \nsignificant budget constraints and lack the resources required to deploy large-scale AI models. As such, \nour design prioritizes computational efficiency to offer a lightweight and cost-effective solution without \ncompromising core functionality. \nTo promote transparency and foster further research in the field, we release both the synthetic training \ndata and the developed neural network architectures as open-source software. This enables \nreproducibility of our results and provides a standardized benchmark for the development of future \nmedical chatbot systems. \nThe system will be integrated into the Virtual Doctor project with testing and medical evaluation \nperformed at the University Hospital Bochum in Germany. The results will be used to further improve \non the framework and increase the benchmark dataset.  \n \nEthics Statement \nAll experiments were performed using synthetic or publicly available datasets. No human or patient \ndata were collected. All procedures comply with institutional and national ethical guidelines, including \nthe EU General Data Protection Regulation (GDPR). \nAuthor Contributions \nD. H. served as the principal investigator and was responsible for overseeing the conceptualization and \nimplementation of all components. J. B. R. developed the virtual avatar pipeline and supervised the \ndevelopment of the MedChat software. D. B. led the conceptualization and core development of the \nMedChat system. J. P. S. and A. C. contributed to the regulatory preparations necessary for future \nclinical testing of MedChat at the University Hospital Bochum, Germany. \nFunding \nNot available. \n \n"}, {"page": 17, "text": " \n17 \nData Availability Statement \nDue to ethical and privacy considerations, the trained autoencoder and diffusion model weights are \nnot openly released but can be made available upon reasonable research request and subject to \nappropriate data use agreements. \n \nReferences \n[1] Jarrahi, Mohammad Hossein. (2018). Artificial Intelligence and the Future of Work: Human-AI \nSymbiosis in Organizational Decision Making. Business Horizons. 61. 10.1016/j.bushor.2018.03.007. \n[2] Sarumi, Oluwafemi & Heider, Dominik. (2024). Large language models and their applications in \nbioinformatics. Computational and Structural Biotechnology Journal. 23. 10.1016/j.csbj.2024.09.031. \n[3] Shi, Yi & Sun, Lin. (2024). How Generative AI Is Transforming Journalism: Development, \nApplication and Ethics. Journalism and Media. 5. 582-594. 10.3390/journalmedia5020039. \n[4] Bengio Y, Hinton G, Yao A, Song D, Abbeel P, Darrell T, Harari YN, Zhang YQ, Xue L, Shalev-\nShwartz S, Hadfield G, Clune J, Maharaj T, Hutter F, Baydin AG, McIlraith S, Gao Q, Acharya A, \nKrueger D, Dragan A, Torr P, Russell S, Kahneman D, Brauner J, Mindermann S. Managing extreme \nAI risks amid rapid progress. Science. 2024 May 24;384(6698):842-845. doi: \n10.1126/science.adn0117. Epub 2024 May 20. PMID: 38768279. \n[5] Alberts IL, Mercolli L, Pyka T, Prenosil G, Shi K, Rominger A, Afshar-Oromieh A. Large \nlanguage models (LLM) and ChatGPT: what will the impact on nuclear medicine be? Eur J Nucl Med \nMol Imaging. 2023 May;50(6):1549-1552. doi: 10.1007/s00259-023-06172-w. Epub 2023 Mar 9. \nPMID: 36892666; PMCID: PMC9995718. \n[6] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving Language \nUnderstanding by Generative Pre-Training. OpenAI Blog.  \n[7] Obradovich N, Khalsa SS, Khan W, Suh J, Perlis RH, Ajilore O, Paulus MP. Opportunities and \nRisks of Large Language Models in Psychiatry. NPP Digit Psychiatry Neurosci. 2024;2(1):8. doi: \n10.1038/s44277-024-00010-z. Epub 2024 May 24. PMID: 39554888; PMCID: PMC11566298. \n[8] Morley J, Machado CCV, Burr C, Cowls J, Joshi I, Taddeo M, Floridi L. The ethics of AI in \nhealth care: A mapping review. Soc Sci Med. 2020 Sep;260:113172. doi: \n10.1016/j.socscimed.2020.113172. Epub 2020 Jul 15. PMID: 32702587. \n[9] Tajabadi, Mohammad & Martin, Roman & Heider, Dominik. (2024). Privacy-Preserving \nDecentralized Learning Methods for Biomedical Applications. Computational and Structural \nBiotechnology Journal. 23. 10.1016/j.csbj.2024.08.024. \n[10] Haenssle HA, Fink C, Schneiderbauer R, Toberer F, Buhl T, Blum A, Kalloo A, Hassen ABH, \nThomas L, Enk A, Uhlmann L; Reader study level-I and level-II Groups; Alt C, Arenbergerova M, \nBakos R, Baltzer A, Bertlich I, Blum A, Bokor-Billmann T, Bowling J, Braghiroli N, Braun R, Buder-\nBakhaya K, Buhl T, Cabo H, Cabrijan L, Cevic N, Classen A, Deltgen D, Fink C, Georgieva I, \nHakim-Meibodi LE, Hanner S, Hartmann F, Hartmann J, Haus G, Hoxha E, Karls R, Koga H, \nKreusch J, Lallas A, Majenka P, Marghoob A, Massone C, Mekokishvili L, Mestel D, Meyer V, \n"}, {"page": 18, "text": " \n18 \nNeuberger A, Nielsen K, Oliviero M, Pampena R, Paoli J, Pawlik E, Rao B, Rendon A, Russo T, \nSadek A, Samhaber K, Schneiderbauer R, Schweizer A, Toberer F, Trennheuser L, Vlahova L, Wald \nA, Winkler J, Wölbing P, Zalaudek I. Man against machine: diagnostic performance of a deep \nlearning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 \ndermatologists. Ann Oncol. 2018 Aug 1;29(8):1836-1842. doi: 10.1093/annonc/mdy166. PMID: \n29846502. \n[11] Choi YJ, Park MJ, Ko Y, Soh MS, Kim HM, Kim CH, Lee E, Kim J. Artificial intelligence \nversus physicians on interpretation of printed ECG images: Diagnostic performance of ST-elevation \nmyocardial infarction on electrocardiography. Int J Cardiol. 2022 Sep 15;363:6-10. doi: \n10.1016/j.ijcard.2022.06.012. Epub 2022 Jun 9. PMID: 35691440. \n[12] Gao Z, Xu Y, Sun C, Wang X, Guo Y, Qiu S, Ma K. A systematic review of asymptomatic \ninfections with COVID-19. J Microbiol Immunol Infect. 2021 Feb;54(1):12-16. doi: \n10.1016/j.jmii.2020.05.001. Epub 2020 May 15. PMID: 32425996; PMCID: PMC7227597. \n[13] Tu, T., Schaekermann, M., Palepu, A. et al. Towards conversational diagnostic artificial \nintelligence. Nature (2025). https://doi.org/10.1038/s41586-025-08866-7 \n[14] Ran Xu, Wenqi Shi, Jonathan Wang, Jasmine Zhou, and Carl Yang. 2025. MedAssist: LLM-\nEmpowered Medical Assistant for Assisting the Scrutinization and Comprehension of Electronic \nHealth Records. In Companion Proceedings of the ACM on Web Conference 2025 (WWW '25). \nAssociation for Computing Machinery, New York, NY, USA, 2931–2934. \nhttps://doi.org/10.1145/3701716.3715186 \n[15] Lu, Meng & Ho, Brandon & Ren, Dennis & Wang, Xuan. (2024). TriageAgent: Towards Better \nMulti-Agents Collaborations for Large Language Model-Based Clinical Triage. 5747-5764. \n10.18653/v1/2024.findings-emnlp.329.  \n[16] J V, Lavan & Sangeetha, Lakshmi. (2025). HealsHealthAI: Unveiling Personalized Healthcare \nInsights with Open Source Fine‐Tuned LLM. 10.1002/9781394249312.ch4.  \n[17] Ruhland JB, Wichmann J, Degtyar D, Martin R, Fehse L, Klau JH, Papenbrock T, Sowa JP, \nCanbay A, Freisleben B, Leyer M, Heider D. The virtual doctor prescribing the future: Diagnostics \nwith interactive clinical decision support. Comput Biol Med. 2025 Sep;196(Pt C):110968. doi: \n10.1016/j.compbiomed.2025.110968. Epub 2025 Aug 20. PMID: 40839937. \n[18] Yingbing Huang, Lily Jiaxin Wan, Hanchen Ye, Manvi Jha, Jinghua Wang, Yuhong Li, Xiaofan \nZhang, and Deming Chen. 2024. Invited: New Solutions on LLM Acceleration, Optimization, and \nApplication. In Proceedings of the 61st ACM/IEEE Design Automation Conference (DAC '24). \nAssociation for Computing Machinery, New York, NY, USA, Article 369, 1–4. \nhttps://doi.org/10.1145/3649329.3663517 \n[19] Winkler A, Kutschar P, Pitzer S, van der Zee-Neuen A, Kerner S, Osterbrink J, Krutter S. Avatar \nand virtual agent-assisted telecare for patients in their homes: A scoping review. J Telemed Telecare. \n2025 Feb;31(2):207-221. doi: 10.1177/1357633X231174484. Epub 2023 Jun 7. PMID: 37287248. \n[20] DeTore, Nicole & Balogun, Oyenike & Eberlin, Elizabeth & Dokholyan, Katherine & Rizzo, \nAlbert & Holt, Daphne. (2024). An artificial intelligence-based virtual human avatar application to \n"}, {"page": 19, "text": " \n19 \nassess the mental health of healthcare professionals: A validation study (Preprint). \n10.2196/preprints.58255.  \n[21] Goodfellow, I. J., “Generative Adversarial Networks”, arXiv:1406.2661, 2014. \ndoi:10.48550/arXiv.1406.2661. \n[22] Ho, J., Jain, A., and Abbeel, P., “Denoising Diffusion Probabilistic Models”, Art. no. \narXiv:2006.11239, 2020. doi:10.48550/arXiv.2006.11239. \n[23] Deutch, G., Gal, R., Garibi, D., Patashnik, O., and Cohen-Or, D., “TurboEdit: Text-Based Image \nEditing Using Few-Step Diffusion Models”, <i>arXiv e-prints</i>, Art. no. arXiv:2408.00735, 2024. \ndoi:10.48550/arXiv.2408.00735. \n[24] Salimans, T. and Ho, J., “Progressive Distillation for Fast Sampling of Diffusion Models”, \n<i>arXiv e-prints</i>, Art. no. arXiv:2202.00512, 2022. doi:10.48550/arXiv.2202.00512. \n[25] Baevski, A., Zhou, H., Mohamed, A., and Auli, M., “wav2vec 2.0: A Framework for Self-\nSupervised Learning of Speech Representations”, <i>arXiv e-prints</i>, Art. no. arXiv:2006.11477, \n2020. doi:10.48550/arXiv.2006.11477. \n[26] J. Grosman, “Fine-tuned XLSR-53 large model for speech recognition in German,” Hugging \nFace, 2021. [Online]. Available: https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-\ngerman \n[27] OHF-Voice, “piper1-gpl: Fast and local neural text-to-speech engine,” Github. [Online]. \nAvailable: https://github.com/OHF-Voice/piper1-gpl \n[28] Kong, J., Kim, J., and Bae, J., “HiFi-GAN: Generative Adversarial Networks for Efficient and \nHigh Fidelity Speech Synthesis”, arXiv:2010.05646, 2020. doi:10.48550/arXiv.2010.05646. \n[29] Kim, J., Kong, J., and Son, J., “Conditional Variational Autoencoder with Adversarial Learning \nfor End-to-End Text-to-Speech”, arXiv:2106.06103, 2021. doi:10.48550/arXiv.2106.06103. \n[30] M. Hansen, “Thorsten medium checkpoint,” Rhasspy, 2023. [Dataset]. Available: \nhttps://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main/de/de_DE/thorsten/medium \n[31] M. Hansen, “Amy medium checkpoint,” Rhasspy, 2023. [Dataset]. Available: \nhttps://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main/en/en_US/amy/medium \n[32] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B., “High-Resolution Image \nSynthesis with Latent Diffusion Models”, <i>arXiv e-prints</i>, Art. no. arXiv:2112.10752, 2021. \ndoi:10.48550/arXiv.2112.10752. \n[33] Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Crystal Lee, Philipp Krähenbühl, Alexei A. \nEfros. A Century of Portraits: A Visual Historical Record of American High School Yearbooks, \nExtreme Imaging Workshop, International Conference on Computer Vision, ICCV 2015. and IEEE \nTransactions on Computational Imaging, September 2017. \n"}, {"page": 20, "text": " \n20 \n[34] Jan Benedikt Ruhland, Iraj Masoudian, Dominik Heider, Enhancing deep neural network training \nthrough learnable adaptive normalization, Knowledge-Based Systems, Volume 326, 2025, 113968, \nISSN 0950-7051, https://doi.org/10.1016/j.knosys.2025.113968. \n[35] Nichol, A. and Dhariwal, P., “Improved Denoising Diffusion Probabilistic Models”, <i>arXiv e-\nprints</i>, Art. no. arXiv:2102.09672, 2021. doi:10.48550/arXiv.2102.09672. \n[36] Disease prediction based on symptoms. en. url: \nhttps://www.kaggle.com/datasets/pasindueranga/disease-prediction-based-on-symptoms (retrieved on \n28.10.2024). \n[37] Disease Prediction Using Machine Learning. url: \nhttps://www.kaggle.com/datasets/kaushil268/disease-prediction-using-machine-\nlearning?resource=download (retrieved on 28. 10. 2024). \n[38] Disease and Symptoms dataset. en. url: \nhttps://www.kaggle.com/datasets/choongqianzheng/disease-and-symptoms-dataset (retrieved on 28. \n10. 2024). \n[39] Abhimanyu Dubey u. a. The Llama 3 Herd of Models. arXiv:2407.21783. Aug. 2024. doi: \n10.48550/arXiv.2407.21783. url: http://arxiv.org/abs/2407.21783 \n[40] Ilya Loshchilov und Frank Hutter. Decoupled Weight Decay Regularization. arXiv:1711.05101. \nJan. 2019. doi: 10.48550/arXiv .1711.05101. url: http://arxiv.org/abs/1711.05101 \n[41] Edward J. Hu u. a. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685. \nOkt. 2021. doi: 10.48550/arXiv.2106.09685. url: http://arxiv.org/abs/2106.09685 \n[42] Yi Liu u. a. Prompt Injection attack against LLM-integrated Applications. arXiv:2306.05499. \nMarch 2024. doi: 10 . 48550 / arXiv . 2306 . 05499. url: http : //arxiv.org/abs/2306.05499 \n[43] meta-llama/Prompt-Guard-86M · Hugging Face. Sep. 2024. url: https://huggingface.co/meta-\nllama/Prompt-Guard-86M (visited on 29. 10. 2024). \n[44] Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J., “Video Diffusion \nModels”, arXiv:2204.03458, 2022. doi:10.48550/arXiv.2204.03458. \n \n \n \n \n \n \n \n"}, {"page": 21, "text": " \n21 \nAppendix A. Ablation Experiments \nOur initial experiments were conducted using the selected dataset to design an autoencoder with a stable \nand well-constrained latent space, thereby improving training robustness during the subsequent \ndiffusion modeling stage. To assess the impact of different latent space regularization strategies, we \ncompared three autoencoder configurations based on the design described in table A1. \n \nTable A1: Overview of the autoencoder architecture developed for this study. The model was \nimplemented and tested across three configurations to investigate the influence of latent space \nregularization. In configuration 1, the tanh activation function at the encoder output was omitted. \nConfiguration 2 integrated the reparameterization trick [32] to enforce a probabilistic latent distribution. \nConfiguration 3, described in detail in this table, constitutes the final configuration adopted for all \nsubsequent diffusion model experiments. \nModule \nLayer / \nOperation \nConfiguration \nOutput \nResolution \nNotes \nEncoder Adaptive \nNormalization \nAdaptiveNorm((1, \nin_channels, 1, 1)) \nN×N \nInput normalization \n \nConvolution \nConv2d(in_channels, 32, 3, \n1, 1) \nN×N \nInitial feature \nextraction \nResBlock \nResBlock(32, 32) \nN×N \nResidual feature \nrefinement \nDownsampling \nConv2d(32, 32, 4, 2, 1) \nN/2×N/2 \nFirst spatial \ndownsampling \nResBlock \nResBlock(32, 64) \nN/2×N/2 \nFeature expansion \nDownsampling \nConv2d(64, 64, 4, 2, 1) \nN/4×N/4 \nSecond spatial \ndownsampling \n \nResBlock \nResBlock(64, 128) \nN/4×N4 \nDeep feature \nencoding \n"}, {"page": 22, "text": " \n22 \n \nDownsampling \nConv2d(128, 128, 4, 2, 1) \nN/8×N/8 \nLatent compression \nResBlock \nResBlock(128, 128) \nN/8×N/8 \nHigh-level feature \nfusion \nProjection \nConv2d(128, \nlatent_channels, 3, 1, 1) \nN/8×N/8 \nLatent embedding \nActivation \nTanh() \nN/8×N/8 \nConstrains latent \nspace to [-1, 1] \nDecoder Adaptive \nNormalization \nAdaptiveNorm((1, \nlatent_channels, 1, 1)) \nN/8×N/8 \nNormalization of \nlatent input \n \nLinear Projection Conv2d(latent_channels, \n128, 1) \nN/8×N/8 \nExpands latent space \ninto feature domain \n \nResBlock \nResBlock(128, 128) \nN/8×N/8 \nResidual learning in \nlatent domain \nUpsampling \nConvTranspose2d(128, 64, \n4, 2, 1) \nN/4×N/4 \nFirst spatial \nupsampling \nResBlock \nResBlock(64, 64) \nN/4×N/4 \nFeature refinement \nUpsampling \nConvTranspose2d(64, 32, 4, \n2, 1) \nN/2×N/2 \nSecond spatial \nupsampling \n \nResBlock \nResBlock(32, 32) \nN/2×N/2 \nFeature refinement \n"}, {"page": 23, "text": " \n23 \n \nUpsampling \nConvTranspose2d(32, 16, 4, \n2, 1) \nN×N \nFinal spatial \nupsampling \nResBlock \nResBlock(16, 16) \nN×N \nFinal feature \nsmoothing \nOutput \nConvolution \nConv2d(16, out_channels, 3, \n1, 1) \nN×N \nImage reconstruction \nActivation \nTanh() \nN×N \nConstrains pixel \nvalues to [-1, 1] \nOur first approach was to establish a base line with an unconstrained latent space, i.e. where the latent \nvariables were allowed to take arbitrary continuous values. The second approach was a variationally \nconstrained latent space, where a normal prior distribution N(0,I) was enforced through the \nreparameterization trick and an additional Kullback-Leibler divergence loss term as described in [32]. \nThe last approach contained a deterministically constrained latent space, where a hyperbolic tangent \nactivation was applied to project latent variables into the range [-1,1]. An adaptive normalization layer \n[34] was incorporated in the decoder to re-establish consistent statistics during reconstruction. \nThe reparameterization trick introduces stochasticity into the latent representation while maintaining \ndifferentiability, enabling efficient gradient-based optimization. It can be expressed as: \n(1)                                                             z=μ+σ×ϵ, ϵ∼N(0,I) \nwhere μ and σ represent the mean and standard deviation predicted by the encoder, and ϵ denotes a \nnoise sample drawn from a standard normal distribution. To regularize the latent space toward a \nstandard normal prior, we employed the Kullback-Leibler divergence loss, defined as: \n(2)                                                 LKL = -0.5 ∑di=1(1+log(σi2)-μi2-σi2) \nwhere the sum is over the full dimensionality d of the latent space. \nThe reconstruction objective for all configurations was the L1 loss between the input and \nreconstructed image: \n(3)                                                                 Lrec=∥x-x∥11 \nThe autoencoder models were trained for 30 epochs using images of 256×256 resolution. To enhance \nthe generalization capability of the network, a set of geometric data augmentation techniques was \napplied, including random rotations, translations, scalings, and shear transformations. All \ntransformations employed zero padding to preserve image dimensions. The training objective combined \n"}, {"page": 24, "text": " \n24 \nan L1 reconstruction loss, as defined in equation (3), with a Kullback-Leibler divergence term serving \nas a regularization component for configuration 2. The regularization coefficient was varied between \n0.000001and 0.01 to assess its impact on latent space stability. Optimization was performed using the \nAdamW algorithm [38], which provides adaptive moment estimation with decoupled weight decay to \nimprove convergence behavior. \nFigure A6 presents the evolution of the L1 reconstruction loss across all three autoencoder \nconfigurations. The relatively weaker reconstruction performance observed in configuration 2 at a \nregularization weight of 0.01 can be attributed to the increased stochasticity introduced by the KL \ndivergence, which induces variability in the decoding process. When the regularization coefficient was \nreduced to 0.000001, the reconstruction quality improved. However, the latent space exhibited poor \nconstraint, with activation magnitudes exceeding an order of magnitude of 10. Balancing the KL and \nL1 loss terms proved challenging, as excessive regularization reduced reconstruction fidelity, whereas \ninsufficient regularization led to latent instability. Consequently, configuration 3 was selected as the \nfinal model, as it achieved a well-defined latent representation suitable for subsequent latent space noise \naugmentation, thereby enhancing the robustness of the downstream diffusion model. \nFigure A7 provides a qualitative comparison between the original input images and their corresponding \nreconstructions for each autoencoder configuration, illustrating the inherent trade-off between \nreconstruction fidelity and latent space regularity. The results for the strongly KL-regularized \nautoencoder are omitted, as training exhibited strong KL divergence dominance. Consequently, \nalthough the latent space distribution closely approximated a normal prior, the reconstruction quality \ndegraded substantially. This behavior, consistent with the quantitative results shown in figure A6, \nindicates that excessive KL regularization overly constrains the latent representation, thereby impairing \nthe network’s ability to accurately reconstruct fine-grained image details. \nFigure A6: Training curves for the different autoencoder configurations. For configuration 2, which \nincorporates a KL divergence term, multiple regularization strengths were evaluated to assess the \neffect of relaxing the normal prior assumption. The curve labeled strong KL constraint corresponds to \n"}, {"page": 25, "text": " \n25 \na regularization coefficient of 0.01, reflecting a strong enforcement of the prior, whereas the weak KL \nconstraint (orange curve) employs a coefficient of 0.000001, allowing greater flexibility in the latent \nspace representation. \n \nFigure A7: Examples of the original image and the reconstructed images using the autoencoder \nconfigurations. \nTo enhance robustness against residual noise introduced during the diffusion process, we augmented \nthe latent space with Gaussian noise sampled from a normal distribution with a mean of 0 and a \nrandomly varying variance. This strategy aimed to improve the model’s generalization capability under \nstochastic perturbations during generation. The architecture of the subsequent U-Net model employed \nfor diffusion-based synthesis is presented in table A2. \nTable A2: Overview of the U-Net architecture employed in this study. The network incorporates \nadaptive normalization, cross-attention mechanisms, and time-conditioning via sinusoidal positional \nembeddings to enable temporally coherent generation. A Conditioning Encoder was introduced to \nintegrate multimodal context from both image and mel-spectrogram inputs, enhancing semantic \nconsistency during diffusion-based video synthesis. The detailed layer configuration, including \ndownsampling, upsampling, and residual attention blocks, is summarized in the table. \nModule \nLayer / \nOperation \nConfiguration / \nParameters \nNotes \n"}, {"page": 26, "text": " \n26 \nInputs \nInput tensor  \nImage tensor \nLatent image tensor from \nAutoencoder \nTimesteps \nInteger tensor  \nTime  \nContext inputs \ncontext_img, \ncontext_mel \nConditioning set \nInput \nProcessing \nAdaptive \nnormalization \nAdaptiveNorm((1, in_ch, \n1, 1)) \nNormalization \nTime embedding \nTime Embedding with \nsinusoidal position \nembedding \nProduces timestep-conditioned \nvector t \nTime \nnormalization \nAdaptiveNorm((1, \ntime_dim)) \nNormalization \n \nContext encoder \nContext Embedding \nProduces concatenated \nconditioning tensor \nInput convolution \nConv2d(in_ch, base_ch, \n3, padding=1) \nProjects x into base feature \nspace \nDownsampling \nRepeated blocks  \nResidual blocks with \ncross attention \nResidual block with \nconditioning and optional \ncross-attention \n"}, {"page": 27, "text": " \n27 \n \nDownsample \nConvolutional \ndownsampling \nSpatial downsampling between \nstages  \nBottleneck  \nResBlockWithCA \nResidual blocks with \ncross attention \nBottleneck residual + attention \nUpsampling  \nRepeated blocks \nResidual blocks with \ncross attention \nFor each stage: upsample, \nconcat skip connection, \nresidual block with \nconditioning & optional \nattention \nOutput \nprocessing \nOutput \nconvolution \nConv2d(ch, in_ch, 3, \npadding=1) \nProjects back to in_ch channels \n(e.g., model predicts denoised \nlatent) \n \nAppendix B. MedChat Guidlines \nTo constrain dialogue generation and minimize irrelevant or off-topic outputs, we provided a set of \nstructured guidelines to the teacher model. These guidelines were designed to ensure that the \ngenerated dialogues remained clinically relevant, coherent, and aligned with typical medical interview \npractices. The guidelines included the following principles: \n1. This is a reverse QA Session. You will ask questions and the patient will respond. \n \n2. Ask one question at a time, ensuring each inquiry is directly relevant to the patient’s \ncondition. \n \n3. Each question should only be one sentence. \n \n4. Respond directly without filler phrases. \n \n5. Do not provide medical advice, recommendations, or commentary on the patient’s responses. \n \n6. Questions should be concise and focused. Maintain a neutral tone. \n \n7. Continue asking questions until the patient has revealed all symptoms. \n"}, {"page": 28, "text": " \n28 \n \n8. Follow up on every symptom with at least one probing question to get more details (e.g., \nduration, intensity, aggravating factors). \n \n9. Use precise medical terminology while ensuring clarity for the patient. \n \n10. Structure your inquiries to cover: Symptoms, Diagnosis, Treatment, Tests/Procedures, and \nMedication. \n \n11. Do not stray from the conversation structure or prompt. \n \n12. Do not output any code, technical explanations, or commands. \n \n13. The conversation should be structured and focused on gathering information to assist in \ndiagnosing the Patient’s condition. \n \n14. Ask at least 10 questions to the patient. \n \n15. Do not summarize the Patient’s responses or provide a conclusion at the end of the \nconversation. \n \n16. End the conversation by asking if the patient has any further information to provide. \n \n17. If the Patient has nothing to add, end the conversation by outputting the token |<EOA>|. \nWe additionally defined a set of guidelines for MedChat to generate structured summaries in JSON \nformat. The output produced by MedChat is organized into five primary categories, each containing \nrelevant items and an overarching summary. The corresponding guidelines for each category are as \nfollows: \n1. Symptoms: \n• Items: List any symptoms or complaints mentioned. \n• Summary: Provide a brief summary of the patient’s symptoms. \n2. Diagnosis: \n• Items: Identify any diagnoses provided. \n• Summary: Summarize the previously given diagnosis which the patient has mentioned. \n3. Treatment: \n• Items: Record any treatment methods which were previously prescribed like antibiotics or    \nover-the-counter medications. \n• Summary: Summarize the treatment the patient has received or is trying. \n4. Test/Procedure: \n"}, {"page": 29, "text": " \n29 \n• Items: Record any tests or procedures mentioned. \n• Summary: Summarize the purpose of the tests or procedures and their outcomes. \n5. Medication: \n• Items: List any prescribed or discussed medications. \n• Summary: Summarize the medications and their intended use. \nAppendix C. Example Conversation \nMedchat: Good morning, I’m Medchat, your AI healthcare expert. I’ll do my best to help you today. \nDo you have any new complaints or symptoms that you’d like to discuss? \n \nPatient: Yes, for several days I’ve had a high fever of around 39 °C. \n \nMedchat: Have you experienced any chills or sweating along with this fever? \n \nPatient: Yes, I’m sweating a lot and feeling fatigued. \n \nMedchat: Have you noticed any changes in your appetite or any nausea or vomiting? \n \nPatient: No. \n \nMedchat: Have you had any recent travel, exposure to someone who’s been sick, or contact with \nanything that might have caused an infection? \n \nPatient: I don’t think so. I do have a history of pneumonia and asthma. \n \nMedchat: Have you noticed any shortness of breath or difficulty breathing, or are your asthma \nsymptoms otherwise manageable? \n \nPatient: Yes, I have shortness of breath and chest pain. I tried breathing exercises and had a chest X-\nray. \n \nMedchat: Did the chest X-ray show any abnormalities, and do you feel pain or tightness in your chest \nthat worsens with deep breathing or coughing? \n \nPatient: The X-ray was normal, but I do feel chest pain when coughing. I tried taking ibuprofen 600 \nmg, but it didn’t help. \n \nMedchat: Have you coughed up any mucus or blood, or experienced any pain or tenderness in your \nback or sides? \n \nPatient: No mucus when coughing. \n \n"}, {"page": 30, "text": " \n30 \nMedchat: Have you noticed any pain or pressure in your ears or head, or any recent changes in your \nhearing or vision? \n \nPatient: No. \n \nMedchat: Do you have any other information about your symptoms or medical history? \n \nPatient: No, I don’t. \n \n \n"}]}