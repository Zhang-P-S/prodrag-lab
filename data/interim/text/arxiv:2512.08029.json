{"doc_id": "arxiv:2512.08029", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.08029.pdf", "meta": {"doc_id": "arxiv:2512.08029", "source": "arxiv", "arxiv_id": "2512.08029", "title": "CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space", "authors": ["Tianxingjian Ding", "Yuanhao Zou", "Chen Chen", "Mubarak Shah", "Yu Tian"], "published": "2025-12-08T20:42:10Z", "updated": "2025-12-08T20:42:10Z", "summary": "Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\\%, and significantly surpasses all other medical-specific large language models.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.08029v1", "url_pdf": "https://arxiv.org/pdf/2512.08029.pdf", "meta_path": "data/raw/arxiv/meta/2512.08029.json", "sha256": "e24da2184ae861bff37a336848052f56571175ed79e60dc7475d26d1e846839c", "status": "ok", "fetched_at": "2026-02-18T02:24:40.809650+00:00"}, "pages": [{"page": 1, "text": "CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling\nContext-Aware Disease Trajectories in Latent Space\nTianxingjian Ding, Yuanhao Zou, Chen Chen, Mubarak Shah, Yu Tian\nInstitute of Artificial Intelligence, University of Central Florida\nhttps://dingtianxingjian.github.io/clarity-project-page/\nAbstract\nClinical decision-making in oncology requires predicting\ndynamic disease evolution, a task current static AI predic-\ntors cannot perform. While world models (WMs) offer a\nparadigm for generative prediction, existing medical ap-\nplications remain limited. Existing methods often rely on\nstochastic diffusion models, focusing on visual reconstruc-\ntion rather than causal, physiological transitions.\nFur-\nthermore, in medical domain, models like MeWM typically\nignore patient-specific temporal and clinical contexts and\nlack a feedback mechanism to link predictions to treatment\ndecisions. To address these gaps, we introduce CLARITY,\na medical world model that forecasts disease evolution di-\nrectly within a structured latent space. It explicitly inte-\ngrates time intervals (temporal context) and patient-specific\ndata (clinical context) to model treatment-conditioned pro-\ngression as a smooth, interpretable trajectory, and thus\ngenerate physiologically faithful, individualized treatment\nplans. Finally, CLARITY introduces a novel prediction-to-\ndecision framework, translating latent rollouts into trans-\nparent, actionable recommendations.\nCLARITY demon-\nstrates state-of-the-art performance in treatment planning.\nOn the MU-Glioma-Post dataset, our approach outperforms\nrecent MeWM by 12%, and significantly surpasses all other\nmedical-specific large language models. Our code and pre-\ntrained models will be made publicly available once ac-\ncepted.\n1. Introduction\nClinical decision-making in oncology requires predicting a\npatient’s disease trajectory, a process defined by high un-\ncertainty.\nWhile AI excels at static outcome prediction\n[10, 11, 19, 25, 26, 28, 34–36, 40, 43, 44], these models\ncritically fail to model how a patient’s disease will dynami-\ncally evolve over time in response to a specific treatment.\nFoundation models like LLMs, despite impressive gen-\neralization [1, 20, 38], lack the specialized clinical rea-\nFigure 1. Conceptual Overview of CLARITY. Moving beyond\nstatic prediction, CLARITY’s latent-based Actor simulates multi-\nple “what-if” disease trajectories (Future Latent Prediction) con-\nditioned on rich Clinical and Temporal Contexts. This is not a\none-way process: Inverse feedback (orange arrow) enables the it-\nerative refinement of Action Proposals, translating predictions into\na concrete, optimized treatment plan for the clinician.\nsoning needed for these dynamic tasks.\nThis gap high-\nlights the need for interpretable predictions. World Mod-\nels (WMs) offer a promising paradigm, using latent-space\nmodeling to “imagine” and plan for future consequences\n[3, 4, 7, 17]. However, despite success in robotics, their\nadoption in medicine remains limited due to the unique\nchallenges of clinical data, temporal reasoning, and inter-\npretability.\nAmong the few existing attempts, frameworks like [3, 4,\n7, 17] often rely on diffusion-based architectures to gener-\nate future states. While diffusion models excel at synthe-\nsis, they are fundamentally misaligned with world model\nobjectives in medical domain.\nSpecifically, their inher-\nent stochastic sampling disrupts temporal consistency and\ncausal reasoning, and they emphasize appearance recon-\nstruction over learning physiologically meaningful transi-\ntions (Limitation 1). These limitations are particularly prob-\nlematic for clinical decision-making, where minor stochas-\n1\narXiv:2512.08029v1  [cs.LG]  8 Dec 2025\n"}, {"page": 2, "text": "ticity can propagate into significant uncertainty.\nWhile world models exist for the natural domain, their\nmedical application remains largely underexplored.\nRe-\ncent work like MeWM [42] introduced a framework using\ndiffusion-based generative models to visually reconstruct\ndisease evolution. However, this approach overlooks criti-\ncal temporal and clinical variability (Limitation 2). It mod-\nels treatment effects as static transitions, ignoring how out-\ncomes depend on follow-up duration (temporal variability)\nor a patient’s unique genomic and treatment history (clini-\ncal variability). Lacking this crucial temporal encoding and\nclinical conditioning, such models cannot produce realistic,\npersonalized predictions.\nFurthermore, although MeWM employs an inverse dy-\nnamics model for evaluation, it suffers from a critical dis-\nconnect between modeling and decision-making [42]. Clin-\nical practice is an iterative loop where physicians adjust\ntreatments based on patient response and anticipated out-\ncomes.\nThe current method lacks this crucial feedback\nmechanism; it cannot feed predicted results back into a\nreasoning process to iteratively refine treatment strategies\n(Limitation 3). This static, one-way process is fundamen-\ntally misaligned with clinical reality, severely limiting its\nvalue as a genuine decision-support tool.\nTo address these challenges, we introduce CLARITY, a\nmedical world model designed to link prediction directly to\ndecision-making. Fig. 1 shows the conceptual overview of\nCLARITY. The model operates by forecasting disease evo-\nlution within a latent space, conditioned on patient-specific\nclinical and temporal contexts.\nIt then employs a well-\ndesigned inverse survival evaluation to achieve a complete\npredictions-to-decision process.\nMotivated by clinical practice, CLARITY models the\npre- to post-treatment transformation as a smooth, inter-\npretable trajectory within a structured latent space. Rather\nthan attempting to reconstruct clinical images (e.g., CT,\nMRIs) via stochastic denoising, this latent-space approach\ncaptures underlying physiological dynamics. This formula-\ntion avoids pixel-level artifacts while preserving anatomical\ncoherence. Diffusion-based reconstruction makes clinically\nmeaningful changes (e.g., tumor shrinkage) directly visible.\nAnother key innovation in CLARITY is its explicit mod-\neling of temporal and clinical contexts. By encoding time\nintervals as continuous embeddings, the model differenti-\nates short-term responses from long-term evolution, pro-\nducing longitudinally consistent predictions aligned with\nphysiological timescales. This temporal modeling allows\nCLARITY to generate continuous risk trajectories for dif-\nferent drug combinations.\nMeanwhile, CLARITY inte-\ngrates clinical personalization (e.g., genomic alterations,\ndemographics, therapeutic history) into a unified represen-\ntation, allowing it to model how disease trajectories diverge\nacross patients with distinct biological profiles. This dual\nfocus enables CLARITY to move beyond static, population-\nlevel averages and anticipate how an individual’s tumor will\nevolve under therapy.\nMoreover, to bridge the critical gap between prediction\nand decision-making, CLARITY introduces a novel In-\nverse Survival Evaluation. This process creates an adap-\ntive feedback loop by feeding predicted outcomes back to\nthe Therapy Policies Agent, enabling it to iteratively re-\nassess and optimize treatment strategies. This prediction-\nto-decision framework translates generative forecasts into\ntransparent, actionable recommendations, directly emulat-\ning a physician’s adaptive reasoning. Overall, our contribu-\ntions are threefold:\n• Latent-space disease forecasting. We model progres-\nsion as smooth, interpretable trajectories within a struc-\ntured latent space, avoiding pixel-level artifacts and pre-\nserving anatomical consistency for trackable forecasts.\n• Temporal and clinical Contexts. We encode continuous\ntime and multimodal patient context (e.g., genomics, de-\nmographics, prior therapies) to generate physiologically\nfaithful and individualized predictions.\n• Prediction-to-Decision Framework.\nWe integrate\nlatent-space rollouts with an Inverse Survival Evaluation,\ntranslating predictions into transparent recommendations\nfor adaptive, patient-tailored decisions.\n2. Related Works\nWorld Models.\nThe development of World Models has\nevolved along distinct paradigms.\nOne branch includes\ngenerative-interactive models like Genie [7], which learn\nto simulate controllable, pixel-level environments from un-\nlabeled video. Another branch features latent-space plan-\nning models like DreamerV3 [17], which learn a compact\nRecurrent State-Space Model (RSSM, [13]). A third, non-\nreconstructive paradigm, exemplified by V-JEPA [6] and\nMuDreamer [8], avoids pixel-level reconstruction entirely\nby predicting representations in latent space. In medicine,\nthese concepts are emerging but often rely on diffusion-\nbased generative models.\nFor example, recent special-\nized frameworks like MeWM [42], function as a genera-\ntive model to synthesize and segment ‘post-treatment tu-\nmors’.\nUnlike these generative approaches, our method\ndiffers in two fundamental ways: (1) Prior medical world-\nmodels, such as diffusion-based MeWM operate in pixel\nspace and synthesize post-treatment images, whereas we\nmodel disease evolution directly through transitions in la-\ntent space without any reconstruction objectives. (2) Exist-\ning approaches predict only one fixed future step, while our\nframework models arbitrary time gaps via time-conditioned\nlatent dynamics.\n2\n"}, {"page": 3, "text": "Survival Analysis.\nTraditional survival analysis relies on\nstatistical models such as Cox regression [12] and Ran-\ndom Survival Forests [22], which provide interpretable haz-\nard estimation but assume proportional hazards and linear\nrelations.\nDeep-learning extensions like DeepSurv [21],\nDeepHit [23], and Deep Survival Machines [31] relax these\nassumptions, learning nonlinear risk mappings from high-\ndimensional clinical data. Recent multimodal frameworks\n[16, 29] further integrate imaging and omics features for\npersonalized prognosis.\nHowever, most existing models\nremain static, predicting fixed survival outcomes without\nmodeling temporal disease dynamics. Dynamic approaches\nsuch as Latent ODE [30] and Dynamic DeepHit [24] in-\ntroduce time continuity, yet lack treatment conditioning\nor long-horizon modeling. Our framework extends these\nparadigms by forecasting patient-specific survival trajecto-\nries through a therapy-aware latent transition model.\n3. Method\n3.1. Overview\nCLARITY is a framework composed of multiple mod-\nules: an MRI Encoder, Therapy Policies (implemented as\nan MLLM Agent), and an Actor (our Diseases Evolution\nModel). These modules interact within an efficient world\nmodel architecture. CLARITY begins with Direct Survival\nEvaluation, as illustrated in Fig. 2 (a), where the Actor\nscores an initial set of therapies proposed by the agent. Se-\nquentially, as shown in Fig. 2 (b), we introduce an Inverse\nSurvival Evaluation process, which feeds these scores back\nto the agent, allowing it to iteratively refine its proposals to\nfind the optimal treatment.\nIn the following sections, we introduce the detailed com-\nponents and processes of CLARITY. Sec. 3.2 introduces the\ndesign and rules of the Therapy Policies Agent. Sec. 3.3\nelaborates on how we define and model the Clinical and\nTemporal Contexts. Sec. 3.4 details the architecture and\ntraining strategy of the Post-Treatment Latent Predictor and\nSurvival Predictor within the Actor module. Sec. 3.5 pro-\nvides further details on the Inverse Survival Evaluation pro-\ncess. Finally, in Sec. 3.6 we discuss the ability of CLARITY\nto predict disease trajectory in arbitrary timeline.\n3.2. Therapy Policies Agent\nIn CLARITY, a multimodal large language model (MLLM)\n(e.g., GPT-4o [20]) serves as the Therapy Policy Agent\nπMLLM.\nConditioned on a high-level goal g (e.g., “min-\nimize the predicted risk score”), the agent takes as input\nthe pre-treatment MRI x0 and clinical context cp, and pro-\nduces structured therapy actions that satisfy medical safety\nconstraints Ω(e.g., avoiding incompatible regimens such as\nco-administration of Bevacizumab and Temozolomide). For-\nmally, the MLLM generates a candidate action set A(0) =\n{a(0)\nj }M0\nj=1, where each a(0)\nj\nrepresents a feasible treatment\nconfiguration.\nEach action a contains various intervention components:\na = {achemo, aradio, abrachy, aimmuno, aadd},\n(1)\ncorresponding respectively to chemotherapy (e.g., Temo-\nzolomide),\nexternal radiotherapy,\nbrachytherapy,\nim-\nmunotherapy, and additional supportive strategies.\nThe\ngeneration process follows guideline-informed prompting\ntemplates to ensure clinical validity and parameter consis-\ntency (drug type, dose, and schedule).\nEach generated therapy description is then encoded by a\npretrained text encoder (e.g., [33]), yielding a dense embed-\nding aligned with the Actor’s latent space:\nhdrug = Pool(Φtext(a)) ∈Rd,\n(2)\nwhere d denotes the embedding dimension of the latent\nspace.\n3.3. Clinical and Temporal Contexts\nAs illustrated in Fig. 2 (a), the Actor receives not only\nthe pre-treatment latent representation and therapy embed-\ndings, but also the Clinical Context and Temporal Con-\ntext. These two components inject individualized and tem-\nporally consistent priors into the latent transition, enabling\nCLARITY to model disease progression in a physiologi-\ncally grounded manner.\nClinical Context.\nFor each patient p, we define a struc-\ntured clinical profile Cp containing key clinical attributes,\nincluding demographic information (e.g., age, sex), molec-\nular biomarkers (e.g., IDH1/2, ATRX, 1p19q co-deletion,\nMGMT methylation index), and treatment-related indica-\ntors. We serialize Cp into a textual prompt and encode it\nusing the medical text encoder [33]. The resulting contex-\ntual embedding is then passed through a lightweight MLP\nto produce the final clinical representation:\nhclin = MLPclin\n\u0000Φtext(Cp)\n\u0001\n∈Rdc,\nwhere dc denotes the dimension of the clinical embedding\nspace. This unified embedding conditions the Actor, en-\nabling patient-specific latent prediction and survival estima-\ntion based on the clinical profile.\nTemporal Context.\nThe temporal component models dis-\nease evolution across arbitrary follow-up intervals. Given\nimaging observations at times tpre and tpost, we encode the\ntime gap ∆t = tpost −tpre into a continuous representation.\nWe encode the time gap using sinusoidal embeddings with\nlog-spaced frequencies:\nγ(∆t) = [sin(ωi∆t), cos(ωi∆t)]dt/2\ni=1 ,\n(3)\n3\n"}, {"page": 4, "text": "Pre-Imaging\nObservation\nActor \n(Diseases Evolution \nModel)\nPre-Treatment\nLatent\nSurvival Analysis Results\nTherapy \nPolicies \nAgent\nTherapy \nPolicies \nAgent\nDrug Combo\nDrug Combo\nMRI Encoder\nTemporal Modeling\n(Sinusoidal position encoding)\nPatient Pathology Context\nPatient Clinical Context\n181 Days\n294 Days\nTemporal\nContext\nClinical Context\nPost-Treatment \nLatent (Prediction)\nPost-Treatment\nLatent\nPredictor\nSurvival \nPredictor\nTextual Encoder\nTextual Encoder\nClinical\nContext\nClinical\nContext\nTemporal\nContext\nTemporal\nContext\nDrug\nCombo\nDrug\nCombo\nConcatenate\nInput\nOutput\nPre-Treatment \nLatent\nActor (Diseases Evolution Model)\nRisk Score \nfor Survival \nAnalysis \nPredict\nHead\nDrug Combos\nDrug Combos\nPatient ID: 0008,\nSex: Male,\nRace: White,\nAge_at_diagnosis_years: 50\nPrimary Diagnosis: GBM\nGenomics: {idh: 0, ...}\nPre-\nImaging\nPost-\nImaging\nCombo 1: \nCombo 2: \nCombo 3: \n...\nOne-by-One\nOne-by-One\nRisk \nScore\nInput\nInput\nRadiation 60gy + TMZ\nTMZ + Avastin\nTMZ + Optune\nFinal Action\nFinal Action\nSurvival Analysis Results\nFrom Direct Survival Evaluation\nSurvival Analysis Results\nFrom Direct Survival Evaluation\nCombo 1\nCombo 2\nCombo 3 \n...\nRisk Score: 0.65\nRisk Score: 0.87\nRisk Score: 0.72\nPre-Imaging\nObservation\nRefined Drug Combos\nRefined Drug Combos\nRefined Combo 4\nRefined Combo 5 \nRefined Combo 6\n...\nPatient Clinical \nContext\nActor \nTemporal Context\nPre-Treatment\nLatent\nCombined Survival \nFeedback\nCombined Survival \nFeedback\nCombo 1\nCombo 2\nCombo 3 \n...\nCombo 4\nCombo 5\nCombo 6 \nCombo 4:\nLowest Risk Score!\nPatient Clinical \nContext\n(b) Inverse Survival \nEvaluation:\nTherapy \nPolicies Agent\nTherapy \nPolicies Agent\nIteration: 1\nStep 1: Propose Therapies\nIteration:K\nEnd\nRadiation 60gy \n+ TMZ\nStep 2: Refine Drugs Combo\nStep 3: Score and Update Feedback\nUpdated Survival \nFeedback\nUpdated Survival \nFeedback\nCombo 4\nCombo 5\nCombo 6 \n0.45\n0.67\n0.79\nStep 4: Combine Feedback\nRisk\nScore\nStep 5: Final Selection\nLooping\nIteration: k\nLooping to Step 1\n(a) Direct Survival Evaluation:\n...\nFigure 2. CLARITY’s Inference Pipeline for (a) Direct Survival Evaluation: A frozen MRI Encoder processes the pre-imaging ob-\nservation to extract a pre-treatment latent representation. In parallel, the Therapy Policies Agent (e.g., GPT4o) takes the patient’s clinical\ncontext to propose multiple candidate drug combos. The Actor module (Diseases Evolution Model) then sequentially evaluates each combo\none-by-one, integrating the pre-treatment latent, clinical context, temporal context, and the specific drug combo to predict a final risk score\nfor survival analysis; (b) CLARITY’s Inverse Survival Evaluation: This diagram illustrates the iterative prediction-to-decision feedback\nloop. Initial risk scores from Direct Survival Evaluation (Fig. 2 (a)) are fed into the Therapy Policies Agent. The Agent then proposes\nupdated drug combos, which the Actor Scores to generate new risk estimates as the accumulated survival feedback. This process repeats,\nrefining the therapy proposals, and after K iterations, the policy with the Lowest Risk Score is selected as the Final Action.\nwhere ωi = 1/100002i/dt, and dt denotes the dimension-\nality of the temporal embedding. This encoding provides\nthe Post-Treatment Latent Predictor with explicit tempo-\nral awareness, enabling it to generate predictions that re-\nflect the expected magnitude of physiological change over\nthe elapsed interval.\nThe sinusoidal encoding represents\nelapsed time between visits, ensuring smooth interpolation\nacross variable intervals.\n3.4. Diseases Evolution Model\nAs shown in Fig. 2 (a), we apply a Diseases Evolution\nModel as the Actor module, which models how a patient’s\ndisease state evolves under therapy and how this evolution\nrelates to survival outcome. It consists of two submodules:\na Post-Treatment Latent Predictor, which forecasts post-\ntreatment latent representations, and a Survival Predictor,\nwhich estimates risk and survival probabilities from latent\nfeatures. The detailed architectures of Post-Treatment La-\ntent Predictor and Survival Predictor are shown in Fig. 3.\nInformation Integration.\nTo generate a comprehensive\npost treatment prediction and conduct an accurate survival\nanalysis, we feed various sources of information into the\nDiseases Evolution Model. Specifically, the model receives\npre-treatment latent zpre, clinical context hclin, temporal\ncontext γ(∆t) and the drug combo hdrug. We concatenate\nthese information input together as integrated representa-\ntion, [zpre, hclin, γ(∆t), hdrug].\nThrough this design, the\nmodel learns to simulate disease evolution trajectories that\nare sensitive to both molecular context and temporal dy-\nnamics, aligning with the deterministic and causal objec-\ntives of a medical world model.\nPost-Treatment Latent Predictor.\nAfter concatenating\nto obtain the integrated representation, we feed it into a\nmulti-layer Transformer to predict the post-treatment latent\nˆzpost and risk score ˆrp. This process is formulated as:\nˆzpost = SelfAttn([zpre, hclin, γ(∆t), hdrug])N + zpre, (4)\nwhere SelfAttn(·)N is a N-layer self-attention Transformer\ndoing differential prediction from the concatenated repre-\nsentation [zpre, hclin, γ(∆t), hdrug].\nSelf-attention enables\nthe model to capture high-order dependencies among pa-\ntient state, clinical background, and therapeutic factors, pro-\nducing a temporally and physiologically consistent post-\ntreatment latent prediction ˆzpost.\n4\n"}, {"page": 5, "text": "Drugs\nAction\nDrugs\nAction\nDrugs\nAction\nDrugs\nAction\nPost-\nTreatment \nLatents\nPost-Treatment\nLatent Predictor\nSurvival Predictor\nTextual Encoder\nTextual Encoder\nClinical\nContext\nClinical\nContext\nTemporal\nContext\nTemporal\nContext\nConcatenate\nInput\nOutput\nPre-Treatment \nLatent\nActor (Training Pipeline)\nSurvival\nProbability\nPredict\nHead\nOutput\nL1\nLoss\nCox\nLoss\nBrier\nLoss\nRisk \nScore \nSurvival Label\n• Survival Time: 201 Days\n• Survival Indicator: 0\nMRI \nEncoder\nPost-Imaging\nObservation\nPost-Treatment \nLatent (Label)\nDrug\nCombo 1\nDrug\nCombo 1\nDrug\nCombos\nContrastive \nLoss\nCombo 1\nPull\nPush\nPush\nCombo 2\nCombo 3\nSimilar\nDissimilar\nInput\nInput\nSelf-Attention\nTransformer Layer\n×N\nTwo-Way\nCross-Attention\nTransformer Layer\n×M\nFigure 3. Training Pipeline of the Actor (Diseases Evolution\nModel). The Post-Treatment Latent Predictor consists of a N-\nlayer self-attention Transformer to forecast post-treatment latents,\ntrained via L1 loss for reconstructing post-treatment latent, and\nContrastive loss for align semantic structure among various la-\ntents. The Survival Predictor is equipped with a M-layer two-\nway cross-attention Transformer to estimate risk score and sur-\nvival probability from pre- and post-treatment latents, supervised\nby Cox and Brier losses.\nThe Post-Treatment Latent Predictor is trained via a\ncombination of latent reconstruction loss and a soft label-\nbased contrastive loss, controlled by a coefficient λ1 as:\nLpred = λ1Llatent + Lcon.\n(5)\nSpecifically, the latent reconstruction loss is computed be-\ntween the predicted and ground-truth latent states as:\nLlatent = ∥ˆzpost −zpost ∥ℓ1,\n(6)\nwhere ℓ1 represents the L1-loss. Additionally, a contrastive\nloss is used to structure the latent space, ensuring that\npredicted states generated from similar treatments remain\nclose. However, treatments are not simply ‘same’ (hard la-\nbel 1) or ‘different’ (hard label 0); they possess nuanced\nsimilarities.\nWe therefore utilize a soft label-based con-\ntrastive loss, where the soft label for any two treatment ac-\ntions ai and aj1 in a batch is defined by the cosine simi-\nlarity of their text embeddings, ui and uj [33]. This tar-\nget similarity distribution (from text embeddings) is then\nused to supervise the latent similarity distribution (from the\npredicted latent ˆzpost,i and ˆzpost,j). The loss minimizes the\nKL divergence between these two distributions, forcing the\nlatent space to adopt the same semantic structure as the\ntreatment-embedding space. This is implemented as a sym-\nmetric cross-entropy loss over all pairs (i, j) in a batch:\nLcon = −\nB\nX\ni\nB\nX\nj\n(pij log(qij) + qij log(pij)) ,\n(7)\n1The i, j and k are indices of their position in a training batch; the same\napplies to any notations in this section.\nwhere B is the batch size, pij and qij are the softmax-\nnormalized similarity scores:\npij =\nexp(sim(ui, uj)/τ1)\nPB\nk̸=i exp(sim(ui, uk)/τ1)\n,\nqij =\nexp(sim(ˆzpost,i, ˆzpost,j)/τ2)\nPB\nk̸=i exp(sim(ˆzpost,i, ˆzpost,k)/τ2)\n,\n(8)\nwhere sim(·, ·) denotes cosine similarity, τ1 and τ2 are the\ntemperature parameters.\nSurvival Predictor.\nTo link disease dynamics with pa-\ntient outcomes, we introduce a Survival Predictor that\njointly processes the pre- and post-treatment latents and pre-\ndicts risk score and survival probability. To enhance the un-\nderstanding of both pre-treatment latent and post-treatment\nlatent, we adopt a Two-Way Cross-Attention mechanism,\nallowing information to flow bidirectionally between zpre\nand ˆzpost. Then the aggregated feature is passed to a MLP-\nbased predction head that outputs a one-year survival prob-\nability ˆp1y and a continuous risk score ˆr. Whole predictor\ncan be formulated as:\n[ˆp1y, ˆr] = MLP\n\u0000CrossAttn(zpre, ˆzpost)M\n+ CrossAttn(ˆzpost, zpre)M\u0001\n,\n(9)\nwhere CrossAttn(·, ·)M indicates a M-layer Transformer\nspecialized for cross attention, and the probability ˆp1y re-\nflects the likelihood of death within one year after the pre-\ndicted post-treatment timepoint, while ˆr provides a relative\nranking signal for survival risk.\nThe survival branch is supervised by a Brier loss [15]\nfor the predicted one-year probability ˆp1y and a Cox partial\nlikelihood loss [12] for the continuous risk score ˆr:\nLsurv = λ2LBrier(ˆp1y, y) + LCox(ˆr, T, δ),\n(10)\nwhere λ2 is a coefficient same as λ1, and (T, δ) denote the\nobserved survival times and event indicators in the label.\nThe Brier loss [15] measures the squared error between the\npredicted one-year survival probability and the binary label:\nLBrier = (ˆp1y −y)2,\n(11)\nwhere y ∈{0, 1} indicates whether the patient survived\nbeyond one year.\nThe Cox partial likelihood [12] is a\nranking-based loss encouraging higher risk scores for pa-\ntients who experience earlier events:\nLCox = −\nX\ni: δi=1\n\nˆri −log\nX\nj: Tj≥Ti\neˆrj\n\n,\n(12)\nwhere Ti is the observed time and δi is the event indica-\ntor. Together, these losses enforce both calibrated survival\nprobability prediction and coherent risk ranking.\n5\n"}, {"page": 6, "text": "This design connects imaging changes with survival rea-\nsoning in one latent space. By modeling temporal and treat-\nment effects on embeddings, CLARITY captures disease\nevolution more stably and clearly.\nTraining Strategy.\nWe jointly train the Post-Treatment\nLatent Predictor and the Survival Predictor in an end-to-end\nfashion. The overall training objective combines the losses\nfrom both Eq. (5) and Eq. (10), ensuring that the latent space\nis optimized for both accurate progression forecasting and\nclinically relevant survival prediction. The final loss func-\ntion L is a sum of the prediction and survival losses:\nL = Lpred + Lsurv.\n(13)\n3.5. Inverse Survival Evaluation\nTo close the loop between prediction and decision-making,\nwe add a training-free inference process that iteratively\nrefines therapy choices using updated survival risk feed-\nback(Fig. 2 (b)). This core prediction-to-decision pipeline\nunfolds over K iterations (indexed by k), which consists of\nthe following five steps.\nStep 1: Propose Therapies based on Feedback.\nGiven\nthe pre-treatment MRI x0, the clinical context cp (Sec. 3.3),\ncombined survival feedback (initially this is survival anal-\nysis results from Direct Survival Evaluation) and the high-\nlevel objective g (Sec. 3.2), the Therapy Policy Agent em-\nploys a textual description of the patient’s condition and\ngoal, and outputs a refined action set A(k), expressed as\na structured text description for the Actor to evaluate.\nStep 2: Refine with Dose and Schedule Variation.\nFor\neach therapy candidate a ∈A(k), we introduce minor, med-\nically grounded variations to its parameters (e.g., drug type,\ndosage level, cycle count, and interval schedule).\nEach\nperturbed version ˜a slightly modifies the original configu-\nration, expanding the exploration space while maintaining\nrealism. If any resulting combination violates the medical\nsafety constraints Ω(e.g. exceeding the maximum cumula-\ntive dose or pairing incompatible drugs), it is directly dis-\ncarded. This rule-based filtering ensures that all retained\ntherapy variants remain compliant with oncology protocols,\nwhile still broadening the search space through medically\ngrounded diversity.\nSteps 3 & 4: Score Survival and Update Feedback.\nAf-\nter refinement guideline-based constraints, each candidate\ntherapy is evaluated by the Actor module one-by-one again,\npredicting its post-treatment latent representation and esti-\nmates the corresponding survival risk score. These results\nof risk score are then be integrated into the current accu-\nmulated survival feedback, yielding ψ(k+1). This updated\ncontext provides survival-grounded evidence for πMLLM to\nrefine subsequent therapy generation in iteration k + 1.\nStep 5: Select Final Therapy.\nAfter K rounds, or early\nstopped by convergence of the best risk, CLARITY selects\nthe final best action a⋆as:\na⋆= arg min\na∈Aall\nˆr(a),\nAall =\nK\n[\nk=1\nA(k),\n(14)\nwhere ˆr(a) represents Eq. (9) the risk score ˆr for drug\ncombo a.\n3.6. Arbitrary Policy-Conditioned Projection\nA key capability of CLARITY is simulating dynamic treat-\nment regimes for counterfactual “what-if” analysis.Future\ntreatment policies are represented as flexible sequences of\nactions ati executed at patient-dependent decision points ti\n, rather than fixed schedules. The model then generates a\nfull latent trajectory ˆz(ti)2 by recursively applying the Ac-\ntor module (Sec. 3.4).\nAt each step ti, given the current latent state ˆz(ti),\nthe unified clinical context hclin, the time-gap embedding\nγ(ti+1 −ti), and the encoded action hdrug\nti\n, the Actor pre-\ndicts the next latent state ˆzti+1 without requiring paired\nimaging. The Survival Predictor then maps ˆz(ti+1) to up-\ndated survival estimates [ˆp(ti+1)\n1y\n, ˆr(ti+1)].\nThis recursive, time-conditioned rollout naturally sup-\nports arbitrary intervals and irregular follow-ups, enabling\nflexible projection under user-defined treatment schedules.\nAdditional qualitative examples are provided in Sec. 4.4.\n4. Experiment\n4.1. Implementations\nFor training and evaluation, we leverage two longitudinal\nbrain tumor cohorts that provide both temporal imaging tra-\njectories and treatment information, which are essential for\nmodeling therapy-conditioned disease evolution in a world-\nmodel framework. The MU-Glioma-Post [32] cohort con-\ntains 203 patients and 654 MRI follow-ups with rich treat-\nment logs and genomic annotations. We split the dataset in\ntraining and validation at the patient level (4:1) to prevent\ntemporal leakage across pairs. For external validation, we\nuse the UCSF-ALPTDG [14] cohort, which includes 298\npatients with longitudinal MRI sequences and correspond-\ning survival outcomes. Additional details are discussed in\nthe Supplementary.\n2ˆz(ti) denotes the predicted post-treatment latent ˆzpost at time step ti,\nand the same notation applies throughout this section.\n6\n"}, {"page": 7, "text": "Table 1. Quantitative comparison on MU-Glioma-Post and UCSF-ALPTDG datasets. Our approach achieves consistently higher\nPrecision, Recall, and F1-score across both benchmarks. The best and second-best results are marked as bold and underline, respectively.\n∗denotes we re-implement their method, training and evaluating on public datasets to conduct a fair comparison.\nMethod\nMU-Glioma-Post [32] (%)\nUCSF-ALPTDG [14] (%)\nPrecision\nRecall\nF1 Score\nPrecision\nRecall\nF1 Score\nGeneral Large Language Models\nGPT-4o [20]\n40.3\n44.0\n42.1\n36.9\n44.9\n40.5\nClaude-4.5-Sonnet [2]\n48.6\n38.0\n41.6\n45.3\n38.6\n41.7\nQwen3-VL [5, 39]\n36.7\n39.4\n38.0\n33.7\n42.9\n35.8\nMedical Knowledge-based Models\nMedGPT [41]\n41.6\n42.1\n41.9\n36.7\n46.3\n40.9\nHuatuo-Vision [9]\n52.3\n46.8\n46.4\n42.1\n51.5\n44.1\nMeWM∗[42]\n45.2\n42.1\n43.6\n39.3\n48.2\n43.3\nOur Approach\n59.7\n52.0\n55.6\n50.5\n47.5\n48.9\nFigure 4. Kaplan–Meier survival curves predicted by MeWM\n(left) and our method (right) on MU-Glioma-Post. Our approach\nproduces a much clearer separation across risk strata, reflected by\na lower log-rank p-value of 0.0017 and a substantially higher C-\nindex of 0.7856. Shaded regions denote 95% confidence intervals.\n4.2. Results on Treatment Exploration\nAs shown in Tab. 1, Our CLARITY framework achieves\nstate-of-the-art results, significantly outperforming all base-\nlines on both datasets. On the MU-Glioma-Post benchmark,\nour approach achieves an F1-score of 55.6%, a substan-\ntial 9.2% absolute improvement over the strongest medical-\nspecific baseline, Huatuo-Vision (46.4%). The trend contin-\nues on UCSF-ALPTDG, where our model achieves 48.9%\nF1, again outperforming the second-best method (44.1%).\nThis highlights a key finding: while general-purpose mod-\nels like GPT-4o and Claude-4.5 perform poorly when\nprompted directly (42.1% and 41.6% F1, respectively), our\nframework successfully leverages the MLLM as part of a\nrobust simulation-to-decision loop, dramatically enhancing\nits effective performance.\nA critical comparison is with MeWM. Direct compari-\nson with the original MeWM [42] was not feasible, as their\nin-house data is not public and their work focuses on CT\ndata, whereas ours targets multi-sequence MRIs. To provide\na fair baseline, we re-implemented the MeWM-style diffu-\nsion predictor (marked as MeWM∗) on our MRI data. As\nshown in the table, our approach (55.6% F1) significantly\noutperforms this MeWM∗baseline (43.6% F1). This re-\nsult validates our design choice to focus on time-aware la-\nFigure 5. Multi-stage decision trajectories generated by our\nmodel. Each stage (e.g., S0) corresponds to an MRI observation.\nDashed lines denote candidate rollouts under different therapy ac-\ntions, while the solid line with an arrow indicates the selected\ntreamtment sequence achieving the lowest predicted risk.\ntent dynamics and survival-aware consistency rather than\ndiffusion-based appearance reconstruction.\n4.3. Survival Analysis\nFigure 4 shows that our model achieves clearer separation\nacross risk strata than the diffusion-based MeWM base-\nline. The survival curves diverge earlier with wider mar-\ngins, yielding a lower log-rank p-value (0.0017 vs. 0.0763)\nand higher C-index (0.7856 vs. 0.7013). We attribute this\nimprovement to avoiding pixel-space diffusion reconstruc-\ntion that tends to introduce stochastic artifacts, while latent-\nspace modeling preserves clinically relevant structure and\ntemporal consistency.\n4.4. Analysis of Disease Decision Trajectory\nFigure 5 visualizes a predicted multi-stage decision trajec-\ntory. Each stage Si corresponds to an MRI observation and\nthe associated latent state. At every stage, the model per-\nforms policy-conditioned state projection, generating multi-\nple candidate branches (dashed lines) corresponding to dif-\n7\n"}, {"page": 8, "text": "Table 2. Ablation analysis of CLARITY’s components on MU-\nGlioma-Post. We compare the latent-based Actor module over\nthe diffusion-based one and demonstrate the cumulative perfor-\nmance gains yielded by integrating clinical and temporal context\nand the inverse survival feedback iteration.\n# Diffusion\n-based\nLatent\n-based Context Feedback\nIteration\nPrec.\n(%)\nRec.\n(%)\nF1\n(%)\n1\n✓\n✗\n✗\n✗\n47.8\n39.8 43.6\n2\n✗\n✓\n✗\n✗\n59.5\n46.5 52.4\n3\n✗\n✓\n✓\n✗\n58.5\n48.1 52.8\n4\n✗\n✓\n✗\n✓(K=3)\n59.3\n49.6 54.0\n5\n✗\n✓\n✓\n✓(K=3)\n59.7\n52.0 55.6\nTable 3. Ablations of CLARITY’s iteration number (# Itera-\ntions) in Inverse Survival Evaluation on MU-Glioma-Post dataset.\n# Iterations\nPrecision\nRecall\nF1-score\n1\n59.7\n48.1\n53.1\n2\n59.3\n48.6\n53.4\n3\n59.7\n52.0\n55.6\n4\n57.2\n53.6\n55.4\nferent treatment actions.\nThe solid lane indicates the selected branch that achieves\nthe lowest predicted risk score, with the corresponding ther-\napy action labeled at each step. Through iterative selection,\nthe model constructs a temporally consistent treatment lane\n(e.g., RT+TMZ →TMZ →CCNU →Avastin+Brachy) that\nminimizes the longitudinal risk trajectory.\n4.5. Ablation Study\nImpact of CLARITY’s Module.\nTab. 2 analyzes the con-\ntribution of each component. (1) Latent vs. Diffusion:\nReplacing diffusion synthesis (# 1) with latent dynamics (#\n2) yields the largest gain (+8.8% in F1), confirming that\ncompact manifolds capture predictive structure better than\nstochastic pixel reconstruction. (2) Clinical Context: In-\ntegrating patient-specific priors (# 3) enhances Recall, indi-\ncating that biological signals guide physiologically faithful\nsimulations. (3) Inverse Evaluation: The feedback loop (#\n4 & 5) drives performance to a peak F1-score of 55.6% (#\n5). This validates that iteratively refining therapies via sur-\nvival feedback is essential for optimized decision-making.\nImpact of Iteration Number.\nTab. 3 investigates the im-\npact of iteration number (K) on the Inverse Survival Eval-\nuation. We observe a steady performance gain as iterations\nincrease, peaking at K=3 with a best F1-score of 55.6%.\nThis trajectory validates that iterative feedback enables our\nmethod to refine proposed therapies based on accumulated\nsurvival signals. However, the slight decline at K=4 in-\ndicates diminishing returns, suggesting that excessive iter-\nations may introduce noise or over-optimization. Conse-\nTable 4.\nAblation of loss combination on MU-Glioma-Post.\nWe observe incremental gains from adding calibration (Brier)\nand structural (Contrastive) objectives, with soft-label supervision\nyielding the optimal C-Index.\n# Loss Combination\nC-index (%)\n1 Llatent + LCox\n76.1\n2 Llatent + LBrier + LCox\n76.8\n3 Llatent + Lcon(Hard Label) + LBrier + LCox\n77.3\n4 Llatent + Lcon(Soft Label) + LBrier + LCox\n78.6\nTable 5. Efficiency comparison of diffusion-based and latent-\nbased methods in CLARITY framework.\nMethod\nFLOPs (T)\nTime (s)\nDiffusion-based(1000 sampling steps)\n61.3\n38.6\nDiffusion-based(500 sampling steps)\n39.5\n19.7\nLatent-based\n4.21\n0.341\nquently, we adopt K=3 as the optimal trade-off.\nImpact of Loss Combination.\nTab. 4 proves that richer\nsupervision enhances predictive capacity. Adding the Brier\nscore (# 2) improves over the baseline (# 1) by enforcing\nprobability calibration (+0.7%). Incorporating contrastive\nlearning (# 3) further sharpens latent transitions.\nCru-\ncially, the soft-label variant (# 4) yields the highest C-index\n(78.6%), confirming that capturing nuanced treatment sim-\nilarities stabilizes latent dynamics better than hard labels.\nEfficiency Comparison.\nTab. 5 highlights the critical\ncomputational advantage of our latent-based dynamics.\nDiffusion-based approach is prohibitively expensive, re-\nquiring hundreds of sampling steps, which need up to 61.3\nTFLOPs and 38.6 seconds for a single simulation. In con-\ntrast, our latent-based predictor reduces computation to 4.21\nTFLOPs (a ∼9-15× reduction) and achieves sub-second\ninference (0.341s). This efficiency gain is essential, mak-\ning our Inverse Survival Evaluation computationally feasi-\nble even in a multi-iteration setting (i.e., K=3).\n5. Conclusion\nWe introduced CLARITY, a medical world model address-\ning critical limitations in prior works. By forecasting dis-\nease evolution in a latent space, we avoid the stochastic-\nity and high computational cost of diffusion-based models.\nCLARITY is the first to explicitly integrate temporal and\nclinical contexts with an Inverse Survival Evaluation.\nExperiment proves that our feedback loop is critical for op-\ntimized decisions, with results outperforming all baselines.\nTherefore, CLARITY represents a significant step towards\n8\n"}, {"page": 9, "text": "computationally feasible and personalized treatment plan-\nning in oncology.\n9\n"}, {"page": 10, "text": "CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling\nContext-Aware Disease Trajectories in Latent Space\nSupplementary Material\n6. Additional Implementation Details\nCLARITY consists of four major components: an MRI en-\ncoder, a Therapy Policies Agent, a text encoder, and an\nActor module.\nFor MRI encoder, we adopt the founda-\ntion model BrainIAC [37], which follows a 12-layers ViT-B\nbackbone with d = dc = dt = 768. For Therapy Policies\nAgent, we employ GPT4o [20] with a specially designed\nprompt. For Text Encoder, we use the MedGemma [33],\nwhich is fine-tuned via 4-bit quantization and Low-Rank\nAdaptation (LoRA, [18]). For the Actor module, we apply\na N = 4 layers Transformer as its Latent Predictor while a\nM = 4 layers Transformer with two-way cross attention as\nits Survival Predictor.\nFor the training strategy, we combine an L1 latent re-\nconstruction loss, Cox survival loss, Brier score loss, and a\ncontrastive consistency loss, with empirically chosen coef-\nficients λ1 = 5 and λ2 = 1.\n7. Generalization Ability to Breast Cancer\nTo verify that our framework’s priority performance is not\nrestricted to brain tumors, we also finetune it on the I-SPY2\n[27] breast cancer dataset. I-SPY2 is a multi-center neoad-\njuvant trial that includes clinical variables, treatment Arms,\nand pCR outcomes. Each Arm corresponds to a specific\ndrug combination such as paclitaxel-based control or pacli-\ntaxel plus an investigational agent.\nWe apply CLARITY to this breast cancer setting us-\ning the standardized clinical context and Arm information.\nTab. 6 shows that ours outperform GPT-4o by 11.2% in Pre-\ncision and 9.2% in F1-score, demonstrating that the same\ndesign readily generalizes to a distinct disease domain.\nTable 6. Performance on the I-SPY2 breast cancer dataset.\nModel\nPrecision (%)\nRecall(%)\nF1(%)\nCLARITY\n44.1\n62.7\n51.8\nGPT-4o\n32.9\n60.7\n42.6\nThese results indicate that our framework is not lim-\nited to neuro-oncology applications; its modular design and\nrepresentation strategy transfer effectively across diseases,\nhighlighting its potential as a general modeling approach\nfor diverse clinical domains.\n(a)\nGround-truth\npost-treatment\nMRI\n(b) Reconstructed MRI from pre-\ndicted post-treatment latent\nFigure 6. Reconstruction comparison for post-treatment MRI.\nFigure 7.\nGround-truth versus predicted post–pre enhance-\nment difference maps ∆(post −pre). Both panels visualize di-\nrectional intensity change, with red indicating increased enhance-\nment and blue indicating decreased enhancement. The structural\nagreement shows that the model faithfully reconstructs treatment-\nsensitive ∆patterns.\n8. Interpretability of Latent Prediction\nTo show the interpretability of framework, we additionally\ntrain a diffusion-based decoder that maps latent represen-\ntations back into MRI space. This enables qualitative in-\nspection of whether the predicted latent trajectory preserves\nmeaningful anatomical and treatment-related changes.\nFig. 6 compares the ground-truth MRI with the\ndiffusion-based reconstruction generated from the predicted\nlatent state. The reconstructions remain structurally coher-\nent, indicating that the latent space retains sufficient infor-\nmation for image-level interpretation. Fig. 7 further con-\ntrasts the pre-treatment MRI with the post-treatment recon-\nstruction derived from the predicted latent state. Observable\nstructural differences align with expected treatment effects,\nsuggesting that the model captures clinically plausible dis-\nease evolution patterns.\nThese qualitative results demonstrate that our latent pre-\ndictions encode meaningful morphological changes that\n1\n"}, {"page": 11, "text": "support interpretability of the model’s dynamics.\n9. Notes on Safety and Validity\nAs shown in step 2 of Sec. 3.5, Ωenforces clinical valid-\nity at all stages: (i) the policy never proposes incompati-\nble drug pairs; (ii) dose/cycle proposals from q are clipped\nto guideline ranges; (iii) history-aware rules prevent con-\nflicts with prior therapies. This feedback-driven, constraint-\naware loop turns simulated outcomes into improved rec-\nommendations and mirrors clinicians’ iterative plan refine-\nment.\n10. Limitations\nWe acknowledge several limitations.\n• Generalizability. Our model is trained on two specific\nglioma cohorts. Its performance on new clinical centers,\nimaging protocols, or patient demographics is not yet val-\nidated.\n• Domain Specificity. The framework is specialized for\nbrain MRI and glioma modeling. Adapting it to other\ncancer types or imaging modalities (e.g., CT) would re-\nquire substantial re-training.\n• Therapy Agent Constraints. The policy agent (πLLM) is\nconstrained by a predefined set of medical rules (Ω). An\nincomplete Ωcould lead to sub-optimal or invalid therapy\nproposals.\n• Computational Cost.\nThe iterative Inverse Survival\nEvaluation (K = 3), while far more efficient than dif-\nfusion, is computationally more expensive than a single\nforward-pass, which may affect real-time deployment.\nReferences\n[1] Abdul Mohaimen Al Radi, Xu Cao, Fanyang Yu, Yuyuan\nLiu, Fengbei Liu, Chong Wang, Yuanhong Chen, Jintai\nChen, Hu Wang, Yanda Meng, et al. Agentic large-language-\nmodel systems in medicine: A systematic review and taxon-\nomy. Authorea Preprints, 2025. 1\n[2] AI Anthropic.\nClaude 3.5 sonnet model card addendum.\nClaude-3.5 Model Card, 3(6), 2024. 7\n[3] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bo-\njanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and\nNicolas Ballas. Self-supervised learning from images with a\njoint-embedding predictive architecture. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15619–15629, 2023. 1\n[4] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido,\nRussell Howes, Matthew Muckley, Ammar Rizvi, Claire\nRoberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-\nsupervised video models enable understanding, prediction\nand planning. arXiv preprint arXiv:2506.09985, 2025. 1\n[5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, et al. Qwen2. 5-vl technical report. arXiv preprint\narXiv:2502.13923, 2025. 7\n[6] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen,\nMichael Rabbat, Yann LeCun, Mahmoud Assran, and\nNicolas Ballas.\nRevisiting feature prediction for learn-\ning visual representations from video.\narXiv preprint\narXiv:2404.08471, 2024. 2\n[7] Jake Bruce, Michael D Dennis, Ashley Edwards, Jack\nParker-Holder, Yuge Shi, Edward Hughes, Matthew Lai,\nAditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Ge-\nnie: Generative interactive environments. In Forty-first Inter-\nnational Conference on Machine Learning, 2024. 1, 2\n[8] Maxime Burchi and Radu Timofte.\nMudreamer: Learn-\ning predictive world models without reconstruction. arXiv\npreprint arXiv:2405.15083, 2024. 2\n[9] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shu-\nnian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei\nZhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, to-\nwards injecting medical visual knowledge into multimodal\nllms at scale. arXiv preprint arXiv:2406.19280, 2024. 7\n[10] Yuanhong Chen, Yuyuan Liu, Chong Wang, Michael Elliott,\nChun Fung Kwok, Carlos Pe˜na-Solorzano, Yu Tian, Feng-\nbei Liu, Helen Frazer, Davis J McCarthy, et al. Braixdet:\nLearning to detect malignant breast lesion with incomplete\nannotations. Medical image analysis, 96:103192, 2024. 1\n[11] Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo,\nand Xiaoying Tang.\nPrior: Prototype representation joint\nlearning from medical images and reports. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 21361–21371, 2023. 1\n[12] David R Cox. Regression models and life-tables. Journal of\nthe Royal Statistical Society: Series B (Methodological), 34\n(2):187–202, 1972. 3, 5\n[13] Andreas Doerr, Christian Daniel, Martin Schiegg, Nguyen-\nTuong Duy, Stefan Schaal, Marc Toussaint, and Trimpe Se-\nbastian. Probabilistic recurrent state-space models. In Inter-\nnational conference on machine learning, pages 1280–1289.\nPMLR, 2018. 2\n[14] Brandon KK Fields, Evan Calabrese, John Mongan, Soon-\nmee Cha, Christopher P Hess, Leo P Sugrue, Susan M\nChang, Tracy L Luks, Javier E Villanueva-Meyer, An-\ndreas M Rauschecker, et al. The university of california san\nfrancisco adult longitudinal post-treatment diffuse glioma\nmri dataset. Radiology: Artificial Intelligence, 6(4):e230182,\n2024. 6, 7\n[15] W Brier Glenn et al. Verification of forecasts expressed in\nterms of probability.\nMonthly weather review, 78(1):1–3,\n1950. 5\n[16] Ahmed Gomaa, Yixing Huang, Amr Hagag, Charlotte\nSchmitter, Daniel H¨ofler, Thomas Weissmann, Katharina\nBreininger, Manuel Schmidt, Jenny Stritzelberger, Daniel\nDelev, et al. Comprehensive multimodal deep learning sur-\nvival prediction enabled by a transformer architecture: A\nmulticenter study in glioblastoma.\nNeuro-Oncology Ad-\nvances, 6(1):vdae122, 2024. 3\n[17] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap.\nMastering diverse control tasks through world\nmodels. Nature, pages 1–7, 2025. 1, 2\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\n2\n"}, {"page": 12, "text": "Lora: Low-rank adaptation of large language models. ICLR,\n1(2):3, 2022. 1\n[19] Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and\nSerena Yeung. Gloria: A multimodal global-local represen-\ntation learning framework for label-efficient medical image\nrecognition. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 3942–3951, 2021. 1\n[20] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-\nman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-\nhinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.\narXiv preprint arXiv:2410.21276, 2024. 1, 3, 7\n[21] Jared L Katzman,\nUri Shaham,\nAlexander Cloninger,\nJonathan Bates, Tingting Jiang, and Yuval Kluger. Deep-\nsurv: personalized treatment recommender system using a\ncox proportional hazards deep neural network. BMC medi-\ncal research methodology, 18(1):24, 2018. 3\n[22] UB Kogalur, H Ishwaran, EH Blackstone, and MS Lauer.\nRandom survival forests.\nAnnals of Applied Statistics, 2,\n2008. 3\n[23] Changhee Lee, William Zame, Jinsung Yoon, and Mihaela\nVan Der Schaar. Deephit: A deep learning approach to sur-\nvival analysis with competing risks. In Proceedings of the\nAAAI conference on artificial intelligence, 2018. 3\n[24] Changhee Lee, Jinsung Yoon, and Mihaela Van Der Schaar.\nDynamic-deephit: A deep learning approach for dynamic\nsurvival analysis with competing risks based on longitudi-\nnal data. IEEE Transactions on Biomedical Engineering, 67\n(1):122–133, 2019. 3\n[25] Minghan Li, Congcong Wen, Yu Tian, Min Shi, Yan Luo,\nHao Huang, Yi Fang, and Mengyu Wang.\nFairfedmed:\nBenchmarking group fairness in federated medical imaging\nwith fairlora. IEEE Transactions on Medical Imaging, 2025.\n1\n[26] Yuyuan Liu, Yu Tian, Chong Wang, Yuanhong Chen, Feng-\nbei Liu, Vasileios Belagiannis, and Gustavo Carneiro. Trans-\nlation consistent semi-supervised segmentation for 3d med-\nical images. IEEE Transactions on Medical Imaging, 2024.\n1\n[27] Luyang Luo, Mingxiang Wu, Mei Li, Yi Xin, Qiong Wang,\nVarut Vardhanabhuti, Winnie CW Chu, Zhenhui Li, Juan\nZhou, Pranav Rajpurkar, et al.\nA large model for non-\ninvasive and personalized management of breast cancer from\nmultiparametric mri. Nature Communications, 16(1):3647,\n2025. 1\n[28] Yan Luo, Min Shi, Yu Tian, Tobias Elze, and Mengyu\nWang.\nHarvard glaucoma detection and progression: A\nmultimodal multitask dataset and generalization-reinforced\nsemi-supervised learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 20471–\n20482, 2023. 1\n[29] Keon Mahmoudi, Daniel H Kim, Elham Tavakkol, Shingo\nKihira, Adam Bauer, Nadejda Tsankova, Fahad Khan, Adilia\nHormigo, Vivek Yedavalli, and Kambiz Nael. Multiparamet-\nric radiogenomic model to predict survival in patients with\nglioblastoma. Cancers, 16(3):589, 2024. 3\n[30] Intae Moon, Stefan Groha, and Alexander Gusev. Survlatent\node: A neural ode based time-to-event model with competing\nrisks for longitudinal data improves cancer-associated ve-\nnous thromboembolism (vte) prediction. In Machine Learn-\ning for Healthcare Conference, pages 800–827. PMLR,\n2022. 3\n[31] Chirag Nagpal, Xinyu Li, and Artur Dubrawski. Deep sur-\nvival machines: Fully parametric survival regression and\nrepresentation learning for censored data with competing\nrisks. IEEE Journal of Biomedical and Health Informatics,\n25(8):3163–3175, 2021. 3\n[32] University of Missouri, Yash Dhemesh, Frank Garrett, James\nGass, Joseph Greaser, Erblin Isufi, Lester J Layfield, Ahmed\nNada, Kamil Porgorzelski, James Sinclair, Nader HM Tahon,\nand Jason Thacker. Mu-glioma-post: University of missouri\npost-operative glioma dataset. The Cancer Imaging Archive\n(TCIA), 2025. 6, 7\n[33] Andrew Sellergren,\nSahar Kazemzadeh,\nTiam Jaroen-\nsri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger,\nShawn Xu, Fayaz Jamil, C´ıan Hughes, Charles Lau,\net al.\nMedgemma technical report.\narXiv preprint\narXiv:2507.05201, 2025. 3, 5, 1\n[34] Min Shi, Yan Luo, Yu Tian, Lucy Q Shen, Nazlee Zebardast,\nMohammad Eslami, Saber Kazeminasab, Michael V Boland,\nDavid S Friedman, Louis R Pasquale, et al. Equitable arti-\nficial intelligence for glaucoma screening with fair identity\nnormalization. NPJ Digital Medicine, 8(1):46, 2025. 1\n[35] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,\nJason Wei, Hyung Won Chung, Nathan Scales, Ajay Tan-\nwani, Heather Cole-Lewis, Stephen Pfohl, et al.\nLarge\nlanguage models encode clinical knowledge. Nature, 620\n(7972):172–180, 2023.\n[36] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery\nWulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R\nPfohl, Heather Cole-Lewis, et al. Toward expert-level med-\nical question answering with large language models. Nature\nMedicine, 31(3):943–950, 2025. 1\n[37] Divyanshu Tak, Biniam A Garomsa, Tafadzwa L Chaun-\nzwa, Anna Zapaishchykova, Juan Carlos Climent Pardo,\nZezhong Ye, John Zielke, Yashwanth Ravipati, Sri Va-\njapeyam, Maryam Mahootiha, et al. A foundation model for\ngeneralized brain mri analysis. medRxiv, 2024. 1\n[38] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 1\n[39] Qwen Team. Qwen3 technical report, 2025. 7\n[40] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanab-\nhuti, and Lequan Yu. Multi-granularity cross-modal align-\nment for generalized medical visual representation learn-\ning. Advances in Neural Information Processing Systems,\n35:33536–33549, 2022. 1\n[41] Ming\nXu.\nMedicalgpt:\nTraining\nmedical\ngpt\nmodel.\nhttps : / / github . com / shibing624 /\nMedicalGPT, 2023. 7\n[42] Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun,\nKang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille,\nLei Zhu, Yu-Dong Zhang, et al. Medical world model: Gen-\n3\n"}, {"page": 13, "text": "erative simulation of tumor evolution for treatment planning.\narXiv preprint arXiv:2506.02327, 2025. 2, 7\n[43] Yuanhao Zou and Zhaozheng Yin. Alignment, mining and\nfusion: Representation alignment with hard negative mining\nand selective knowledge fusion for medical visual question\nanswering. In Proceedings of the Computer Vision and Pat-\ntern Recognition Conference, pages 29623–29633, 2025. 1\n[44] Yuanhao Zou and Zhaozheng Yin. Mvcm: Enhancing multi-\nview and cross-modality alignment for medical visual ques-\ntion answering and medical image-text retrieval.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) Workshops, pages 180–\n190, 2025. 1\n4\n"}]}