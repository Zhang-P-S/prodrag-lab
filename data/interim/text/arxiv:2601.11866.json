{"doc_id": "arxiv:2601.11866", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.11866.pdf", "meta": {"doc_id": "arxiv:2601.11866", "source": "arxiv", "arxiv_id": "2601.11866", "title": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving", "authors": ["Kie Shidara", "Preethi Prem", "Jonathan Kim", "Anna Podlasek", "Feng Liu", "Ahmed Alaa", "Danilo Bernardo"], "published": "2026-01-17T01:13:48Z", "updated": "2026-01-17T01:13:48Z", "summary": "Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.11866v1", "url_pdf": "https://arxiv.org/pdf/2601.11866.pdf", "meta_path": "data/raw/arxiv/meta/2601.11866.json", "sha256": "27c7045ba03cfe6bba082bb4c66a9e4727013f1e82c65cd82342208c09d9ef5c", "status": "ok", "fetched_at": "2026-02-18T02:21:23.044576+00:00"}, "pages": [{"page": 1, "text": "ADVANCES IN LLM REASONING ENABLE FLEXIBILITY IN\nCLINICAL PROBLEM-SOLVING\nA PREPRINT\nKie Shidara\nWeill Institute of Neurology and Neurosciences\nUniversity of California, San Francisco\nPreethi Prem\nCarle Illinois College of Medicine\nUniversity of Illinois Urbana-Champaign\nJonathan Kim\nDepartment of Neurology and Neurological Sciences\nStanford University\nAnna Podlasek\nImage Guided Therapy Research Facility\nUniversity of Dundee\nFeng Liu\nDepartment of Systems Engineering\nStevens Institute of Technology\nAhmed Alaa\nDepartment of EECS\nUniversity of California Berkeley\nDanilo Bernardo\nWeill Institute of Neurology and Neurosciences\nUniversity of California, San Francisco\ndbernardoj@gmail.com\nJanuary 21, 2026\nABSTRACT\nLarge Language Models (LLMs) have achieved high accuracy on medical question–answer (QA)\nbenchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked\nwhether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We\nassessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the\nmedicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which\nutilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts\nwhere they become suboptimal. We found that strong reasoning models avoided Einstellung-based\ntraps more often than weaker reasoning models, achieving human-level performance on mARC. On\nquestions most commonly missed by physicians, the top 5 performing models answered 55% to 70%\ncorrectly with high confidence, indicating that these models may be less susceptible than humans to\nEinstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility\nin medical reasoning, achieving performance on par with humans on mARC.\n1\nIntroduction\nThe versatility and strong performance of Large Language Models (LLMs) across multiple domains1 have motivated\ntheir assessment in clinical contexts2. High scores have been reported on the United States Medical Licensing\nExam (USMLE)3, USMLE-styled question banks4–6, subspecialty board exams7,8, and validated clinical reasoning\nbenchmarks9. These results have been linked to emergent reasoning abilities10,11, yet performance in realistic, novel\nscenarios remains the crucial test12,13.\nWhile LLMs display impressive factual recall and pattern recognition, evidence shows that their reasoning abilities,\nparticularly in medical contexts, remain poor14. Studies have shown that when used for real-life clinical tasks requiring\nreasoning15 or abstraction16, LLMs often produce inconsistent, overconfident, and logically flawed responses17. Despite\nsucceeding on medical board-style questions, these models struggle to generalize beyond familiar exam patterns, failing\narXiv:2601.11866v1  [cs.CL]  17 Jan 2026\n"}, {"page": 2, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\nin settings that require flexible rationalization, such as determining admission status, radiological investigation(s)\nrequest status, and antibiotic prescription status13. This highlights that these limitations arise from an overreliance on\nsurface-level correlations within the training data rather than genuine reasoning17. LLMs inability to plan15, reason\nunder uncertainty14, or reflect metacognitively17 leads to reasoning failures that are masked by superficially fluent\nand overconfident language generation. This overconfidence in incorrect answers poses unique risks in medicine,\nwhere diagnostic reasoning depends on contextual synthesis rather than rote memorization18. The literature on LLMs\nconverges on a key insight that current LLMs may imitate medical reasoning without underlying cognitive flexibility,\nthus calling into question their capabilities and safety in real-world medical scenarios17, and necessitating targeted\nevaluation of LLM reasoning capabilities.\nMore recently, however, LLMs with reasoning of operation have emerged which utilize detailed multi-step reasoning\nchains, reminiscent of human-like thinking about a problem before answering19. Reasoning models have demonstrated\ngradual performance improvements in reasoning-based benchmarks such as Francois Chollet’s Abstraction and Rea-\nsoning Corpus (ARC) challenge20, suggesting that they have improved generalized reasoning abilities which may\npotentially translate to improved performance at medical reasoning. We previously introduced a novel medical QA\nevaluation, medical abstraction and reasoning corpus (mARC), as a probe to assess flexibility in medical reasoning\nby stress-testing the Einstellung effect, a cognitive bias in which reliance on familiar patterns inhibits the discovery\nof better solutions21,22. mARC problems were designed to elicit the Einstellung effect, rigid, heuristic completions\ntriggered by familiar cues, and led to inflexible reasoning by LLMs14. Here, we assess whether newer reasoning models\nmay overcome Einstellung effect in mARC.\nClinical Vignette (Input)\nPatient p presents with reduced alertness (S)\nCue C: Anticoagulation\nBlocker B: Anencephaly\nBackground medical knowledge K\n1. BrainBleed(x) →BrainPresent(x)\n2. Anencephaly(x) →¬BrainPresent(x)\nDefault Heuristic\nC ⇒def H\n(Anticoag →Bleed)\nAdversarial Option\n“Obtain CT Head”\nFailure to suppress default heuristic\nDeductive Constraint from K and B\nK ∪{B} ⊢¬H\n(Anencephaly →No Brain →No Bleed)\nCorrect Option\n“Obtain More Clinical Info”\nSuccessful override of default heuristic\nused with B\nactivates\nBLOCKS\nDriven by susceptibility\nto Einstellung effect\nDriven by reasoning over\nbackground knowledge K\nFigure 1: Demonstration of Einstellung effect evoked by failure to override default heuristics. The vignette presents\na cue (C) that triggers a default heuristic for brain bleed (H), and a blocker (B) that, together with background medical\nknowledge K (e.g., that intracranial hemorrhage requires a brain and anencephaly entails absence of brain), entails ¬H.\nAn Einstellung effect type reasoning failure occurs when the model prioritizes the default heuristic C ⇒def H over the\ndeductive constraint K ∪{B} ⊢¬H.\n2\nMethods\nMedicine Abstraction and Reasoning Corpus (mARC) Dataset.\nmARC questions follow the USMLE-style\nmultiple-choice format and comprises 100 author-written multiple choice questions crafted to resist rote pattern\nmatching or interpolation from existing medical QA corpora, which were previously validated by board-trained special-\nists14. We instantiate the Einstellung effect, rigidity of thought elicited by familiar problem cues21,22, as adversarial\ndesign elements and measure whether test-takers can override these pressures. Fifty-three percent of questions in-\nclude “seek more data” to test adaptive deferral. Subspecialties span neurology, neurosurgery, infectious disease,\nobstetrics-gynecology, ophthalmology, HEENT, hematology-oncology, gastroenterology, pulmonology, critical care,\ncardiology, and emergency medicine. To isolate flexibility from knowledge gaps, items require at most broadly taught,\nearly-clinical knowledge that were previously validated as appropriate for medical-school graduates14.\n2\n"}, {"page": 3, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\nOperationalizing the Einstellung effect. Each item triggers a stereotyped heuristic while making that heuristic coun-\nterproductive; correct solutions require cognitive flexibility. We demonstrate formalization of the Einstellung effect\ndemonstrating cue conflict with decisive blockers in Figure 1, with detailed description below.\n1. Knowledge base and default rule. Background medical knowledge K was represented as:\nBrainBleed(x) →BrainPresent(x),\nAnencephaly(x) →¬BrainPresent(x),\nyielding:\nK ⊢Anencephaly(x) →¬BrainBleed(x),\ni.e., an anencephalic patient cannot have an intracranial hemorrhage.\nClinicians were assumed to hold a cue-based default rule:\nAnticoag(x) ⇒def BrainBleed(x),\ncapturing the stereotyped concern for brain bleed in anticoagulated patients.\n2. Vignette construction (cue conflict with decisive blocker). We constructed a clinical vignette for a patient p that\ninstantiated:\nB := Anencephaly(p),\nC := Anticoag(p),\ntogether with a non-specific symptom S (reduced alertness) compatible with multiple etiologies, including brain\nbleed. Under K, B is a decisive blocker for\nH := BrainBleed(p),\nsince:\nK ∪{B} ⊢¬H\nand\nK ∪{B, H} is inconsistent.\n3. Response options and coding. Two options are provided amongst answer selections:\nEinstellung Trap: Obtain CT head — unrevised application of the default heuristic\nC ⇒def H despite the blocker B.\nCorrect Reasoning: Obtain more clinical information — overriding the default heuris-\ntic in light of K ∪{B} ⊢¬H and seeking alternative explanations\nfor S.\nData Collection and Analysis. We compared LLM performance to physician performance on mARC. Physicians (five\ntotal; specialties included pediatrics, internal medicine, and neurology) were recruited from UCSF Medical Center and\nkolabtree.com. Ethical approval was obtained from the UCSF IRB (IRB#24-42911); informed consent was obtained\nfrom all participants; all experiments complied with relevant guidelines. Physicians took the exam online with a 2-hour\nlimit; the reported human score is the mean of five physician accuracies.\nModels assessed included OpenAI GPT-4o23, OpenAI o124, OpenAI 5.1, Claude-Sonnet/Opus25, Google Gemini26,\nMistral27, DeepSeek R1 and V328, Llama29, and Grok30; closed-source models were evaluated via provider APIs,\nopen-source via HuggingFace/Lambda Labs. Chain-of-thought (CoT) prompting followed MMLU-Pro methodol-\nogy31,32; temperature was 0 when possible; otherwise defaults followed Wang et al.32.\nUncertainty estimation used agreement-based and entropy-based sample-consistency (SC)33–35, which outperforms\ntoken probability and elicitation in the medical domain36. To induce stochasticity across runs, we varied patient age by\nup to 10 days (no neonatal/infant cases). We constructed reliability plots and Brier scores following Lyu et al.35. We\nreport average accuracy, sample- and entropy-based consistency (SC and EC) across 15 runs per model (plateauing\nof these measures has been reported beyond this sample size37). Confidence intervals were computed by bootstrap\n(2,000 resamples) with Benjamini–Hochberg correction. The mARC problem dataset and analysis code are available at\nhttps://github.com/bernardolab/mARC-Reasoning.\n3\nResults\nAdvanced reasoning models (Figure 2, hatched bars) close the performance gap with physicians relative to their\nnon-reasoning counterparts (solid bars) on mARC. No significant difference in performance was observed between the\nClaude 4.1 Opus, Gemini 2.5-Pro, GPT-5.1, and Grok-4-Fast-Reasoning and human performance. For all reasoning\nmodels, the highest reasoning effort was selected. The best performing model was Claude 4.1 Opus with mean\nperformance of 0.751 [95% Confidence Interval (CI), 0.738-0.763]. As previously reported, non-reasoning models\ndemonstrate poor performance at mARC14.\n3\n"}, {"page": 4, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\nFigure 2: Comparison of model performance on mARC. Colored bars indicate model accuracies with 95% bootstrap\nconfidence intervals (CI) denoted by black range bar. The rightmost bar shows average human performance across 5\nphysicians with mean 0.66 and 95% CI 0.55 to 0.75. Accuracies shown for LLMs are mean accuracies across 15 runs.\nAdvanced reasoning models (hatched bars) demonstrated improved performanced compared to their non-reasoning\ncounterparts (solid bars). No significant difference in performance was observed between the Claude 4.1 Opus, Gemini\n2.5-Pro, GPT-5.1, and Grok-4-Fast-Reasoning and human performance (paired bootstrap test). The best performing\nmodel was Claude 4.1 Opus with mean performance of 0.75 [95% CI of 0.74-0.76].\nReasoning Figures 3 and 4 illustrate the mARC adversarial strategy that aims to exploit the Einstellung effect, cognitive\nbias that LLMs may be susceptible to from being trained to predict the next token in commonly occurring textual\nstructures14. The strategy works by embedding long-tail or out-of-distribution medical reasoning patterns into the\nproblem. These correct but uncommon reasoning paths are then placed alongside answer options that contain more\nfrequent, high-probability token sequences. This contrast leverages the model’s inherent tendency to favor familiar\nor statistically likely heuristic completions over deductive reasoning, thus increasing the chance of incorrect answers.\nQualitatively, top reasoning models apply followed deductive constraints to avoid Einstellung traps and instead select\nthe “seek additional information” (Figures 3 and 4). In the example question shown in Figure 3, o1’s response reveals a\nfailure in fundamental medical commonsense reasoning (blood pressure cannot be checked on the forehead), which\nis overcome by the stronger reasoning model (GPT-5.1). These examples illustrate that modern reasoning models\ncan override high-probability textual cues by adherence to deductive constraints and by recognizing information\ninsufficiency, strategies indicative of cognitive flexibilty needed to counteract Einstellung effects.\nWe assessed uncertainty estimation and calibration with both agreement-based and entropy-based sample-consistency\n(SC). Agreement-based SC provided stable uncertainty estimates across models (Figure 5), relative to entropy-based\nuncertainty estimation. We observed a general trend toward improved calibration in more recent, advanced reasoning\nmodels.\nNext, to test whether models can concretely overcome human-like fixation, we analyzed the subset of items for which a\nphysician majority (≥3/5) was incorrect—the human-miss set (20/100 items). Human accuracy on this set was 36%\n(95% CI 26–46%), vs. 73% on the remaining items. Using 15 stochastic trials per question and two-sided Wilson 90%\nCIs38,39, we labeled per-item outcomes as confidently correct (lower bound > 0.5), confidently incorrect (upper bound\n< 0.5), or indeterminate. On this set, using Wilson 90% decisiveness, the best model Claude-4.1-Opus (under mean run\naccuracy) is decisively correct on 55.0% [39.9, 80.0] of items, decisively wrong in 25.0% [9.9, 45.0], and indeterminate\nin 20.0% [0.0, 35.0]. Grok-4-Fast-Reasoning demonstrated relatively better performance on the human-miss set,\nhowever, had poorer mean accuracy compared to Claude-4.1 Opus on the entire mARC dataset. These model-wins\nversus physicians provide evidence that stronger reasoning models can avoid Einstellung biases in cases where humans\nare most susceptible.\n4\n"}, {"page": 5, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\nFigure 3: A newer reasoning model rejects the stereotyped “recheck BP” reflex at an invalid site, recognizes unreli-\nable measurement, and treats the patient based on clinical status. This reflects information sufficiency and context\nre-anchoring rather than rote, inflexible pattern completion.\n4\nDiscussion\nGiven that AI development has long drawn from human cognition40–42 and that the frontier of AI models, LLMs,\nhave been trained on vast quantities of human-generated text, it is plausible that models inherit human-like inductive\nbiases43–45. Understanding such biases is necessary to gauge their trustworthiness in real-world, clinical contexts. Here,\nwe demonstrate that strong reasoning LLMs show less vulnerability to the Einstellung effect in medical QA tasks\ncompared to their weaker reasoning and non-reasoning counterparts.\nThe improving reasoning abilities of LLMs have led to their steady performance improvements in reasoning-based\nbenchmarks such as Francois Chollet’s Abstraction and Reasoning Corpus (ARC) challenge20, as well as human expert-\nlevel performances in various human competitions in mathematics, such as in the International Math Olympiad46. Their\nability to exhibit \"reasoning\" has been attributed to recent advancements in their ability to utilize detailed multi-step\nreasoning chains, which is reminiscent of human-like “thinking” about a problem before outputting an answer19.\nEnhanced LLM reasoning ability leads to improved cognitive flexibility leading to improved generalization under\ndistribution shift20,47, which may explain modern reasoning model resistance to the Einstellung effect. While benchmark\nsuccess can reflect superficial shortcut learning48–52, our results nonetheless qualitatively identify behaviors associated\nwith advancements in reasoning with stronger reasoning models: adherence to deductive constraints and explicit\nrecognition of contexts which lack sufficient information for non-equivocal decision making.\nWe acknowledge several limitations in our work. mARC is intentionally compact (100 adversarial items) because\ncrafting long-tail, Einstellung-provoking scenarios is nontrivial. Future releases will expand coverage and incorporate\nprogrammatic out-of-distribution (OOD) generation53. In addition, we only provided qualitative content validity\nof \"reasoning\" ability examined on mARC; formal construct validation of Einstellung will be pursued in the future.\nFurthermore, even with the improved calibration seen with reasoning models, residual errors and areas of overconfidence\npersist, particularly in smaller models. We therefore advocate development of clinician-in-the-loop reasoning model\ndeployment with deferral capable models that abstain when uncertainty is high or when the problem lies in the long-tail\nregime49.\n5\n"}, {"page": 6, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\nFigure 4: The strong reasoning model defeats a high-frequency anticoagulation →hemorrhage lure by applying strict\nlogical negation.\nFigure 5: Uncertainty estimation on mARC using entropy- and agreement-based sample-consistency (SC). Newer/larger\nreasoning models showed improvements in accuracy, Brier scores and supplied usable confidence signals. Compared\nwith baseline performance on MMLU-Pro (black markers), mARC induces a distribution shift; nevertheless, SC remains\ninformative for deferral.\n6\n"}, {"page": 7, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\nFigure 6: Stacked three-way outcomes on the human-miss subset (N = 20). Model-win denotes items where the model\nis confidently correct while a majority of physicians erred; Both-miss denotes confident model error on physician-miss\nitems; Indeterminate denotes neither. The leading reasoning model produces model-wins on nearly half of the most\nchallenging items, indicating the ability to bypass human-like fixation.\n5\nConclusion\nOur results show that recent advances in reasoning models improve their reasoning flexibility, enabling them to\novercome overreliance on learned heuristics exhibited by their predecessors on mARC. These stronger reasoning models\nmatch physician-level performance on mARC and surpass physician performance on the subset of mARC questions\nthey most often missed, indicating a potential to overcome human cognitive biases such as the Einstellung effect. With\nfurther improvements in their reasoning ability, reasoning LLMs coupled with selective-deferral may become suitable\nfor clinician-augmented decision support.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with\ngpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[2] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol,\nand Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):\n259–265, 2023.\n[3] Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David\nChartash, et al. How does chatgpt perform on the united states medical licensing examination (usmle)? the\nimplications of large language models for medical education and knowledge assessment. JMIR medical education,\n9(1):e45312, 2023.\n7\n"}, {"page": 8, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\n[4] Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin\nEaton, Howard Antony Riina, Ilya Laufer, Paawan Punjabi, et al. Health system-scale language models are\nall-purpose prediction engines. Nature, 619(7969):357–362, 2023.\n[5] Cheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa, Cheryl Martin,\nMona G Flores, Ying Zhang, Tanja Magoc, et al. A study of generative large language model for medical research\nand healthcare. NPJ digital medicine, 6(1):210, 2023.\n[6] Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian D\nDavison, Hui Ren, et al. A generalist vision–language foundation model for diverse biomedical tasks. Nature\nMedicine, pages 1–13, 2024.\n[7] Jack B Longwell, Ian Hirsch, Fernando Binder, Galileo Arturo Gonzalez Conchas, Daniel Mau, Raymond Jang,\nRahul G Krishnan, and Robert C Grant. Performance of large language models on medical oncology examination\nquestions. JAMA Network Open, 7(6):e2417641–e2417641, 2024.\n[8] Marc Cicero Schubert, Wolfgang Wick, and Varun Venkataramani. Performance of large language models on a\nneurology board–style examination. JAMA network open, 6(12):e2346721–e2346721, 2023.\n[9] Stephanie Cabral, Daniel Restrepo, Zahir Kanjee, Philip Wilson, Byron Crowe, Raja-Elie Abdulnour, and Adam\nRodman. Clinical reasoning of a generative artificial intelligence model compared with physicians. JAMA Internal\nMedicine, 184(5):581–583, 2024.\n[10] Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can large language\nmodels reason about medical questions? Patterns, 5(3), 2024.\n[11] Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Beomseok\nSohn, Yongsik Sim, Dongha Lee, and Jinyoung Yeo. Large language models are clinical reasoners: Reasoning-\naware diagnosis framework with prompt-generated rationales. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 38(16), pages 18417–18425, 2024.\n[12] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer,\nMarcus Makowski, Rickmer Braren, Georgios Kaissis, et al. Evaluation and mitigation of the limitations of large\nlanguage models in clinical decision-making. Nature medicine, 30(9):2613–2622, 2024.\n[13] Christopher YK Williams, Brenda Y Miao, Aaron E Kornblith, and Atul J Butte. Evaluating the use of large\nlanguage models to provide clinical recommendations in the emergency department. Nature Communications, 15\n(1):8236, 2024.\n[14] Jonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu, Ahmed Alaa, and Danilo Bernardo. Limitations of large\nlanguage models in clinical problem-solving arising from inflexible reasoning. Scientific Reports, 15(1):39426,\n2025.\n[15] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Plan-\nbench: An extensible benchmark for evaluating large language models on planning and reasoning about change.\nAdvances in Neural Information Processing Systems, 36, 2024.\n[16] Melanie Mitchell, Alessandro B Palmarini, and Arseny Moskvichev. Comparing humans, gpt-4, and gpt-4v on\nabstraction and reasoning tasks. arXiv preprint arXiv:2311.09247, 2023.\n[17] Maxime Griot, Coralie Hemptinne, Jean Vanderdonckt, and Demet Yuksel. Large language models lack essential\nmetacognition for reliable medical reasoning. Nature communications, 16(1):642, 2025.\n[18] Steven J Durning, Michelle E Costanzo, Thomas J Beckman, Anthony R Artino Jr, Michael J Roy, Cees van der\nVleuten, Eric S Holmboe, Rebecca S Lipner, and Lambert Schuwirth. Functional neuroimaging correlates of\nthinking flexibility and knowledge structure in memory: Exploring the relationships between clinical reasoning\nand diagnostic thinking. Medical teacher, 38(6):570–577, 2016.\n[19] Sara Vera Marjanovi´c, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia,\nAditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, et al. Deepseek-r1 thoughtology: Let’s think about\nllm reasoning. arXiv preprint arXiv:2504.07128, 2025.\n[20] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv\npreprint arXiv:2412.04604, 2024.\n[21] Abraham S Luchins. Mechanization in problem solving: The effect of einstellung. Psychological monographs, 54\n(6):i, 1942.\n[22] Merim Bilali´c, Peter McLeod, and Fernand Gobet. The mechanism of the einstellung (set) effect: A pervasive\nsource of cognitive bias. Current Directions in Psychological Science, 19(2):111–115, 2010.\n8\n"}, {"page": 9, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\n[23] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila\nWelihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.\n[24] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.\n[25] Anthropic.\nThe claude 3 model family:\nOpus, sonnet, haiku.\nhttps://wwwcdn.anthropic.com/\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ModelCardClaude3.pdf, 2024. Accessed: 2025-01-22.\n[26] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal models. arXiv\npreprint arXiv:2312.11805, 2023.\n[27] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n[28] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948, 2025.\n[29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[30] xAI. Grok. https://x.ai/.\n[31] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n[32] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran\nArulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language understanding\nbenchmark. arXiv preprint arXiv:2406.01574, 2024.\n[33] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their\nuncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.\n[34] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171,\n2022.\n[35] Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki,\nMrinmaya Sachan, and Chris Callison-Burch. Calibrating large language models with sample consistency. arXiv\npreprint arXiv:2402.13904, 2024.\n[36] Thomas Savage, John Wang, Robert Gallo, Abdessalem Boukil, Vishwesh Patel, Seyed Amir Ahmad Safavi-Naini,\nAli Soroush, and Jonathan H Chen. Large language model uncertainty proxies: discrimination and calibration for\nmedical diagnosis and treatment. Journal of the American Medical Informatics Association, 32(1):139–149, 2025.\n[37] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination\ndetection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.\n[38] Sam Bowyer, Laurence Aitchison, and Desi R Ivanova. Position: Don’t use the clt in llm evals with fewer than a\nfew hundred datapoints. arXiv preprint arXiv:2503.01747, 2025.\n[39] Lawrence D Brown, T Tony Cai, and Anirban DasGupta. Interval estimation for a binomial proportion. Statistical\nscience, 16(2):101–133, 2001.\n[40] Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. Neuroscience-inspired\nartificial intelligence. Neuron, 95(2):245–258, 2017.\n[41] Anthony Zador, Sean Escola, Blake Richards, Bence Ölveczky, Yoshua Bengio, Kwabena Boahen, Matthew\nBotvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, et al. Catalyzing next-generation artificial\nintelligence through neuroai. Nature communications, 14(1):1597, 2023.\n[42] Sreejan Kumar, Theodore R Sumers, Takateru Yamakoshi, Ariel Goldstein, Uri Hasson, Kenneth A Norman,\nThomas L Griffiths, Robert D Hawkins, and Samuel A Nastase. Shared functional specialization in transformer-\nbased language models and the human brain. Nature communications, 15(1):5523, 2024.\n[43] Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. Cognitive bias in decision-making\nwith llms. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 12640–12653,\n2024.\n9\n"}, {"page": 10, "text": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving\n[44] Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, and Quanyan Zhu. Exploring prosocial\nirrationality for llm agents: A social cognition view. arXiv preprint arXiv:2405.14744, 2024.\n[45] Saeid Naeini, Raeid Saqur, Mozhgan Saeidi, John Giorgi, and Babak Taati. Large language models are fixated by\nred herrings: Exploring creative problem solving and einstellung effect using the only connect wall dataset. arXiv\npreprint arXiv:2306.11167, 2023.\n[46] Yuri Chervonyi, Trieu H Trinh, Miroslav Olšák, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk\nJung, Vikas Verma, Quoc V Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry\nwith alphageometry2. arXiv preprint arXiv:2502.03544, 2025.\n[47] Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness: An analysis of cot in\nplanning. arXiv preprint arXiv:2405.04776, 2024.\n[48] R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L Griffiths. Embers of autore-\ngression: Understanding large language models through the problem they are trained to solve. arXiv preprint\narXiv:2309.13638, 2023.\n[49] Lea Goetz, Nabeel Seedat, Robert Vandersluis, and Mihaela van der Schaar. Generalization—a key challenge for\nresponsible ai in patient-facing clinical applications. npj Digital Medicine, 7(1):126, 2024.\n[50] Arseny Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptarc benchmark: Evaluating\nunderstanding and generalization in the arc domain. arXiv preprint arXiv:2305.07141, 2023.\n[51] Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization:\nData contamination and trustworthy evaluation for large language models. arXiv preprint arXiv:2402.15938,\n2024.\n[52] Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, and S Yu Philip.\nWhen llms meet cunning texts: A fallacy understanding benchmark for large language models. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.\n[53] Shulin Huang, Linyi Yang, Yan Song, Shuang Chen, Leyang Cui, Ziyu Wan, Qingcheng Zeng, Ying Wen, Kun\nShao, Weinan Zhang, et al. Thinkbench: Dynamic out-of-distribution evaluation for robust llm reasoning. arXiv\npreprint arXiv:2502.16268, 2025.\n10\n"}]}