{"doc_id": "arxiv:2512.11573", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.11573.pdf", "meta": {"doc_id": "arxiv:2512.11573", "source": "arxiv", "arxiv_id": "2512.11573", "title": "Visualizing token importance for black-box language models", "authors": ["Paulius Rauba", "Qiyao Wei", "Mihaela van der Schaar"], "published": "2025-12-12T14:01:43Z", "updated": "2025-12-12T14:01:43Z", "summary": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.11573v1", "url_pdf": "https://arxiv.org/pdf/2512.11573.pdf", "meta_path": "data/raw/arxiv/meta/2512.11573.json", "sha256": "654edd7e1ce2a4a1eb24e6676891258d6b8edca41dbcab46617c5e2527c61d5d", "status": "ok", "fetched_at": "2026-02-18T02:24:19.967028+00:00"}, "pages": [{"page": 1, "text": "Visualizing token importance for black-box language models\nPaulius Rauba∗\nQiyao Wei∗\nMihaela van der Schaar\nUniversity of Cambridge\nUniversity of Cambridge\nUniversity of Cambridge\nAbstract\nWe consider the problem of auditing black-box\nlarge language models (LLMs) to ensure they\nbehave reliably when deployed in production\nsettings, particularly in high-stakes domains\nsuch as legal, medical, and regulatory compli-\nance. Existing approaches for LLM auditing\noften focus on isolated aspects of model behav-\nior, such as detecting specific biases or eval-\nuating fairness. We are interested in a more\ngeneral question—can we understand how the\noutputs of black-box LLMs depend on each\ninput token? There is a critical need to have\nsuch tools in real-world applications that rely\non inaccessible API endpoints to language\nmodels. However, this is a highly non-trivial\nproblem, as LLMs are stochastic functions\n(i.e. two outputs will be different by chance),\nwhile computing prompt-level gradients to ap-\nproximate input sensitivity is infeasible. To\naddress this, we propose Distribution-Based\nSensitivity Analysis (DBSA), a lightweight\nmodel-agnostic procedure to evaluate the sen-\nsitivity of the output of a language model for\neach input token, without making any distri-\nbutional assumptions about the LLM. DBSA\nis developed as a practical tool for practition-\ners, enabling quick, plug-and-play visual ex-\nploration of LLMs reliance on specific input\ntokens.\nThrough illustrative examples, we\ndemonstrate how DBSA can enable users to\ninspect LLM inputs and find sensitivities that\nmay be overlooked by existing LLM inter-\npretability methods.\n1\nIntroduction\nAuditing black-box language models is a fundamen-\ntal requirement to ensuring they behave reliably when\nProceedings of the 28th International Conference on Artifi-\ncial Intelligence and Statistics (AISTATS) 2025, Mai Khao,\nThailand.\nPMLR: Volume 258.\nCopyright 2025 by the\nauthor(s).\ndeployed in the real-world. Society implicitly expects\nLLMs to operate ethically, comply with regulations,\nand deliver technically sound responses without causing\nharm. This has resulted in the development of broad\naudit frameworks focused on governance and regulatory\noversight (Mökander et al., 2023; Meskó and Topol,\n2023; Raji et al., 2022). Despite these high-level frame-\nworks, practical algorithms that practitioners could\nemploy to audit LLMs are scarce.\nWhy is there a need for practical LLM audit al-\ngorithms? While high-level frameworks are necessary,\nthere is an increasing demand for practical algorithms\nthat provide detailed, interpretable insights into LLM\nbehavior. The urgency for such tools arises from two\nfactors: mitigating real-world harm and complying with\nregulatory demands. Without precise auditing tools,\nLLMs may propagate biases or deliver inconsistent out-\nputs, leading to societal harm (Gehman et al., 2020;\nNozza, Bianchi, Hovy, et al., 2021) or diverge from\ntheir intended purpose over time, causing unforseen\nnegative impacts (Lauer, 2021; Rahwan, 2018; Dafoe,\n2018). Regulatory frameworks like the EU AI Act are\nconsidering classifying LLMs as “high-risk AI systems”\nthat would require mandatory third-party audits be-\nfore deployment (Helberger, Diakopoulos, et al., 2023;\nMökander et al., 2023). Existing tools do not meet\nthese needs because they either focus narrowly on spe-\ncific biases or require complex, benchmark-oriented\nsetups that are not adaptable to diverse real-world\nscenarios.\nWhat is missing from current LLM auditing\nmethods? The auditing landscape for LLMs is domi-\nnated by methods that target isolated dimensions, such\nas bias detection (Borkan et al., 2019; Dixon et al.,\n2018; Park, Shin, and Fung, 2018), fairness evaluation\n(Garg et al., 2019), or adversarial robustness (Geisler\net al., 2024). While these approaches are valuable for\ntargeted analysis, they lack the flexibility and general\napplicability required by practitioners who need quick,\ninterpretable feedback on LLM behavior across varied\ndomains. What is missing is a practical, easy-to-deploy,\nstatistically-grounded tool that provides actionable,\nvisual insights into the behavior of any black-box LLM.\narXiv:2512.11573v1  [cs.CL]  12 Dec 2025\n"}, {"page": 2, "text": "Visualizing token importance for black-box language models\nTable 1: Why sensitivity analysis is important\nfor model auditing. Two examples show sensitivity\nscores for each word when an LLM is asked for legal\nadvice. Darker shades indicate higher importance. In\nExample 1, the model focuses on the alleged crime,\nwhile in Example 2, its sensitivity to the person’s name\nindicates an undesired reliance when the name is irrel-\nevant to the LLM response. Example 2 could prompt\nfurther investigation in practical scenarios.\nLegend:\nMost important\nLeast important\nExample 1 (Desired behavior):\nJohn\nDoe\nis\naccused\nof\nmurder .\nExample 2 (Undesired behavior):\nJohn\nDoe\nis\naccused\nof\nmurder .\nHow can sensitivity analysis serve as a practi-\ncal and interpretable tool for auditing LLMs?\nWe diverge from existing auditing mechanisms and de-\nfine an entirely new task—developing a model-agnostic\nframework for LLM sensitivity analysis. Sensitivity\nanalysis offers a practical and interpretable way to au-\ndit LLMs by showing how model outputs change with\nvariations in input. In a nutshell, the primary question\nthat sensitivity analysis for language models answers\nis the following:\n“How do the answers produced by a language\nmodel depend on each input token?”\nSuch an approach allows practitioners to visualize and\ninterpret the influence of each token, providing a model-\nagnostic way of auditing LLM behavior. For exam-\nple, Table 1 shows how sensitivity analysis can re-\nveal unwanted model behavior when evaluating legal\nprompts, providing clear, actionable insights that stan-\ndard benchmarks might overlook. The ability to iden-\ntify such context-dependent sensitivities allows practi-\ntioners to explore if model behavior is appropriate for\nthe given scenario.\nHowever, implementing such a tool poses unique chal-\nlenges. Repurposing existing gradient-based methods\nfor black-box LLMs (Zeiler and Fergus, 2014) is impos-\nsible because we assume no access to LLM internals.\nAnother alternative could be to manually change the\ninput to a desired perturbation and inspect the changes.\nHowever, this is problematic because LLMs are stochas-\ntic generators, i.e. two outputs will vary by chance.\nTherefore, it is insufficient to look at two individual\nanswers due to the stochastic nature of the LLMs; one\nmust consider the entire distribution of possible answers\nrather than individual outputs.\nOur solution. To provide a practical solution for these\nchallenges, we introduce Distribution-Based Sensitivity\nAnalysis (DBSA). DBSA is a lightweight, exploratory\ntool that finds the most important tokens in an input\nprompt. It is designed to be immediately usable by\npractitioners without requiring access to proprietary\nmodel internals or specialized knowledge. To achieve\nthis, we approximate sensitivity analysis by considering\neach token’s nearest neighbors in embedding space,\nperforming Monte Carlo sampling for each neighbor\nand comparing Monte Carlo estimates on a reduced\nsimilarity space. With this formulation, we (i) capture\nthe stochastic nature of LLM outputs more faithfully\nthan point estimates; (ii) connect LLM outputs to\ndistributional testing; (iii) quantify effect size; (iv)\nmaintain model and input agnosticism, and (v) allow\nto directly visualize and highlight tokens based on their\nimportance.\nWhat practical value does DBSA offer? DBSA\nis designed for immediate application across various\ndomains such as legal, medical, and customer service,\nwhere understanding token-level behavior is important.\nExisting methods, such as gradient-based sensitivity\nand bias evaluation techniques, are limited to specific\nuse cases or require access to model internals. DBSA\naddresses the entirely new task of generalized sensi-\ntivity analysis for black-box LLMs, applicable across\ndiverse domains. Because this is a new interpretability\nsetup, it does not lend itself naturally to benchmark\ncomparisons—a reason why we focus on illustrative\nexamples and ablation studies in Sec. 5 - 6. We see\nDBSA as having direct applicability in many applica-\ntions, such as visualizing all tokens in a prompt (Table\n2), top n tokens and computing effect sizes (Table 3),\nand others.\n­\nContributions. 1\n⃝Conceptually, we introduce an\nentirely new task—auditing how the output distri-\nbution of black-box LLMs depend on specific input\ntokens (Sec. 2). 2\n⃝Technically, we develop DBSA,\na lightweight, easily deployable model-agnostic al-\ngorithm that uses finite-sample approximations and\nstatistical testing to measure the impact of token-\nlevel changes (Sec. 3 - 4). 3\n⃝Empirically, we show\nhow DBSA offers black-box interpretability (Sec. 5)\nand build trust in the method by performing relevant\nquantitative ablations (Sec. 6).\n2\nUnique challenges of obtaining token\nimportance scores in black-box\nlanguage models\nWe start by defining our problem (Sec. 2.1) and explain-\ning why evaluating LLM responses requires looking at\ndistributions of answers (Sec. 2.2). We then explore\nthe unique challenges this poses (Sec. 2.3), proposing\na procedure to address them (Sec. 3).\n"}, {"page": 3, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\n2.1\nProblem formulation\nLet X denote the input space, where each input x ∈X\nis a sequence of words or tokens x = (t1, t2, . . . , tn).\nLet Y denote the output space of the machine learning\nsystem. We define the system as a stochastic mapping\nS : X →P(Y), where P(Y) denotes the space of\nprobability distributions over Y.\nObjective. Our objective is to address the following\nresearch question: Given a black-box ML system S, an\ninput x ∈X, and an individual token(s) ti at position i\nin x, how can we systematically measure and interpret\nthe sensitivity of the output distribution S(x) with\nrespect to small perturbations of ti? We measure such\nminimal changes in token-level sensitivity as follows.\nDefinition 1 (Token-Level Sensitivity). The token-\nlevel sensitivity of the machine learning system S at\nposition i in input x ∈X is defined as the expected\nchange in the output distribution S(x) resulting from\na small perturbation δti to the word ti, normalized by\nthe magnitude of the perturbation:\nSensitivity(ti) =\nlim\n∥δti∥→0\nω\n\u0000S(x), S\n\u0000x[ti←ti+δti]\n\u0001\u0001\n∥δti∥\nx[ti←ti+δti] denotes the input sequence where ti has been\nperturbed by δti and ω is a suitable distance measure.\n2.2\nAssessing sensitivity through comparison\nof output distributions\nHow can we evaluate token-level sensitivity?\nThis\nrequires (a) analyzing how the entire distribution of\nresponses would change under the defined sensitivity\nmetric; (b) understanding how to approximate such\nsmall perturbations in a discrete token space and a\nblack-box setting; (c) evaluate how to take into account\nthe inherent stochasticity of language model responses.\nWe address these in turn.\nFirst, we need a framework to evaluate how the entire\ndistribution of responses would change under a par-\nticular change in the input. To do this, we take the\napproach of using Monte Carlo estimates of perturbed\ninputs as a measure of constructing such a distribution\n(Rauba, Wei, and Schaar, 2024). Suppose we swap an\ninput token with its nearest neighbor in embedding\nspace. A naive approach might involve comparing indi-\nvidual outputs from S for the original input x and its\nperturbed version x[ti←ti+δti]:\ny = S(x),\ny′ = S\n\u0000x[ti←ti+δti]\n\u0001\nHowever, since S produces random outputs even for the\nsame input, such a comparison between single samples\ny and y′ is insufficient. Any observed difference could\nmerely be a consequence of the system’s stochasticity\nrather than the effect of the perturbation δti.\nTo address this challenge, we employ a statistical ap-\nproach that compares the entire output distributions of\nS for the original and perturbed inputs (Rauba, Wei,\nand Schaar, 2024). Specifically, we consider the distri-\nbutions S(x) and S\n\u0000x[ti←ti+δti]\n\u0001\nover the output space\nY. Therefore, our objective is to determine whether\nthe swapped nearest neighbor token δti leads to a sig-\nnificant change in the output distribution. This can be\nformalized as a hypothesis testing problem:\nH0 : S(x) = S\n\u0000x[ti←ti+δti]\n\u0001\n(No effect)\nH1 : S(x) ̸= S\n\u0000x[ti←ti+δti]\n\u0001\n(Has an effect)\nThe primary benefit of such a distributional formula-\ntion is that it captures the full stochastic behavior of\nS instead of just a single realization. This means we\ncould perform statistical inference by directly compar-\ning these distributions and understanding how much\nthe outputs have shifted across the whole output space,\neven detecting subtle shifts that might not be apparent\nfrom individual samples. However, while theoretically\ngrounded, the usage of such a hypothesis testing frame-\nwork in the context of LLMs poses unique practical\nchallenges that do not appear in regular settings.\n2.3\nChallenges with analyzing token-level\noutput distributions\nBuilding on the foundational work of Rauba, Wei, and\nSchaar (2024), we identify three key challenges in an-\nalyzing output distributions for token-level sensitiv-\nity, with particular emphasis on the discrete nature of\nnearest-neighbor perturbations.\n▶Challenge 1: Computational intractability. As\nestablished in prior work, the fundamental computa-\ntional barrier lies in the exponential size of the output\nspace Y. For language models operating on vocabulary\nV with sequence length L, we have |Y| = |V |L possi-\nble outputs. The probability of any output sequence\ny = (y1, y2, . . . , yL) ∈Y under input x is given by:\nS(x)(y) = P(y | x) =\nL\nY\nt=1\nP(yt | y<t, x)\nwhere y<t = (y1, . . . , yt−1) represents the token history.\nComparing distributions S(x) and S(x[ti←ti+δti]) re-\nquires evaluating these probabilities across the entire\noutput space - a computation that grows exponentially\nwith both vocabulary size and sequence length.\n▶Challenge 2: Semantic interpretability. Fur-\nthermore, statistical differences between output distri-\n"}, {"page": 4, "text": "Visualizing token importance for black-box language models\nbutions may not reflect meaningful semantic changes.\nThat is, while the outputs might be different token-\nwise, they might convey the same semantic meaning.\nConsider the outputs:\ny1: “Targeted radiation therapy is suggested”, y2: “We\nsuggest targeted radiation therapy”\nWhile S(x)(y1) ̸= S(x)(y2) in general, both convey\nidentical medical recommendations. Therefore, we re-\nquire a way to evaluate outputs such that we take into\naccount whether the semantic meaning has changed.\n▶Challenge 3: Discrete token space constraints.\nThird, we require to perform swaps in discrete token\nspace. The definition of token-level sensitivity in Defi-\nnition 1 assumes the existence of infinitesimal pertur-\nbations δti. However, in a discrete token space without\naccess to embeddings, such continuous perturbations\nare undefined. Instead, we must work with discrete\njumps between tokens, where the smallest possible per-\nturbation is a substitution with the nearest neighbor:\nmin\nδti ∥δti∥=\nmin\ntj∈N(ti) dist(ti, tj)\nwhere N(ti) denotes the set of nearest neighbors of\ntoken ti.\nIn order to develop a method to visualize tokens, we\nmust make it computationally tractable, evaluate the\neffect of sensitivity on a semantic level and take into\naccount the discrete nature of the token space. The\nfollowing sections introduce an approach that addresses\nthis.\n­\nTakeaway. The discrete nature of nearest-neighbor\ntoken perturbations introduces fundamental con-\nstraints on sensitivity analysis that require extend-\ning existing distributional approaches to handle non-\ninfinitesimal changes in the black-box LLM setting.\n3\nToken-level sensitivity with finite\nsample approximations\nHow can we address the challenges outlined in Sec-\ntion 2.3?\nWe propose a light-weight finite sample\napproximation procedure that enables practical esti-\nmation of token-level sensitivity through hypothesis\ntesting on output distributions.\n3.1\nAddressing outlined challenges\n▶Addressing challenge 1: Computational in-\ntractability. We approximate the intractable distribu-\ntions S(x) and S(x[ti←t′\ni]) using finite samples obtained\nvia Monte Carlo sampling of the LLM. We generate n\n5.00\n10.00\n15.00\nCosine Similarity\n0.1\n0.2\n0.3\nFrequency\nReference Answer\n5.00\n10.00\n15.00\nCosine Similarity\n0.1\n0.2\n0.3\nReference and perturbed\nOriginal input\nPerturbed input\nFigure 1: Example of null and alternative dis-\ntributions. The null distribution P0 (left, blue) is\nconstructed based on the intrinsic variability of re-\nsponses. The alternative distribution with a perturbed\ninput of a single nearest neighbor P1 (right, red) is\nquantified with respect to the original distributions.\nThis figure shows the output distribution change in the\ncosine similarity space.\nindependent samples from each distribution, which is\nused to calculate the token-level sensitivities.\nˆS(x) = {y(1), y(2), . . . , y(n)},\ny(j) ∼S(x),\nˆS(x[ti←t′\ni]) = {y′(1), y′(2), . . . , y′(n)},\ny′(j) ∼S(x[ti←t′\ni]).\n▶Addressing challenge 2:\nSemantic inter-\npretability. Recall that statistical differences between\noutput distributions may not reflect meaningful seman-\ntic changes. To ensure that we are indeed measuring\nsemantic distances, we define a semantic similarity func-\ntion s : Y × Y →[0, 1] that quantifies the semantic\nsimilarity between outputs. We use a similarity metric\nin Sec. 4 to evaluate distributional differences.\n▶. Addressing challenge 3: Discrete token space\nconstraints). We approximate the infinitesimal per-\nturbation δti by substituting ti with one of its k nearest\nneighbors in the embedding space. Let ϕ : W →Rd\nbe a token embedding function, and let Nk(ti) de-\nnote the set of k nearest neighbors of ti based on the\nsimilarity metric s. We select a small perturbation\nt′\ni ∈Nk(ti) and define the perturbation magnitude as\n∥δti∥= ∥ϕ(t′\ni) −ϕ(ti)∥.\n3.2\nQuantifying token-level sensitivity\nLet P0 be the distribution of similarities between pairs\nwithin ˆS(x), and P1 be the distribution of similarities\nbetween pairs from ˆS(x) and ˆS(x[ti←t′\ni]):\nP0 =\nn\ns\n\u0010\ny(i), y(j)\u0011\n| 1 ≤i, j ≤n\no\n,\nP1 =\nn\ns\n\u0010\ny(i), y′(j)\u0011\n| 1 ≤i, j ≤n\no\n,\n"}, {"page": 5, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nwhere y(i), y(j) i.i.d.\n∼S(x) and y′(j) i.i.d.\n∼S(x[ti←t′\ni]).Here,\nP0 captures the intrinsic variability within the original\noutput (equivalent to the null distribution), whereas\nP1 captures the cross-distribution similarities between\nthe original and swapped token outputs (equivalent to\nthe alternative distribution), c.f. Figure 1, where each\nswapped token is a nearest neighbor of the original\ntoken. We have therefore constructed two distributions\nwhich represent the variability in answer similarities as\na proxy for sensitivity (Def. 1). In Sec. 4, we show how\nsuch distributions can be used to obtain token-level\nsensitivities and associated p-values.\nOur hypothesis test is then formulated as:\nH0 : E[P0] = E[P1],\n(No significant effect),\nH1 : E[P0] ̸= E[P1],\n(Perturbation shifts similarity).\nWe estimate the token-level sensitivity S at position i\nby computing the average difference between the null\ndistribution and alternative distribution.\nAdvantages. With this formulation, we (i) capture\nthe stochastic nature of LLM outputs more faithfully\nthan point estimates; (ii) connect LLM outputs to\nfrequentist hypothesis testing; (iii) quantify effect size;\nand (iv) maintain model and input agnosticism.\n­\nTakeaway. Using finite-sample approximations can\naddress the three primary challenges associated with\nevaluating token-level sensitivity.\n4\nDistribution-based sensitivity\nanalysis\nNow, we present a light model-agnostic methodology\nfor assessing the token-level sensitivity of LLMs. Our\napproach avoids restrictive distributional assumptions\nabout LLM outputs. We enable frequentist statisti-\ncal hypothesis testing using effect size and p-values\nthrough the construction of null and alternative dis-\ntributions. Importantly, our framework is applicable\nto any perturbation and any language model, with the\nminimal requirement of being able to sample from the\nlanguage model’s output distribution and construct\nembeddings. We outline the general procedure below\nand provide other implementation details in Appendix\nB.\nDBSA in a nutshell. DBSA computes token-level\nsensitivity by measuring the effect of token perturba-\ntions on LLM output distributions. Given an input\nsequence x and its unique tokens T, indices Iw for each\ntoken w are identified, and for each i ∈Iw, we find\nthe k nearest neighbors Nk(ti) of ti in the embedding\nspace using a distance function D. For each neighbor\nt′\ni ∈Nk(ti), a perturbed sequence x(t′\ni) is generated\nby replacing ti with t′\ni. The LLM produces samples\nY = {y(j)}n\nj=1 ∼S(x) and Y ′ = {y′(j)}m\nj=1 ∼S(x(t′\ni)),\nand their embeddings Φ and Φ′ are used to compute\npairwise similarities with a function s.\nDistance measure.\nThere are many possible dis-\ntances metrics which might offer different proper-\nties.\nWe seek for a distance which would (i) di-\nrectly compute pairwise similarities; (ii) avoid inter-\nmediate density estimation; (iii) capture different mo-\nments of the distribution; (iv) would directly com-\npute p-values, and (v) scale well with the number\nof samples. A natural choice for this is the energy\ndistance.\nThe energy distance between\nˆS(x) and\nˆS(x(t′\ni)) is calculated as E( ˆS(x), ˆS(x(t′\ni))) = 2A −\nB −C, where A =\n1\nnm\nPn\na=1\nPm\nb=1 s(ϕ(y(a)), ϕ(y′(b))),\nB\n=\n1\nn2\nPn\na=1\nPn\nb=1 s(ϕ(y(a)), ϕ(y(b))), and C\n=\n1\nm2\nPm\na=1\nPm\nb=1 s(ϕ(y′(a)), ϕ(y′(b))). With this distance,\nsensitivity scores ω(w′\ni)\nwi\nand p-values p(w′\ni)\nwi\nare computed\nfor each token position and neighbor, then averaged\nacross neighbors and instances to yield final scores\nωw and pw for each token w. This model-agnostic ap-\nproach only requires the ability to sample outputs and\ncompute embeddings, ensuring broad applicability.\nThe chosen distance measure is a design choice and\nnot a necessary component of the sensitivity analysis\nmore broadly. One can choose a different ω, a different\nnumber of nearest neighbors or embedding components\nto suit each use case separately.\nWe provide more implementation details in Appendix\nB, including an algorithm for DBSA and a discussion\non different design choices. We perform some major\nablations of such choices in Sec. 6 and Appendix C.\n­\nTakeaway. A distribution-based approach provides\na lightweight solution that captures the full vari-\nability of LLM outputs, enabling practitioners to\naudit model behavior without requiring access to\ninternals.\n5\nIllustrative examples\nPurpose and scope. In this section, we present two\nillustrative examples that demonstrate how DBSA can\nbe used as a decision-support tool in real-world do-\nmains. Importantly, DBSA can be used in addition to\nother audit mechanisms to aid decision support. Our\nexamples are based on text-based prompts because,\nunlike many common LLM interpretability methods,\nwe do not assume any ground-truth labels. All experi-\nmental details can be found in Appendix D1.\n1Associated\ncode\nfor\nDBSA\ncan\nbe\nfound\nhere:\nhttps://github.com/vanderschaarlab/visualizing-token-\n"}, {"page": 6, "text": "Visualizing token importance for black-box language models\nComparison to other auditing algorithms. Exist-\ning sensitivity methods cannot be directly repurposed\nfor these use cases due to the unique problem formula-\ntion (Sec. 2) that requires understanding token-level\nimpact on LLMs.\nIllustrative Example I: Legal Systems\nExample I. Using DBSA in legal audit\nThe Problem. A tech company uses an AI-powered\nsystem to analyze legal contracts for potential risks\nby querying a language model for advice. Recently,\nthe system missed a critical clause that resulted in\nmassive legal damage.\nThe Need. The company cannot rely on black-\nbox LLMs for contract evaluation. It needs more\ntransparency and currently has no way of auditing\nmodels.\nThey want to implement an additional\nmonitoring system which can help understand how\nthe answers might change if the information were\nslightly different.\nContext. The consequences of misinterpreting key\ncontractual elements can be severe. In this example, we\nillustrate how DBSA can be integrated into the auditing\nprocess to provide token-level insights that directly\ninform legal teams. Table 2 provides an example to\nvisualize DBSA.\nTable 2: Example of using DBSA to audit legal\ndocuments for which words impact answers the\nmost. Each word is highlighted based on its normalized\nimpact on the output distribution ω.\nDarker color\nhighlights greater impact on answers.\nLegend:\nMost important\nLeast important\nCompany\nA\nagrees\nto\npay\nCompany\nB\n$10\nmillion\nfor\ndeveloping\na\nrevolutionary\nAI\nsoftware\nwithin\n12\nmonths\n.\nIf\nCompany\nB\nfails\nto\ndeliver\na\nfully\nfunctional\nproduct\nby\nthe\ndeadline\n,\nthey\nmust\nrefund\n50\n%\nof\nthe\npayment\nand\nprovide\nan\nadditional\n3\nmonths\nof\ndevelopment\nat\nno\nextra\ncost\n.\nHowever\n,\nif\nthe\ndelay\nis\ndue\nto\ncircumstances\nbeyond\nCompany\nB\n’\ns\nreasonable\ncontrol\n,\nthese\npenalties\nshall\nnot\napply\n.\nThis\nagreement\nis\ngoverned\nby\nCalifornia\nlaw\nand\nany\ndisputes\nshall\nbe\nresolved\nthrough\nbinding\narbitration\n.\nDiscussion I. DBSA ensures the LLM focuses on crit-\nical contractual terms like “agreement”, “California”,\nand “AI software” It also identifies irrelevant terms,\nsuch as “product” or “governed”, confirming their mini-\nmal impact. This validates the model’s behavior, giving\nlegal teams confidence in its outputs. Therefore, DBSA\ncan be used to highlight how LLM answers depend\nimportance\non each word to corroborate expert knowledge and\nevaluate LLM safety.\nIllustrative Example II: Clinical Support\nExample II. Using DBSA in clinical support\nThe Problem. A hospital has implemented an\nLLM-powered clinical decision support system to as-\nsist doctors. The system analyzes patient data (e.g.\nmedical history) to provide treatment recommen-\ndations. Recently, there was a near-miss incident\nwhere the system suggested an incorrect diagnosis\nthat could have led to potentially harmful treatment.\nUpon review, it was discovered that a single word\nin the patient’s symptom description significantly\naltered the system output.\nThe Need. The hospital cannot use an LLM with-\nout having an additional mechanism to understand\nthe impact of each word on the output of the model.\nContext.\nDecision-support systems must be inter-\npretable and reliable, as their outputs directly impact\npatient safety. We can employ DBSA to identify the\ntop k words which change the output the most with\nminor variability and compute their effect sizes (ω) as\nwell as p-values. We show such an example in Table 3.\nTable 3: Example of using DBSA to identify top\nwords affecting the output and identify effects\nof minor variations. The top k = 5 words are high-\nlighted, and their effect size, p-value, and statistical\nsignificance (p < α, where α = 0.05) are computed.\nWe see that none of the words affect the output signifi-\ncantly due to the high p-value.\nLegend:\nMost important\nLeast important\nPatient is a 45-year-old male presenting with progressive dyspnea\non exertion over the past two weeks. On\nexamination , patient ap-\npears mildly distressed.\nLower\nextremities show 2+ pitting edema to\nmid\n-shin bilaterally. Chest X-ray shows pulmonary vascular conges-\ntion. Clinical presentation is consistent with new-onset\ncongestive\nheart failure, likely due to\nhypertensive\nheart disease.\nWord\nEffect size\np-value\np < α\ncongestive\n0.08\n0.11\n✗\nexamination\n0.07\n0.16\n✗\nLower\n0.07\n0.28\n✗\nmid\n0.07\n0.39\n✗\nhypertensive\n0.06\n0.31\n✗\nDiscussion II. DBSA confirms the LLM’s stability,\nas the highlighted words are appropriate and expected,\ndemonstrating consistency in recommendations. In this\nway, DBSA can be used to highlight the top k words\nand evaluate their effect size.\n"}, {"page": 7, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\n­\nTakeaway. Existing sensitivity methods are inade-\nquate for highlighting the importance of each token.\nDBSA offers a model-agnostic solution to evaluate\nmodel sensitivity to each token.\n6\nInsights Experiments\nPurpose and scope. To supplement the illustrative\nexamples above, we aim to provide a preliminary analy-\nsis into the effectiveness and sensitivity of DBSA across\nvarious parameters and conditions. We do this by per-\nforming comprehensive empirical testing, ablating most\nof our design components and evaluating how they af-\nfect the results of DBSA. All experimental details can\nbe found in Appendix .\n6.1\nExperiment I: Effects across models\nSetup. We compare DBSA sensitivity analysis results\nacross language models to assess consistency in identi-\nfied sensitivities. Inputs are perturbed using nearest\nneighbors, and effect size distributions are evaluated\nacross LLMs (see Figure 2) using the prompt from our\nclinical support example.\n0.00\n0.02\n0.04\n0.06\nEffect sizes (omega) of words\nGemma\nGPT35\nLlama31\nMistral7B\nGPT4\nMPSD\nPhi3\nDistribution of effect sizes for tokens\nFigure 2: Comparison of effect sizes across dif-\nferent language models.\nThe chart describes ω\nvalues for each word for seven large language models.\nThe y-axis lists the models analyzed (full names in the\nAppendix, abbreviated for clarity). This comparison\nillustrates how the models vary in their sensitivity to\ninput perturbations.\nTakeaway I. DBSA shows variation in sensitivity pat-\nterns across models, indicating that model architecture\ninfluences token importance. This validates DBSA’s\ncapability to identify these variations.\n6.2\nExperiment II: Sensitivity across\nlanguage models\nSetup. We compute Spearman rank correlation coeffi-\ncients between models to quantify similarity in sensi-\ntivity rankings, assessing if models exhibit consistent\nbehavior in token sensitivity. Results are in Table 4.\nTable 4: Spearman Rank Coefficients between\nLLMs. Higher values indicate stronger correlation in\nword rankings between models.\nModel\nGPT-4\nGPT-3.5\nSmolLM\nMagicPrompt\nMistral-7B\nPhi-3-mini\nLlama-3-8B\nGPT-4\n1.00\n0.14\n0.03\n-0.07\n-0.02\n0.04\n-0.07\nGPT-3.5\n0.14\n1.00\n0.03\n0.09\n0.38\n-0.07\n0.17\nSmolLM\n0.03\n0.03\n1.00\n0.16\n0.25\n0.05\n0.23\nMagicPrompt\n-0.07\n0.09\n0.16\n1.00\n0.44\n0.43\n0.38\nMistral-7B\n-0.02\n0.38\n0.25\n0.44\n1.00\n0.16\n0.25\nPhi-3-mini\n0.04\n-0.07\n0.05\n0.43\n0.16\n1.00\n0.11\nLlama-3-8B\n-0.07\n0.17\n0.23\n0.38\n0.25\n0.11\n1.00\nTakeaway II. The analysis reveals substantial vari-\nability in sensitivity patterns across models, with some\nshowing higher alignment (e.g., models from similar\narchitectures like GPT-4 and GPT-3.5) while others\ndemonstrate divergent sensitivity behaviors. DBSA\neffectively identifies these discrepancies, offering a reli-\nable tool for practitioners to evaluate model robustness\nand make informed decisions when considering model\nreplacements or optimizations.\n6.3\nExperiment III: Evaluating similarity\nfunctions\nSetup. We test three similarity metrics (Cosine, L1,\nL2) to verify if DBSA’s sensitivity analysis is consistent\nacross metrics. We measure the Spearman correlation\nof token ω scores across LLMs to ensure comparable\nresults (see Fig, 3).\nS(Cosine, L1)\nS(Cosine, L2)\nS(L1, L2)\nSimilarity Measures\n0.95\n1.00\nSpearman \n Correlation\nSpearman Correlation Across Models\nFigure 3: Comparison of similarity functions (Co-\nsine, L1, L2) in DBSA. We show the average Spear-\nman correlation across models (blue) and individual\nmodels (grey). Results indicate all three measures yield\nconsistent outcomes, demonstrating their interchange-\nability within the DBSA framework.\nTakeaway III. All three similarity metrics produce\nsimilar results, demonstrating DBSA’s robustness. This\nconfirms that practitioners can choose any standard\nmetric without compromising the quality of analysis,\noffering flexibility in deploying DBSA.\n6.4\nExperiment IV: Evaluating the effect of\nMonte Carlo samples\nSetup.\nWe assess how varying Monte Carlo (MC)\nsample sizes affects DBSA’s stability and reliability. We\nmeasure Spearman correlation between model outputs\n"}, {"page": 8, "text": "Visualizing token importance for black-box language models\n3\n5\n10\n20\n100\nMC Samples\n0.0\n0.2\n0.4\n0.6\nSpearman\n (GPT-4 and GPT-35)\nExample I.\n3\n5\n10\n20\n100\nMC Samples\nExample II.\n3\n5\n10\n20\n100\nMC Samples\nExample III.\nFigure 4: Stabilization of Spearman correlation\nbetween GPT-3.5 and GPT-4 with increasing\nMonte Carlo (MC) samples for three prompts.\nThe figure shows that with fewer MC samples, the\nSpearman correlation fluctuates, indicating variabil-\nity. As MC samples increase, the correlation stabilizes,\nshowing greater reliability.\n(e.g., GPT-4, GPT-3.5) to determine if larger sample\nsizes stabilize results across three prompts (Fig. 4).\nTakeaway IV. As MC samples increase, DBSA’s out-\nputs stabilize across larger models, confirming the im-\nportance of sample size for accuracy. This shows that\nDBSA’s reliability can be tuned based on sample quan-\ntity, offering guidelines for practitioners. This experi-\nment establishes that DBSA provides consistent\n6.5\nOther empirical insights\nWe perform evaluation on other components, such as\nperforming ablations by evaluating different distance\nfunctions (Energy, Earth Mover’s Distance, or mean\ndistance of cosine similarities) and the effect of choosing\na different number of nearest neighbors (Appendix C).\n7\nRelated work\nExisting methods for evaluating the sensitivity and\nbehavior of large language models (LLMs) primarily\ninclude gradient-based approaches, bias measurement,\ncounterfactual fairness, and text summarization met-\nrics. Gradient-based methods (Geisler et al., 2024)\nrequire access to model internals, making them imprac-\ntical for black-box models, and focus on performance\nrather than interpretability. Bias easurement tech-\nniques (Borkan et al., 2019; Dixon et al., 2018; Park,\nShin, and Fung, 2018) quantify fairness but are limited\nto specific subgroups and require human annotation,\nwhich restricts their flexibility and applicability to ar-\nbitrary perturbations. Counterfactual fairness ap-\nproaches (Garg et al., 2019) commonly rely on specific\nassumptions, labeled data, and fundamentally answer\na different question, i..e. that of fairaness. Text sum-\nmarization metrics such as ROUGE and BERTScore\n(Bhandari et al., 2020; Zhang et al., 2019; Zhao et al.,\n2019; Lin, 2004) assess text generation quality but are\nnot designed for inspecting output variability for input\nperturbations.\nDBSA is a tangential auditing framework to the above-\nmentioned frameworks which answers a fundamentally\ndifferent question. Because this paper defines an en-\ntirely new task—that of developing a model-agnostic\nframework for token-level sensitivity — we encourage\npractitioners to use DBSA in addition to any existing\ntechniques that are used to audit LLM models. This\nis useful, as DBSA supports statistical inference, com-\nputes effect sizes, and allows exploration of any input\nperturbation, offering a human-interpretable approach\nthat prioritizes qualitative understanding over bench-\nmark performance. This is also important, as there\nhave been direct calls for practical mechanisms to au-\ndit language models in addition to existing methods\nMökander et al., 2023; Meskó and Topol, 2023. Ta-\nble 5 summarizes these methods across five criteria.\nExtended related work can be found in Appendix A.\n8\nDiscussion\nThe need for practical algorithms to audit LLMs.\nThere is a growing need for practical algorithms that\ncan be used to audit LLMs.\nDBSA is a versatile,\nmodel-agnostic framework designed for practitioners\nwho wish to understand how sensitive the model is to\ninformation in the prompt. DBSA addresses a new task\nwithin the LLM auditing literature that other methods\nhave so far overlooked.\nWe see DBSA functioning\nas a diagnostic instrument that complements existing\nframeworks, especially in high-stakes settings. DBSA\nhas immediate practical relevance to many practitioners\nwho lack practical tools to audit LLMs. With this, we\nhope our work lays the foundation for a new class of\nstatistics-based auditing tools.\nLimitations. DBSA requires access to an embedding\nfunction and the ability to sample from the LLM mul-\ntiple times which could limit its accessibility to some\npractitioners. We outline token-level sensitivity chal-\nlenges in Sec. 2. While our solution in Sec. 3 addresses\nthese, it has limitations, such as finding reliable nearest\nneighbors (which may vary in magnitude in embedding\nspace) or selecting appropriate distance metrics/em-\nbedding functions. Finally, we wish to acknowledge\nthe importance of ensuring that DBSA is deployed re-\nsponsibly and does not inadvertently enable malicious\nactors to exploit vulnerabilities in LLMs.\nDirections for Future Work (1) DBSA could be ex-\ntended to have a multi-level approach to interpretability.\nWe think this is a non-trivial (but extremely fruitful)\nextension of our current work. Such an extension would\nhave to address at least five different non-trivial chal-\nlenges: (i) which levels are worth testing (multiple to-\nkens? semantic meaning? sentence-level?); (ii) the mu-\ntual information between levels and their interactions\n(we cannot assume independence; otherwise, this exten-\n"}, {"page": 9, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nTable 5: Comparison to related work. DBSA addresses the entirely new task of generalized sensitivity\nanalysis for black-box LLMs for which there are no ground-truth labels. This differs from other sensitivity- or\ninterpretability-based methods. Abbreviations: (I): Usable on any black-box model; (II): Enables statistical\ninference; (III): Computes the effect size; (IV): Assumption-free\nMethod\nExample Works\n(I)\n(II)\n(III)\n(IV)\nRepresentative Question\nGradient Methods\n(Geisler et al., 2024)\n✗\n✓\n✗\n✓\nHow does model output change under\ninfinitesimal perturbation?\nMeasuring\nUnin-\ntended Bias\n(Borkan et al., 2019; Dixon et al., 2018;\nPark, Shin, and Fung, 2018)\n✓\n✓\n✓\n✗\nDoes this model have unintended bi-\nases in certain subgroups?\nCounterfactual Fair-\nness\n(Garg et al., 2019)\n✗\n✗\n✓\n✗\nHow would the prediction change if the\nsensitive attribute were different?\nText\nSummariza-\ntion\n(Bhandari et al., 2020; Zhang et al.,\n2019; Zhao et al., 2019; Lin, 2004)\n✗\n✗\n✓\n✓\nHow well is this text summarized?\nSensitivity analysis\nfor LLMs\nOur work (DBSA)\n✓\n✓\n✓\n✓\nDo the responses change if we change\nany input in the prompt? If so, how?\nsion loses its meaning); (iii) dealing with higher variance\nestimators (due to the non-zero covariances between\nlevels); (iv) dealing with semantic drift (i.e. change in\nthe meaning of the input across multiple places); and\n(v) repurposing the highlighting methodology (e.g. our\ncurrent work offering highlights of tokens is extremely\nintuitive — how can this be achieved otherwise?). (2)\nVariance decomposition. Token probabilities could fa-\ncilitate a formal decomposition of variability in output\ndistributions, akin to an ANOVA-style partitioning\nof variance. Specifically, this could disentangle within-\nprompt variability (changes due to perturbations within\na given input) from between-prompt variability (differ-\nences arising from distinct inputs). Such an approach\ncould quantify the relative contributions of perturba-\ntions at various levels—be it tokens, phrases, or broader\ninput features—to the overall stochastic behavior of\nthe model. (3) Bayesian sensitivity. Token probabili-\nties could be used within a Bayesian framework, where\nthey serve as priors to model the effects of input per-\nturbations. By updating these priors with observed\noutputs, DBSA could estimate posterior distributions,\nproviding a probabilistic characterization of how input\nmodifications alter the likelihood of specific outputs.\nReferences\nAstorga, Nicolás et al. (2024). “Active learning with\nllms for partially observed and cost-aware scenar-\nios”. In: Advances in Neural Information Processing\nSystems 37, pp. 20819–20857.\nBelrose, Nora et al. (2023). “Eliciting latent predictions\nfrom transformers with the tuned lens”. In: arXiv\npreprint arXiv:2303.08112.\nBhandari, Manik et al. (2020). “Re-evaluating eval-\nuation in text summarization”. In: arXiv preprint\narXiv:2010.07100.\nBorkan, Daniel et al. (2019). “Nuanced metrics for\nmeasuring unintended bias with real data for text\nclassification”. In: Companion proceedings of the 2019\nworld wide web conference, pp. 491–500.\nClark, Kevin (2019). “What Does Bert Look At? An\nAnalysis of Bert’s Attention”. In: arXiv preprint\narXiv:1906.04341.\nCowgill, Bo and Catherine Tucker (2017). “Algorith-\nmic bias: A counterfactual perspective”. In: NSF\nTrustworthy Algorithms 3.\nDafoe, Allan (2018). “AI governance: a research\nagenda”. In: Governance of AI Program, Future of\nHumanity Institute, University of Oxford: Oxford,\nUK 1442, p. 1443.\nDixon, Lucas et al. (2018). “Measuring and mitigat-\ning unintended bias in text classification”. In: Pro-\nceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society, pp. 67–73.\nEnguehard, Joseph (2023). “Sequential Integrated\nGradients: a simple but effective method for ex-\nplaining language models”. In: arXiv preprint\narXiv:2305.15853.\nGallegos, Isabel O et al. (2024). “Bias and fairness in\nlarge language models: A survey”. In: Computational\nLinguistics, pp. 1–79.\nGao, Bo (2015). “Exploratory visualization design to-\nwards online social network privacy and data liter-\nacy”. In.\n"}, {"page": 10, "text": "Visualizing token importance for black-box language models\nGarg, Sahaj et al. (2019). “Counterfactual fairness\nin text classification through robustness”. In: Pro-\nceedings of the 2019 AAAI/ACM Conference on AI,\nEthics, and Society, pp. 219–226.\nGehman, Samuel et al. (2020). “Realtoxicityprompts:\nEvaluating neural toxic degeneration in language\nmodels”. In: arXiv preprint arXiv:2009.11462.\nGeisler, Simon et al. (2024). “Attacking large language\nmodels with projected gradient descent”. In: arXiv\npreprint arXiv:2402.09154.\nGhandeharioun, Asma et al. (2024). “Patchscope: A\nunifying framework for inspecting hidden represen-\ntations of language models”. In: arXiv preprint\narXiv:2401.06102.\nHadi, Muhammad Usman et al. (2023). “A survey on\nlarge language models: Applications, challenges, limi-\ntations, and practical usage”. In: Authorea Preprints\n3.\nHelberger, Natali, Nicholas Diakopoulos, et al. (2023).\n“ChatGPT and the AI Act”. In: Internet Policy Re-\nview 12.1, pp. 1–6.\nJung, Jongbin et al. (2018). “Algorithmic decision mak-\ning in the presence of unmeasured confounding”. In:\narXiv preprint arXiv:1805.01868.\nLauer, Dave (2021). “You cannot have AI ethics without\nethics”. In: AI and Ethics 1.1, pp. 21–25.\nLin, Chin-Yew (2004). “Rouge: A package for automatic\nevaluation of summaries”. In: Text summarization\nbranches out, pp. 74–81.\nLiu, Tennison et al. (2024). “Large language models to\nenhance bayesian optimization”. In: arXiv preprint\narXiv:2402.03921.\nLundberg, SM and SI Lee (n.d.). A unified approach to\ninterpreting model predictions. NIPS’17: Proceedings\nof the 31st International Conference on Neural Infor-\nmation Processing Systems. December 2017 [Cited\n2021 Jul 20].\nMeskó, Bertalan and Eric J Topol (2023). “The im-\nperative for regulatory oversight of large language\nmodels (or generative AI) in healthcare”. In: NPJ\ndigital medicine 6.1, p. 120.\nMohri, Christopher and Tatsunori Hashimoto (2024).\n“Language models with conformal factuality guaran-\ntees”. In: arXiv preprint arXiv:2402.10978.\nMökander, Jakob et al. (2023). “Auditing large lan-\nguage models: a three-layered approach”. In: AI and\nEthics, pp. 1–31.\nMontavon, Grégoire et al. (2017). “Explaining nonlinear\nclassification decisions with deep taylor decomposi-\ntion”. In: Pattern recognition 65, pp. 211–222.\nMorris, John X et al. (2023). “Text embeddings re-\nveal (almost) as much as text”. In: arXiv preprint\narXiv:2310.06816.\nNozza, Debora, Federico Bianchi, Dirk Hovy, et al.\n(2021). “HONEST: Measuring hurtful sentence com-\npletion in language models”. In: Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies. Association for Compu-\ntational Linguistics.\nPark, Ji Ho, Jamin Shin, and Pascale Fung (2018).\n“Reducing gender bias in abusive language detection”.\nIn: arXiv preprint arXiv:1808.07231.\nRahwan, Iyad (2018). “Society-in-the-loop: program-\nming the algorithmic social contract”. In: Ethics and\ninformation technology 20.1, pp. 5–14.\nRaji, Inioluwa Deborah et al. (2022). “Outsider over-\nsight: Designing a third party audit ecosystem for ai\ngovernance”. In: Proceedings of the 2022 AAAI/ACM\nConference on AI, Ethics, and Society, pp. 557–571.\nRauba, Paulius, Nabeel Seedat, Krzysztof Kacprzyk, et\nal. (2024). “Self-healing machine learning: A frame-\nwork for autonomous adaptation in real-world en-\nvironments”. In: Advances in Neural Information\nProcessing Systems 37, pp. 42225–42267.\nRauba, Paulius, Nabeel Seedat, Max Ruiz Luyten, et\nal. (2024). “Context-aware testing: A new paradigm\nfor model testing with large language models”. In:\nAdvances in Neural Information Processing Systems\n37, pp. 112505–112553.\nRauba, Paulius, Qiyao Wei, and Mihaela van der\nSchaar (2024). “Quantifying perturbation impacts\nfor large language models”. In: arXiv preprint\narXiv:2412.00868.\nRibeiro, Marco Tulio, Sameer Singh, and Carlos\nGuestrin (2016). “\" Why should i trust you?\" Explain-\ning the predictions of any classifier”. In: Proceedings\nof the 22nd ACM SIGKDD international conference\non knowledge discovery and data mining, pp. 1135–\n1144.\nSikdar, Sandipan, Parantapa Bhattacharya, and Kieran\nHeese (2021). “Integrated directional gradients: Fea-\nture interaction attribution for neural NLP models”.\nIn: Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp. 865–878.\nSingh, Chandan et al. (2024). “Rethinking interpretabil-\nity in the era of large language models”. In: arXiv\npreprint arXiv:2402.01761.\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan\n(2017). “Axiomatic attribution for deep networks”.\nIn: International conference on machine learning.\nPMLR, pp. 3319–3328.\nWang, Haotao et al. (2023). “How robust is your fair-\nness? evaluating and sustaining fairness under unseen\ndistribution shifts”. In: Transactions on machine\nlearning research 2023.\n"}, {"page": 11, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nWebster, Kellie et al. (2020). “Measuring and reduc-\ning gendered correlations in pre-trained models”. In:\narXiv preprint arXiv:2010.06032.\nWen, Yuxin et al. (2024). “Hard prompts made easy:\nGradient-based discrete optimization for prompt tun-\ning and discovery”. In: Advances in Neural Informa-\ntion Processing Systems 36.\nYuksekgonul, Mert et al. (2024). “TextGrad: Auto-\nmatic\" Differentiation\" via Text”. In: arXiv preprint\narXiv:2406.07496.\nZeiler, Matthew D and Rob Fergus (2014). “Visualiz-\ning and understanding convolutional networks”. In:\nComputer Vision–ECCV 2014: 13th European Con-\nference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part I 13. Springer, pp. 818–833.\nZhang, Tianyi et al. (2019). “Bertscore: Evaluat-\ning text generation with bert”. In: arXiv preprint\narXiv:1904.09675.\nZhao, Wei et al. (2019). “MoverScore: Text gener-\nation evaluating with contextualized embeddings\nand earth mover distance”. In: arXiv preprint\narXiv:1909.02622.\nZou, Andy et al. (2023). “Representation engineering:\nA top-down approach to ai transparency”. In: arXiv\npreprint arXiv:2310.01405.\nChecklist\n1. For all models and algorithms presented, check if\nyou include:\n(a) A clear description of the mathematical set-\nting, assumptions, algorithm, and/or model.\n[Yes] We include a clear description of the\nmathematical setting in Section 2, and appro-\npriate algorithmic details.\n(b) An analysis of the properties and complexity\n(time, space, sample size) of any algorithm.\n[Yes] We analyze challenges including compu-\ntation complexity in Section 2.\n(c) (Optional) Anonymized source code, with\nspecification of all dependencies, including\nexternal libraries. [No] All source code will\nbe made available upon publication.\n2. For any theoretical claim, check if you include:\n(a) Statements of the full set of assumptions of all\ntheoretical results. [Yes] We include a clear\nstatement of assumptions in Section 2.\n(b) Complete proofs of all theoretical results. [Not\nApplicable]\n(c) Clear explanations of any assumptions. [Yes]\nWe include a clear explanation in Section 2\n3. For all figures and tables that present empirical\nresults, check if you include:\n(a) The code, data, and instructions needed to re-\nproduce the main experimental results (either\nin the supplemental material or as a URL).\n[Yes] We include the data used in this experi-\nment, and we discuss the model design choices\nthat are made. All source code and reproduc-\ntion instructions will be made available upon\npublication.\n(b) All the training details (e.g., data splits, hy-\nperparameters, how they were chosen). [Yes]\nAll experiment details are described in Section\n5.\n(c) A clear definition of the specific measure or\nstatistics and error bars (e.g., with respect to\nthe random seed after running experiments\nmultiple times). [Yes] We clearly describe the\nfunctions and metrics we are using.\n(d) A description of the computing infrastructure\nused. (e.g., type of GPUs, internal cluster,\nor cloud provider). [Yes] We describe further\nexperiment settings, including the compute\nused, in the appendix.\n4. If you are using existing assets (e.g., code, data,\nmodels) or curating/releasing new assets, check if\nyou include:\n(a) Citations of the creator If your work uses exist-\ning assets. [Yes] We appropriately document\nthe models that we are using.\n(b) The license information of the assets, if appli-\ncable. [Yes] We appropriately document the\nlicense information.\n(c) New assets either in the supplemental mate-\nrial or as a URL, if applicable. [Yes] We have\nfurther specification in the supplementary ma-\nterials.\n(d) Information about consent from data provider-\ns/curators. [Not Applicable] We do not collect\ndata from external sources.\n(e) Discussion of sensible content if applicable,\ne.g., personally identifiable information or of-\nfensive content. [Not Applicable] There are\nno sensible contents in this paper.\n5. If you used crowdsourcing or conducted research\nwith human subjects, check if you include:\n(a) The full text of instructions given to partici-\npants and screenshots. [Not Applicable] We\ndo not use crowdsourcing or conducted re-\nsearch with human subjects.\n(b) Descriptions of potential participant risks,\nwith links to Institutional Review Board (IRB)\napprovals if applicable. [Not Applicable] We\ndo not use crowdsourcing or conducted re-\nsearch with human subjects.\n"}, {"page": 12, "text": "Visualizing token importance for black-box language models\n(c) The estimated hourly wage paid to partici-\npants and the total amount spent on partici-\npant compensation. [Not Applicable] We do\nnot use crowdsourcing or conducted research\nwith human subjects.\n"}, {"page": 13, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nSupplementary Materials\nAppendix Contents\n1. Extended Related Work\n2. Implementation Details\n3. Extended Experiments\n4. Experimental Details\n5. Prompt Examples\n"}, {"page": 14, "text": "Visualizing token importance for black-box language models\nA\nExtended related work\nA.1\nFeature Attribution\nThe most relevant statistical methods for interpretability fall under \"Feature attribution\", such as SHAPLEY\nvalues, conformal prediction, etc. However, these methods are currently limited to the closed-end text generation\ndomain, requires standardized text input/output formats and generation formats, and/or ground truth answers.\nIntuitively, these methods come from the statistical machine learning domain, and therefore are much better\nsuited to e.g. tabular prediction tasks. Their application on LLMs is an adaptation rather than a new method\noverall. To the best of our knowledge, there is not a well-founded theory for SHAPLEY values in LLMs, especially\nthe open-end text generation domain. On the other hand, there is one paper on the theory of conformal prediction\nin LLMs, but not related to perturbations (Mohri and Hashimoto, 2024).\nMore specifically, feature attribution methods deserve its name because a score is assigned to each input feature,\nreflecting its impact on the generated model output. Early attribution methods that have been developed include\n(1) perturbation-based methods (Lundberg and Lee, n.d.), (2) gradient-based methods (Sundararajan, Taly, and\nYan, 2017; Montavon et al., 2017), (3) linear approximations (Ribeiro, S. Singh, and Guestrin, 2016). Recently,\nthese methods have been specifically adapted for LLMs (Sikdar, Bhattacharya, and Heese, 2021; Enguehard,\n2023). See (C. Singh et al., 2024) for a more complete literature review.\nThe gradient-inspired methods both break down the sentence into atomic elements (in language that would\nbe tokens from byte pair encodings). Then, the gradient for each token can be calculated with respect to the\nfinal answer. This method suffers the same drawbacks as SHAPLEY values, specifically requiring standardized\ninput/output text formats and/or ground truth answers. These papers demonstrate their method on closed-end\ntasks, such as text classification, prediction, etc. These methods were developed in tabular prediction tasks\nor compute vision tasks, and we have not done a really good job in adapting these methods to LLMs. An\ninteresting parallel can be made that DBSA is attempting to adapt gradient-inspired methods on any open-end\ntext generation.\nA.2\nFairness/Bias\nCertain methods in bias measurement and fairness quantification are also similar in spirit to DBSA. However,\nthese works are limited to addressing specific subgroups, and while optional, they sometimes require human\nannotation (e.g. for bias annotations), which restricts their flexibility and applicability to arbitrary perturbations.\nFor example, DisCo (Webster et al., 2020) is a seminal piece of work that proposes to look at model output\ndistributions under perturbation in the prompt. To begin, each prompt template (e.g., “[X] is [MASK]”; “[X] likes\nto [MASK]”) has two slots. The first slot is manually filled with attributes associated with a social group (the\noriginal paper featured gendered names and nouns, but easily extended to other sensitive groups with well-defined\nword lists), and the second slot is filled by the model’s top three candidate predictions. The final score is the\nnumber of different predictions between social groups across all prompt templates. Bias is measured by the\ndifference between normalized probability scores for two binary and opposing social group words. The main\nlimitation of this approach is that it only addresses specific subgroups, and the output distribution is rather\nlimited (closed-end generation). For a detailed discussion on this topic, see (Gallegos et al., 2024).\nSimilar ideas have come up under works that investigate fairness under distributional shift. For example, (Wang\net al., 2023) shows that while earlier methods proposed to adapt the model to be fair on the current known data\ndistribution, or requires unlabeled data from the target distribution (assuming the target distribution is known),\nor requires the existence of a joint causal graph to represent the data distribution for all domains, their work\naims to generalize fairness learned on current distribution to unknown and unseen target distributions. The\nfield of fairness/bias also features the use of sensitivity analysis to explore how various aspects of the feature\nvector will affect a given outcome. Due to the introduction of SHAPLEY values, there has been an increase in\nattention from fairness researchers. (Cowgill and Tucker, 2017) proposed to perturb feature vectors to measure\nthe effect on model performance of specific interventions. (Gao, 2015) investigated visual mechanisms to better\ndisplay fairness/bias issues with data to users. (Jung et al., 2018) used sensitivity analysis to evaluate sensitive\nvariables and their relationships with classification outcomes, indicating that sensitivity analysis can help to\nbetter understand uncertainty with respect to fairness. However, these methods are subject to the same problems\ndiscussed before. Namely, they are not general enough in their input/output formats, and they sometimes require\n"}, {"page": 15, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nhuman annotation (e.g. for bias annotations).\nOn an application level, we believe DBSA can directly improve the auditing of language modeling based systems to\nevaluate whether they are fair and reliable. That is, many applications today use language model as a system-level\ncomonent for solving tasks. Consider a few examples. Language models are used to generate hypotheses to test\nexisting ML systems (Rauba, Seedat, Ruiz Luyten, et al., 2024), improve information acquisition (Astorga et al.,\n2024), improve model robustness (Rauba, Seedat, Kacprzyk, et al., 2024) or even improve model hyperparameters\n(Liu et al., 2024). Many such applications exist, too broad to be covered here (Hadi et al., 2023). In most such\nworks, the limitation section contains words of caution against fairness and bias—how can language models be\ntrusted in such cases when they are used as external systems? Our work proposes a direct way to perform such\naudits.\nA.3\nDirect Prompting\nDirect prompting methods generally work by either prompting along the lines of \"Can you explain your logic\"\nto an LLM, or in the case of Chain-of-Thought (CoT), goes back to changing specific values in the prompt in\ntasks like mathematical question-answering. In general, these methods either requires human intervention and\nexamination, or ground truth answers labels, e.g. for process supervision, in context learning, RAG, etc. There\nhas been efforts to make prompting methods gradient-inspired, see (Yuksekgonul et al., 2024) for an example.\nEarly adversarial attacks on LLMs apply simple character or token operations to trigger the LLM to generate\nincorrect predictions. Since these attacks usually generate misspelled prompts, they are easy to block in real-world\napplications. More recently, jailbreaking prompts are intentionally designed to bypass the LLM built-in safeguard\ncapabilities, eliciting the generation of harmful content. However, the discrete nature of text has significantly\nimpeded learning more effective adversarial attacks against LLMs. Even for recent work that has developed\ngradient-based optimizers for efficient text modality attacks ((Wen et al., 2024) presented a gradient-based\ndiscrete optimizer that is suitable for attacking the text pipeline of CLIP), they still require access to a white-box\nlanguage model, and is limited in their input/output formats.\nOther interpretability methods.\nWe end the discussion on related works by documenting other interpretability\nmethods that are also interesting approaches, but not immediately extended to the DBSA research. One promising\nmethod for understanding LLM representations is by looking at their attention heads (Clark, 2019), embeddings\n(Morris et al., 2023), and representations (Zou et al., 2023). There are also methods that directly decode an output\ntoken to understand what is represented at different positions and layers (Belrose et al., 2023; Ghandeharioun\net al., 2024). Finally, influence functions are interesting to study LLM behavior, but the idea of influence\nfunctions is limited to the impact on adding one example on the training set. Also, influence functions require\nthe derivative, i.e. white-box access to LLMs. In general, methods in mechanistic interpretability (e.g. Attention\nhead importance, circuit analysis, influence functions) are interesting approaches to address LLM interpretability,\nbut none of them are easily extended to the DBSA framework, and we leave comparison to these methods for\nfuture work.\n"}, {"page": 16, "text": "Visualizing token importance for black-box language models\nB\nImplementation details\nWe provide a comprehensive guide for implementing Distribution-Based Sensitivity Analysis (DBSA). This section\noutlines the step-by-step process with specific code snippets and technical considerations.\nB.1\nPrerequisites and Setup\nOur implementation uses Python 3.8 with NumPy, SciPy, Scikit-learn, and sentence-transformers libraries. We\nassume access to an LLM through a function llm(prompt, n) that generates n responses given a prompt.\nB.2\nText Preprocessing and Tokenization\nWe tokenize the input text using a regular expression that captures words, numbers, and punctuation:\ndef tokenize_and_prepare_for_scoring(text):\ntokens = re.findall(r’\\$?\\d+(?:,\\d+)*(?:\\.\\d+)?|\\w+|[^\\w\\s]’, text)\ntoken_positions = OrderedDict()\nfor index, token in enumerate(tokens):\nif token not in token_positions:\ntoken_positions[token] = {’positions’: [], ’score’: None, ’pval’: None}\ntoken_positions[token][’positions’].append(index)\nreturn tokens, token_positions\nB.3\nSampling and Perturbing Responses\nKey Point: We sample multiple responses for both original and perturbed prompts to approximate their\ndistributions.\ndef get_responses(prompt, n=40):\nreturn llm(prompt, n=n)\ndef perturb_sentence(tokens, perturb_index, neighbor):\nnew_tokens = tokens.copy()\nnew_tokens[perturb_index] = neighbor\nreturn new_tokens\nThere are many ways to generate perturbations. In our example, we generate perturbations by asking an external\nLLM to generate k synonyms. Other possibilities include relying on external libraries, e.g. word2vec.\nEmbeddings. We use the OpenAI sentence transformer model ”text-embedding-ada-002” to generate embeddings.\nB.4\nComputing Energy Distance\nKey Point: We use energy distance as our metric for comparing distributions.\ndef compute_energy_distance(X, Y, distance=’cosine’):\nn, m = len(X), len(Y)\ndists_XY = cdist(X, Y, distance)\ndists_XX = cdist(X, X, distance)\ndists_YY = cdist(Y, Y, distance)\nterm1 = (2.0 / (n * m)) * np.sum(dists_XY)\nterm2 = (1.0 / n**2) * np.sum(dists_XX)\nterm3 = (1.0 / m**2) * np.sum(dists_YY)\nreturn term1 - term2 - term3\n"}, {"page": 17, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nB.5\nPerforming Statistical Tests\nWe use a permutation test to calculate p-values:\ndef permutation_test_energy(X, Y, num_permutations=500, distance=’cosine’):\ncombined = np.vstack((X, Y))\nn = len(X)\nE_values = []\nfor _ in range(num_permutations):\nnp.random.shuffle(combined)\nperm_X, perm_Y = combined[:n], combined[n:]\nE_perm = compute_energy_distance(perm_X, perm_Y, distance)\nE_values.append(E_perm)\nreturn np.array(E_values)\ndef compute_energy_distance_fn(baseline_embeddings, perturbed_embeddings):\nE_n = compute_energy_distance(baseline_embeddings, perturbed_embeddings)\nE_values = permutation_test_energy(baseline_embeddings, perturbed_embeddings)\np_value = np.mean(E_values >= E_n)\nreturn E_n, p_value\nB.6\nAlternative Implementations\nAlternatively, we could combine the distance calculation + statistical test into one step:\ndef calculate_difference_and_pvalue(arr1, arr2, num_permutations=1000, mode=\"energy\"):\nif mode == \"mean\":\nobserved_diff = np.abs(np.mean(arr1) - np.mean(arr2))\nelif mode == \"EMD\":\nobserved_diff = wasserstein_distance(arr1, arr2)\nelif mode == \"energy\":\nobserved_diff = energy_distance(arr1, arr2)\nelse:\nraise ValueError(f\"Invalid mode: {mode}\")\ncombined = np.concatenate([arr1, arr2])\nn1, n2 = len(arr1), len(arr2)\npermutation_diffs = []\nfor _ in range(num_permutations):\nnp.random.shuffle(combined)\npermuted_arr1, permuted_arr2 = combined[:n1], combined[n2:]\nif mode == \"mean\":\npermuted_diff = np.mean(permuted_arr1) - np.mean(permuted_arr2)\nelif mode == \"EMD\":\npermuted_diff = wasserstein_distance(permuted_arr1, permuted_arr2)\nelif mode == \"energy\":\npermuted_diff = energy_distance(permuted_arr1, permuted_arr2)\nelse:\nraise ValueError(f\"Invalid mode: {mode}\")\npermutation_diffs.append(permuted_diff)\np_value = np.mean(np.abs(permutation_diffs) >= np.abs(observed_diff))\nreturn observed_diff, p_value\nOptimization. For improved efficiency, especially with longer texts, we implement parallel processing using\nPython’s multiprocessing module to distribute token perturbation and analysis across available CPU cores.\n"}, {"page": 18, "text": "Visualizing token importance for black-box language models\nC\nExtended experiments\nPurpose. This section provides additional insights into the similarity between choosing different distance\nfunctions for the energy-based calculations across different models. We run each model with different distances\nand evaluate the Spearman rank correlation between the given token outputs, where 1 indicates perfect ranking-\nbased correlaction and 0 indicates no relationship.\nDiscussion. Across all models, we find that the distance metrics consistently rank the same words as important\nfor DBSA.\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.99\n0.99\n0.99\n1.00\n1.00\n0.99\n1.00\n1.00\n0.994\n0.995\n0.996\n0.997\n0.998\n0.999\n1.000\n(a) GPT-3.5\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.97\n0.98\n0.97\n1.00\n0.99\n0.98\n0.99\n1.00\n0.970\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\n(b) GPT-4\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.96\n0.97\n0.96\n1.00\n1.00\n0.97\n1.00\n1.00\n0.965\n0.970\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\n(c) SmolLM-135M\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.9975\n0.9980\n0.9985\n0.9990\n0.9995\n1.0000\n(d) MagicPrompt-Stable-Diffusion\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.99\n0.99\n0.99\n1.00\n1.00\n0.99\n1.00\n1.00\n0.990\n0.992\n0.994\n0.996\n0.998\n1.000\n(e) Phi-3-mini-4k-instruct\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.98\n0.98\n0.98\n1.00\n1.00\n0.98\n1.00\n1.00\n0.9800\n0.9825\n0.9850\n0.9875\n0.9900\n0.9925\n0.9950\n0.9975\n1.0000\n(f) Mistral-7B-Instruct-v0.2\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.98\n0.98\n0.98\n1.00\n1.00\n0.98\n1.00\n1.00\n0.984\n0.986\n0.988\n0.990\n0.992\n0.994\n0.996\n0.998\n1.000\n(g) Meta-Llama-3.1-8B-Instruct\ncosine\nl1\nl2\ncosine\nl1\nl2\n1.00\n0.98\n0.99\n0.98\n1.00\n1.00\n0.99\n1.00\n1.00\n0.986\n0.988\n0.990\n0.992\n0.994\n0.996\n0.998\n1.000\n(h) gemma-2-9b-it\nFigure 5: Similarity functions for various models\n"}, {"page": 19, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nD\nExperimental Details\nD.1\nAlgorithm\nThe following is an implementation of DBSA that is used for evaluating the sensitivity of each word. We use the\nsame notation as in the main part of the paper.\nAlgorithm 1: Distribution-Based Sensitivity Analysis\nRequire: Input sequence x = (w1, . . . , wn); sample sizes n, m; nearest neighbors k; embedding function\nϕ : Y →Rd; similarity function s : Rd × Rd →R; distance function D : Rd × Rd →R\nEnsure: Sensitivity scores {Ew, pw}w∈T\n1: T ←{w | w ∈x}\n// Unique tokens\n2: for all w ∈T do\n3:\nIw ←{i | ti = w}\n// Token positions\n4:\nfor all i ∈Iw do\n5:\nNk(ti) ←arg minw′∈X,w′̸=ti{D(ϕ(w′), ϕ(ti))}k\n6:\nfor all t′\ni ∈Nk(ti) do\n7:\nx(t′\ni) ←(w1, . . . , wi−1, t′\ni, wi+1, . . . , wn)\n8:\nY ←{y(j)}n\nj=1 ∼S(x), Y ′ ←{y′(j)}m\nj=1 ∼S(x(t′\ni))\n9:\nΦ ←{ϕ(y(j))}n\nj=1, Φ′ ←{ϕ(y′(j))}m\nj=1\n10:\nA(w′\ni)\nwi\n←\n2\nnm\nPn\na=1\nPm\nb=1 s(ϕa, ϕ′\nb)\n11:\nB(w′\ni)\nwi\n←\n1\nn2\nPn\na=1\nPn\nb=1 s(ϕa, ϕb)\n12:\nC(w′\ni)\nwi\n←\n1\nm2\nPm\na=1\nPm\nb=1 s(ϕ′\na, ϕ′\nb)\n13:\nE(w′\ni)\nwi\n←A(w′\ni)\nwi\n−B(w′\ni)\nwi\n−C(w′\ni)\nwi\n14:\np(w′\ni)\nwi\n←PermutationTest(Φ ∪Φ′, E(w′\ni)\nwi )\n15:\nend for\n16:\n¯Ewi ←1\nk\nP\nt′\ni∈Nk(ti) E(w′\ni)\nwi , ¯pwi ←1\nk\nP\nt′\ni∈Nk(ti) p(w′\ni)\nwi\n17:\nend for\n18:\nEw ←\n1\n|Iw|\nP\ni∈Iw ¯Ewi, pw ←\n1\n|Iw|\nP\ni∈Iw ¯pwi\n19: end for\n20: return {Ew, pw}w∈T\nD.2\nLanguage models used\nCompute Resources.\nAll the experiments in this paper were carried out on an A100 machine, with NVIDIA\ndriver version 535.183.06 and CUDA12. The GPU VRAM is 80GB.\nLanguage Models.\nA total of 8 language models were used for the experiments of this paper\n• GPT-3.5. The GPT-3.5 model is a deployment taken from OpenAI endpoints. The deployment version is\ngpt-35-1106.\n• GPT-4. The GPT-4 model is a deployment taken from OpenAI endpoints. The deployment version is\ngpt-4-0613-20231016.\n• SmolLM-135M. The SmolLM-135M model is taken from Huggingface. The full model specification is\nHuggingFaceTB/SmolLM-135M.\n• MagicPrompt-Stable-Diffusion. The MagicPrompt-Stable-Diffusion model is taken from Huggingface.\nThe full model specification is Gustavosta/MagicPrompt-Stable-Diffusion.\n• Phi-3-mini-4k-instruct. The Phi-3-mini-4k-instruct model is taken from Huggingface. The full model\nspecification is microsoft/Phi-3-mini-4k-instruct.\n"}, {"page": 20, "text": "Visualizing token importance for black-box language models\n• Mistral-7B-Instruct-v0.2. The Mistral-7B-Instruct-v0.2 model is taken from Huggingface. The full model\nspecification is mistralai/Mistral-7B-Instruct-v0.2.\n• Meta-Llama-3.1-8B-Instruct. The Meta-Llama-3.1-8B-Instruct model is taken from Huggingface. The\nfull model specification is meta-llama/Meta-Llama-3.1-8B-Instruct.\n• gemma-2-9b-it. The gemma-2-9b-it model is taken from Huggingface. The full model specification is\ngoogle/gemma-2-9b-it.\nD.3\nHyperparameter details\nIn our experiments with DBSA, we conducted ablations for most hyperparameters, as detailed in the main\npaper. However, it’s challenging to be fully exhaustive. The following list outlines the key hyperparameters and\nconsiderations for each step of the DBSA process:\n1. LLM Sampling: This step involves standard LLM sampling hyperparameters. We set the temperature\nto 1 and the max-length to 256. For more details on these and other sampling parameters, refer to the\nHuggingface documentation.\n2. Perturbation Generation: This step requires selecting a perturbation method (e.g., synonyms, antonyms)\nand a function to generate these perturbations. In our experiments, we defined sensitivity as nearest-neighbor\nsensitivity to approximate gradients of the input prompt. We used GPT-4 to generate synonyms for each\nword, finding it empirically superior to alternatives like word2vec.\n3. Perturbed Prompt Sampling: This step uses the same hyperparameters as Step 1.\n4. Embedding: We used OpenAI’s Ada embeddings, as they effectively capture meaningful semantic information\nin the prompts.\n5. Distance Calculation: The selection of a distance function is key to measuring the disparity between\nresponse embeddings. Our experiments showed that cosine, L1, and L2 distances yielded similar results. We\nopted for cosine distance in our final experiments.\n6. Statistical Analysis: The choice of statistical metric for calculating effect size between distributions during\npermutation testing is important. We selected the energy distance as our metric for effect size.\n"}, {"page": 21, "text": "Paulius Rauba∗, Qiyao Wei∗, Mihaela van der Schaar\nE\nPrompt examples\nE.1\nPrompts\nThe following prompts were the primary prompts used to evaluate DBSA sensitivity.\nListing 1: Prompt example 1\n1 text_legal = \"\"\"Company A agrees to pay\nCompany B $10\nmillion\nfor\ndeveloping a\nrevolutionary AI software\nwithin 12 months. If Company B fails to deliver a fully\nfunctional\nproduct by the deadline , they must\nrefund 50% of the\npayment\nand\nprovide an additional 3 months of development at no extra\ncost. However , if the\ndelay is due to circumstances\nbeyond\nCompany B’s reasonable\ncontrol , these\npenalties\nshall not apply. This\nagreement is governed by California\nlaw and any\ndisputes\nshall be resolved\nthrough\nbinding\narbitration.\"\"\"\nListing 2: Prompt example 2\n1 text_medical = \"\"\"Patient is a 45-year -old male\npresenting\nwith\nprogressive\ndyspnea on\nexertion\nover the past two weeks. On examination , patient\nappears\nmildly\ndistressed. Lower\nextremities\nshow 2+ pitting\nedema to mid -shin\nbilaterally. Chest\nX-ray shows\npulmonary\nvascular\ncongestion. Clinical\npresentation is consistent\nwith new -onset\ncongestive\nheart failure , likely due to hypertensive\nheart\ndisease.\n\"\"\"\nListing 3: Prompt example 3\n1 text_trading = \"\"\"A senior\nexecutive is accused of insider\ntrading , allegedly\nusing\nconfidential\ninformation to gain\nsubstantial\nfinancial\nbenefits . The\ndefendant\nmaintains\nhis\ninnocence , claiming\nthat all\ninvestment\ndecisions\nwere\nbased on\npublic\ndata .\"\"\"\nListing 4: Prompt example 4\n1 text_manufacturing = A manufacturing\ncompany\nwas sued for\nproducing a faulty\nproduct\nthat\ncaused\nsignificant\ninjuries to a customer .\"\"\"\n"}, {"page": 22, "text": "Visualizing token importance for black-box language models\nE.2\nExample nearest neighbors\nThis section outlines example nearest neighbors for different tokens.\nListing 5: Closest words for prompt 3\n1 closest_words = {\"Defendant\": [\"Accused\", \"Respondent\", \"Litigant\"], \".\": [\",\", \"!\", \"\n?\"], \",\": [\";\", \".\", \":\"], \"Senior\": [\"Top -tier\", \"High -ranking\", \"Upper -level\"],\n\"Executive\": [\"Administrator\", \"Officer\", \"Manager\"], \"Accused\": [\"Alleged\", \"\nCharged\", \"Indicted\"], \"Insider\": [\"Internal\", \"In -house\", \"Privileged\"], \"Trading\n\": [\"Dealing\", \"Stock -jobbing\", \"Market\nmanipulation\"], \"Allegedly\": [\"Supposedly\"\n, \"Reportedly\", \"Purportedly\"], \"Using\": [\"Utilizing\", \"Employing\", \"Applying\"], \"\nConfidential\": [\"Private\", \"Secret\", \"Classified\"], \"Information\": [\"Data\", \"\nDetails\", \"Intelligence\"], \"To\": [\"Toward\", \"For\", \"In order to\", \"So as to\"], \"\nGain\": [\"Acquire\", \"Secure\", \"Obtain\"], \"Substantial\": [\"Significant\", \"\nConsiderable\", \"Major\"], \"Financial\": [\"Monetary\", \"Fiscal\", \"Economic\"], \"\nBenefits\": [\"Advantages\", \"Gains\", \"Profits\"], \"Maintains\": [\"Affirms\", \"Asserts\",\n\"Insists\"], \"Innocence\": [\"Guiltlessness \", \" Blamelessness \", \"Purity\"], \"Claiming\"\n: [\"Asserting\", \"Stating\", \"Contending\"], \"Investment\": [\"Financial\", \"Capital\", \"\nAsset\"], \"Decisions\": [\"Choices\", \" Determinations \", \"Conclusions\"], \"Public\": [\"\nOpen\", \"Publicly\navailable\", \"Common\"], \"Data\": [\"Information\", \"Statistics\", \"\nFacts\"], \"A\": [\"An\", \"One\", \"Any\"], \"Is\": [\"Exists\", \"Stands\", \"Remains\", \"\nConstitutes\"], \"Of\": [\"Concerning\", \"Regarding\", \"About\", \"Pertaining to\"], \"Using\n\": [\"Utilizing\", \"With the help of\", \"Via\", \"By means of\"], \"To\": [\"Toward\", \"For\"\n, \"In order to\", \"So as to\"], \"The\": [\"This\", \"That\", \"Said\"], \"His\": [\"Their\", \"\nIts\", \"Her\"], \"That\": [\"Which\", \"This\", \"What\"], \"All\": [\"Every\", \"Each\", \"Any\"],\n\"Were\": [\"Had been\", \"Were\nbeing\", \"Used to be\"], \"Based\": [\"Founded\", \"\nEstablished\", \"Built\", \"Grounded\"], \"On\": [\"Upon\", \"Over\", \"About\", \"Concerning\"]}\nListing 6: Closest words for prompt 4\n1 closest_words = {\"Manufacturing\": [\"Production\", \"Industrial\", \"Fabrication\"], \"\nCompany\": [\"Corporation\", \"Firm\", \"Business\"], \"Was\": [\"Had been\", \"Was being\", \"\nWere\"], \"Sued\": [\"Prosecuted\", \"Litigated\nagainst\", \"Indicted\"], \"For\": [\"Because\nof\", \"Due to\", \"On account of\"], \"Producing\": [\"Creating\", \"Making\", \"Generating\"\n], \"A\": [\"One\", \"An\", \"Any\"], \"Faulty\": [\"Defective\", \"Damaged\", \" Malfunctioning \"\n], \"Product\": [\"Good\", \"Item\", \"Commodity\"], \"That\": [\"Which\", \"Who\", \"That\nwhich\"\n], \"Caused\": [\"Provoked\", \"Led to\", \"Resulted in\"], \"Significant\": [\"Major\", \"\nConsiderable\", \"Substantial\"], \"Injuries\": [\"Harm\", \"Damage\", \"Trauma\"], \"To\": [\"\nTowards\", \"In relation to\", \"With\nrespect to\"], \"A\": [\"One\", \"An\", \"Any\"], \"\nCustomer\": [\"Consumer\", \"Client\", \"Purchaser\"], \".\": [\",\", \"!\", \"?\"]}\n"}]}