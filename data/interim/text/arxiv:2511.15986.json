{"doc_id": "arxiv:2511.15986", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.15986.pdf", "meta": {"doc_id": "arxiv:2511.15986", "source": "arxiv", "arxiv_id": "2511.15986", "title": "Fairness in Multi-modal Medical Diagnosis with Demonstration Selection", "authors": ["Dawei Li", "Zijian Gu", "Peng Wang", "Chuhan Song", "Zhen Tan", "Mohan Zhang", "Tianlong Chen", "Yu Tian", "Song Wang"], "published": "2025-11-20T02:38:00Z", "updated": "2025-11-24T15:59:06Z", "summary": "Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.15986v2", "url_pdf": "https://arxiv.org/pdf/2511.15986.pdf", "meta_path": "data/raw/arxiv/meta/2511.15986.json", "sha256": "e8d28972b14d0293b95a237bd4a87847bdc61be707609ee360b374a9eac63e2f", "status": "ok", "fetched_at": "2026-02-18T02:26:35.465311+00:00"}, "pages": [{"page": 1, "text": "Fairness in Multi-modal Medical Diagnosis with Demonstration Selection\nDawei Li1*\nZijian Gu2*\nPeng Wang3†\nChuhan Song4†\nZhen Tan1\nMohan Zhang5\nTianlong Chen5\nYu Tian6\nSong Wang6\n1Arizona State University\n2University of Rochester\n3University of Virginia\n4UCL\n5University of North Carolina at Chapel Hill\n6University of Central Florida\nAbstract\nMultimodal large language models (MLLMs) have shown\nstrong potential for medical image reasoning, yet fair-\nness across demographic groups remains a major con-\ncern. Existing debiasing methods often rely on large la-\nbeled datasets or fine-tuning, which are impractical for\nfoundation-scale models. We explore In-Context Learning\n(ICL) as a lightweight, tuning-free alternative for improv-\ning fairness.\nThrough systematic analysis, we find that\nconventional demonstration selection (DS) strategies fail\nto ensure fairness due to demographic imbalance in se-\nlected exemplars. To address this, we propose Fairness-\nAware Demonstration Selection (FADS), which builds de-\nmographically balanced and semantically relevant demon-\nstrations via clustering-based sampling. Experiments on\nmultiple medical imaging benchmarks show that FADS con-\nsistently reduces gender-, race-, and ethnicity-related dis-\nparities while maintaining strong accuracy, offering an effi-\ncient and scalable path toward fair medical image reason-\ning. These results highlight the potential of fairness-aware\nin-context learning as a scalable and data-efficient solution\nfor equitable medical image reasoning.\n1. Introduction\nRecent advances in medical image reasoning have been\ndriven by the rapid development of multimodal large lan-\nguage models (MLLMs) [16, 31, 37, 41], which unify\nvisual perception and linguistic reasoning within a sin-\ngle foundation architecture.\nBy leveraging large-scale\nvision–language pretraining and instruction tuning, these\nmodels demonstrate impressive capabilities in medical re-\nport generation, visual question answering, and disease lo-\ncalization.\nThe ability of MLLMs to generalize across\nmodalities and clinical tasks positions them as powerful\ntools for improving diagnostic accuracy, efficiency, and ac-\n*These authors contributed equally to this work.\n†These authors also contributed equally to this work.\ncessibility in medical imaging workflows [17, 23, 35].\nHowever, fairness and equity remain fundamental chal-\nlenges in deploying medical AI responsibly. Medical imag-\ning data inherently reflect population imbalance, device het-\nerogeneity, and institutional biases, which can lead to un-\neven model performance across demographic subgroups.\nPrior studies [5, 15] have documented that such disparities\noften disadvantage underrepresented populations, resulting\nin unequal diagnostic outcomes and reduced trust in AI-\nassisted healthcare systems. Addressing fairness in MLLMs\nis particularly important because biased predictions in high-\nstakes clinical contexts can propagate or even amplify exist-\ning healthcare inequities.\nExisting\nfairness-improvement\ntechniques—such\nas\nreweighting [14], adversarial debiasing [22], and domain\nadaptation [6]—face major limitations when applied to\nMLLMs. These methods typically depend on large-scale\nlabeled data and costly retraining, which are impractical\nin medical imaging due to the scarcity of expert annota-\ntions. Moreover, foundation-scale MLLMs are often black-\nbox systems with inaccessible internal parameters, mak-\ning direct fairness regularization infeasible.\nEven when\nfine-tuning is technically possible, it may lead to catas-\ntrophic forgetting [8], erasing the broad multimodal reason-\ning ability crucial for other downstream tasks. These chal-\nlenges highlight the need for lightweight, data-efficient, and\ntuning-free strategies to improve fairness in medical image\nreasoning.\nIn this work, we explore In-Context Learning (ICL) [4,\n24, 30] as a promising and practical mechanism to enhance\nfairness without model retraining.\nICL enables adapta-\ntion through contextual exemplars, or demonstrations, pre-\nsented at inference time, allowing MLLMs to adjust behav-\nior flexibly based on provided examples. We first conduct\ncomprehensive analyses to examine how different demon-\nstration selection (DS) strategies [44]—such as random,\nsimilarity-based,\nand clustering-based methods—affect\nfairness across multiple medical imaging datasets and de-\nmographic attributes. Our findings reveal that existing DS\nheuristics cannot guarantee fairness consistency, as demo-\n1\narXiv:2511.15986v2  [cs.CV]  24 Nov 2025\n"}, {"page": 2, "text": "Challenges\nFairness\nVersatility\nChallenge I:Labeling Cost\nChallenge II: Black-box\nChallenge III:Forgetting\nAnalysis\nRandom Clustering Similarity\nLLaVA LLaVA-Med Qwen3\nPneumothorax Effusion\nEdema\nGender\nRace\nEthnicity\nFinding 1: Existing DS methods can't bring consistenty\nimprovement fairness for medical image analysis.\nFinding 2: The failure of DS methods source from data\nbias of selected demonstration in sensitive attributes.\nFairness-Aware DS\nTraining Set\nC1\nC2\nCk\ns = 0\ny = 0\ns = 0\ny = 1\ns = 1\ny = 1\ns = 0\ny = 1\nD/4 Similar Samples\nD/4 Similar Samples\nD/4 Similar Samples\nD/4 Similar Samples\nDemonstration\nSet D(x)\nK-Means\nClustering\nPartision\nAggregation\n...\nK sub-clusters\neach groups\nDS Methods\nMLLMs\nTasks\nAttributes\nFigure 1. Overview of challenges and findings. We investigate whether ICL can improve fairness in medical image reasoning. Different\ndemonstration selection (DS) strategies—Random, Similarity, and K-Means—are analyzed across attributes (Gender, Race, Ethnicity).\nEmpirical results reveal that data imbalance in selected demonstrations is the primary source of fairness degradation.\ngraphic imbalances within selected demonstrations propa-\ngate bias to model predictions. Motivated by these insights,\nwe design a new method, Fairness-Aware Demonstration\nSelection (FADS), which constructs balanced and semanti-\ncally relevant exemplars through demographic-aware clus-\ntering and subgroup-level sampling.\nExtensive experi-\nments show that FADS effectively mitigates both data-\ndriven and model-induced bias, providing a stable fair-\nness–performance trade-off. We further analyze the mech-\nanisms behind FADS, offering empirical insights into how\nbalanced demonstrations contribute to equitable multimodal\nreasoning.\nOur main contributions are summarized as follows:\n• We identify and formalize fairness challenges unique to\napplying MLLMs in medical image reasoning, empha-\nsizing the limitations of conventional resource-intensive\nfairness interventions.\n• We introduce ICL as a lightweight, tuning-free strategy\nfor fairness enhancement and systematically study the im-\npact of demonstration selection on bias propagation.\n• We propose a fairness-aware ICL framework (FADS)\nthat mitigates both data-driven and model-induced biases\nthrough balanced exemplar construction, achieving im-\nproved fairness and stability across tasks and datasets.\n2. Analysis: Can In-Context Learning Boost\nFairness in Medical Image Analysis?\nIn this section, we empirically examine whether ICL can\nimprove fairness in MLLMs for medical image reasoning.\nSpecifically, we analyze how different DS strategies in-\nfluence fairness and task performance across demographic\ngroups. We begin by introducing three commonly used DS\nstrategies and then present quantitative analyses that reveal\ntheir limitations and underlying causes of unfairness.\n2.1. Demonstration Selection\nGiven a labeled pool XL = {(xi, yi, si)}N\ni=1, where xi de-\nnotes the textual or multimodal input, yi is the task label,\nand si represents a sensitive attribute, the goal of demon-\nstration selection is to construct, for each test query x, a\ndemonstration set D(x) = {(xj, yj, sj)}K\nj=1 of size K to\nbe used in ICL. We investigate three representative strate-\ngies below.\nRandom Selection. [29] The most straightforward strategy\nrandomly samples K examples from the labeled pool with-\nout considering semantic or demographic information:\nDrand(x) = Sample\n\u0000XL, K\n\u0001\n,\nwhere Sample(·) denotes uniform sampling without re-\nplacement. This method is computationally efficient and\nwidely used as a baseline. However, its demonstrations may\nbe semantically irrelevant to the query, introducing noise\nand potentially amplifying demographic imbalance.\nSimilarity-based Selection. [20] A more targeted strategy\nselects demonstrations that are semantically close to the\nquery x.\nEach xi ∈XL is encoded as an embedding\nei = Menc(xi) using a pretrained text or multimodal en-\ncoder Menc(·). The cosine similarity between the query and\neach candidate is computed as\nf(x, xi) =\nex · ei\n∥ex∥∥ei∥.\nThe K most similar samples are then selected:\nDsim(x) = arg\nmax\nD⊆XL,|D|=K\nX\n(xi,yi,si)∈D\nf(x, xi).\nThis approach prioritizes contextual relevance and is com-\nmonly adopted in retrieval-based ICL frameworks.\nYet,\nas we later show, focusing solely on semantic similarity\n2\n"}, {"page": 3, "text": "can inadvertently favor majority subgroups prevalent in the\ndataset.\nClustering-based Selection. [45] To ensure greater diver-\nsity, clustering-based strategies partition the candidate pool\ninto C clusters {C1, C2, . . . , CC} in the embedding space\n(e.g., via K-Means) and sample a fixed number of demon-\nstrations from each cluster. Let Kc = ⌊K/C⌋denote the\nnumber of demonstrations per cluster:\nDclust(x) =\nC\n[\nc=1\nSample\n\u0000Cc, Kc\n\u0001\n.\nEach Cc contains semantically similar samples within\nthe cluster but diverse examples across clusters, ensuring\nbroader coverage of both semantic and demographic sub-\ngroups.\n2.2. Analysis Results\nExperimental Setup. We evaluate whether ICL can en-\nhance fairness under different DS strategies using two\nmultimodal large language models: Qwen2.5-VL-7B and\nLLaVA-Med.\nEach method constructs 8–16 demonstra-\ntions per query without fine-tuning. Fairness is measured by\nAverage Disparity (AD) across Gender, Race, and Eth-\nnicity, and performance is evaluated using Accuracy, Pre-\ncision, Recall, and F1-score. Figure 2 presents quantitative\ncomparisons, while Figure 3 illustrates the relationship be-\ntween data bias and fairness.\nFinding 1: Existing DS methods fail to consistently im-\nprove fairness.\nAcross both models, DS strategies dis-\nplay unstable and often contradictory fairness trends (Fig-\nure 2). For Qwen, the Similarity-based method achieves\nthe highest accuracy (65.3%) but also the largest Ethnicity\nAD (15.0%), indicating that stronger performance does not\nguarantee equitable outcomes. K-Means reduces Gender\nand Race ADs but lowers overall accuracy, while Random\nselection shows mixed effects across attributes. A similar\npattern emerges in LLaVA-Med, where Random achieves\nsmaller Race AD (14.7%) but extremely high Ethnicity AD\n(28.0%). These findings demonstrate that conventional DS\nheuristics cannot ensure fairness stability, as outcomes de-\npend more on dataset composition and model sensitivity\nthan on true fairness enhancement.\nFinding 2:\nUnfairness arises from demographic im-\nbalance in selected demonstrations. To quantify demo-\ngraphic imbalance, we compute the Max Diff metric, de-\nfined as the difference between the maximum and minimum\nproportional representation among sensitive groups:\nMaxDiff = max(ratio) −min(ratio).\nA smaller value indicates balanced demographics, while a\nlarger value signifies stronger bias. For example, a 75/25\nmale–female split yields MaxDiff = 0.50, whereas perfect\nparity corresponds to MaxDiff = 0.\nFigure 3 visualizes the relationship between data bias\n(MaxDiff) and fairness disparity (AD). A strong positive\ncorrelation (R = 0.67) emerges:\nhigher demographic\nimbalance leads to greater fairness gaps.\nFor instance,\nRandom-Ethnicity\n(MaxDiff=86.3%)\nand\nSimilarity-\nEthnicity (82.3%) yield the largest ADs (8.5–15.0%),\nwhile more balanced attributes such as Gender exhibit\nsmaller disparities. These results reveal that DS methods,\ndespite improving contextual relevance, often propagate\ndemographic skew from exemplars to model predictions.\nSummary. Our analysis demonstrates that fairness incon-\nsistency in current DS strategies stems from demographic\nimbalance in the selected demonstrations. Data bias directly\ncorrelates with fairness degradation, motivating the need for\na fairness-aware selection mechanism. This insight forms\nthe basis of our proposed Fairness-Aware Demonstration\nSelection framework, introduced in the next section.\n3. Fairness-Aware Demonstration Selection\nBuilding upon our analysis in Section 2.1, which revealed\nthat demographic imbalance in selected exemplars is the\nprimary source of fairness degradation, we propose the\nFairness-Aware Demonstration Selection (FADS) frame-\nwork. FADS is a lightweight, tuning-free method designed\nto enhance fairness in MLLMs through ICL. Rather than re-\ntraining or fine-tuning the model, FADS strategically selects\nbalanced and representative demonstrations to mitigate both\ndata-driven and model-induced biases.\nGiven a labeled dataset XL = {(xi, yi, si)}N\ni=1, where xi\ndenotes the input (textual or multimodal), yi ∈{0, 1} is the\ntask label, and si ∈{0, 1} represents a sensitive attribute\n(e.g., gender or race), the objective of FADS is to construct,\nfor each query x, a fairness-aware demonstration set D(x)\nthat is both semantically relevant and demographically bal-\nanced.\nStep 1: Data Bias Mitigation.\nTo decorrelate sensitive\nattributes from outcome labels, FADS first clusters the la-\nbeled samples into K groups in the embedding space using\na pretrained encoder (e.g., Sentence-BERT). Each cluster\nCi is then subdivided into four sub-clusters C(s,y)\ni\n, corre-\nsponding to each (s, y) pair. The goal is to identify clusters\nwith approximately uniform subgroup distributions. FADS\nselects Nd clusters by minimizing deviation in subgroup\nproportions:\nG = arg\nmin\nG⊆{Ci}\nX\nCi∈G\nX\ns,y\n1\n|Ci|\n\f\f\f |C(s,y)\ni\n| −1\n4|Ci|\n\f\f\f.\nThe resulting subset G = {G1, G2, . . . , GNd} represents a\ncandidate pool where sensitive attributes and outcome la-\n3\n"}, {"page": 4, "text": "Zero-shot\nRandom\nSimilarity\nK-means\n0\n20\n40\n60\n80\n100\nAccuracy %\n63.2\n59.5\n60.0\n59.3\nPerformance (Accuracy %)\nZero-shot\nRandom\nSimilarity\nK-means\n0\n1\n2\n3\n4\n0.40\n2.40\n3.10\n0.30\nGender AD (%)\nZero-shot\nRandom\nSimilarity\nK-means\n0\n5\n10\n15\n15.90\n14.70\n16.20\n11.60\nRace AD (%)\nZero-shot\nRandom\nSimilarity\nK-means\n0\n5\n10\n15\n20\n25\n30\n6.20\n28.00\n19.20\n28.00\nEthnicity AD (%)\nZero-shot\nRandom\nSimilarity\nK-means\n0\n20\n40\n60\n80\n100\nAccuracy %\n64.9\n65.0\n65.3\n64.7\nZero-shot\nRandom\nSimilarity\nK-means\n0\n2\n4\n6\n5.10\n4.10\n3.20\n1.80\nZero-shot\nRandom\nSimilarity\nK-means\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n11.30\n5.60\n4.20\n3.60\nZero-shot\nRandom\nSimilarity\nK-means\n0\n5\n10\n15\n13.20\n8.50\n15.00\n12.70\nQwen\nLLaVA-Med\nZero-shot\nRandom\nSimilarity\nK-means\nFigure 2. Comparison of performance and fairness metrics for Random, Similarity, and K-Means demonstration selection methods\non Qwen (bottom row) and LLaVA-Med (top row). Conventional DS strategies exhibit inconsistent and sometimes conflicting fairness\nbehaviors.\n0\n20\n40\n60\n80\nMax Diff (%)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nAttribute AD (%)\nRandom (per-attr)\nSimilarity (per-attr)\nK-means (per-attr)\nRand-Gender\nRand-Race\nRand-Ethnicity\nSim-Gender\nSim-Race\nSim-Ethnicity\nKM-Gender\nKM-Race\nKM-Ethnicity\nFigure 3. Correlation between data imbalance and fairness dis-\nparity. Each point corresponds to one attribute under a DS method\n(Random, Similarity, or K-Means). Larger demographic imbal-\nance (MaxDiff) in the selected demonstrations correlates with\nhigher Average Disparity (AD), indicating that biased exemplar\ncomposition directly drives fairness degradation.\nbels are more evenly distributed. This step mitigates data\nbias in the demonstration space before exemplar selection.\nStep 2: Balanced Demonstration Selection.\nFor each\nquery sample x, FADS constructs a balanced demonstration\nset D(x) of size D. Within each (s, y) subgroup, M = D/4\nexamples are selected from the filtered clusters based on\ntheir cosine similarity to the query in the embedding space:\nDs,y(x) = arg max\nDs,y\nX\nc∈Gs,y\nf(x, c),\nD(x) =\n[\ns,y\nDs,y(x),\nwhere f(x, c) measures the similarity between the query\nand candidate. This ensures that the selected demonstra-\ntions are both contextually relevant and demographically\nbalanced, preventing bias propagation from imbalanced ex-\nemplars.\nFADS systematically addresses the fairness limitations\nobserved in conventional demonstration selection. By in-\ncorporating demographic balance into exemplar retrieval,\nit reduces data-induced unfairness while preserving con-\ntextual alignment. Importantly, FADS achieves these im-\nprovements without modifying model parameters or requir-\ning additional training, offering a practical plug-and-play\napproach to enhance fairness in medical image reasoning\nwith MLLMs.\n4. Experiments\n4.1. Experimental Setup\nDatasets.\nWe evaluate our method on the following two\nwidely used medical vision-language datasets.\n• FairCLIP Glaucoma [21]:\n10,000 fundus (SLO)\nimages with clinical notes from the Harvard Glau-\ncoma Fairness dataset (7K train / 2K val / 1K test).\n4\n"}, {"page": 5, "text": "Table 1. Main experiment: Qwen on Glaucoma (8-shot). FADS yields the best overall fairness–performance trade-off. All AD values are\nreported in percentage points (%).\nMethod\nAccuracy (%)\nGender AD (%) ↓\nRace AD (%) ↓\nEthnicity AD (%) ↓\nAvg AD (%) ↓\nZero-shot\n64.9\n5.14\n11.33\n13.18\n9.88\nRandom\n65.0\n4.14\n5.56\n8.53\n6.08\nSimilarity\n65.3\n3.21\n4.15\n15.03\n7.46\nK-means\n64.7\n1.81\n3.63\n12.70\n6.05\nFADS (ours)\n66.4\n3.73\n7.09\n6.42\n5.75\nEach sample is annotated with gender (Male/Female),\nrace (White/Black/Asian), and ethnicity (Hispanic/Non-\nHispanic/Unknown).\n• CheXpert Plus [12]: We curate balanced subsets focus-\ning on edema, pleural effusion, and pneumothorax classi-\nfication with 2,500 samples (1,250 positive, 1,250 nega-\ntive) per disease.\nModels.\nWe evaluate on three vision-language models:\nQwen2.5-VL-7B-Instruct [28], LLaVA-Med [16] (med-\nical domain-adapted), and LLaVA-v1.6-Vicuna-7B [18]\n(general-purpose).\nFor embedding computation, we use\nGME-Qwen2-VL-2B [43].\nBaselines.\nWe compare six 8-shot demonstration selec-\ntion strategies:\n• Zero-shot: No demonstrations.\n• Random: Randomly select 8 demonstrations.\n• Similarity: Select 8 most similar demonstrations based\non embedding distance.\n• K-means: Cluster training data into K = 64 clusters and\nselect representatives from 8 random clusters.\n• FADS (Ours): Fairness-aware selection with K = 64\nbalanced clusters, stratified sampling on one sensitive at-\ntribute (e.g., gender), and balanced labels (4 positive, 4\nnegative).\n• FADS-Interaction (Ours): Extended FADS considering\nintersectional groups (e.g., Female-White) with K = 64\nclusters and minimum 2 samples per demographic com-\nbination.\n• FADS-Adaptive (Ours): Adaptive variant that reduces\nminimum sample requirement to 1 for underrepresented\ngroups (< 5%).\nFairness Metrics.\nWe measure fairness across three de-\nmographic attributes using:\n• MaxAccGap / AD (Accuracy Difference):\nmaxa Accuracy(a) −mina Accuracy(a)\nLower values indicate better fairness (0 = perfect fair-\nness, 1 = maximum unfairness).\nFurther Studies.\nWe conduct ablations on: (1) dataset\nsize\n(2.5K\nvs\n5K),\n(2)\nsensitive\nattribute\nselection\n(gender/race/ethnicity), (3) number of clusters (K\n∈\n{16, 32, 64, 128}), (4) minimum samples per combination\n(1/2/5), and (5) number of shots (0/4/8/16).\n4.2. Main Results\nConclusion 1: Existing DS methods cannot bring consis-\ntent fairness improvement for medical image analysis.\nAcross methods, conventional DS heuristics (Random,\nSimilarity, K-means) produce unstable and sometimes con-\ntradictory fairness outcomes on the Glaucoma benchmark\n(Table 1 and Fig. 2). For example, Similarity obtains the\nhighest task accuracy (65.3%) but exhibits the largest Eth-\nnicity AD (15.03%); K-means reduces Gender and Race\nADs in some cases but leaves Ethnicity AD high (12.70%).\nRandom selection can reduce one attribute gap while wors-\nening another. These results indicate that optimizing for\nsemantic relevance or cluster coverage alone does not guar-\nantee equitable subgroup representation.\nAnalysis. The root cause is that DS heuristics optimize\ndifferent objectives (relevance or coverage) without demo-\ngraphic constraints. Consequently, the exemplar pool cho-\nsen by a method can be balanced on one attribute but heav-\nily skewed on another, producing attribute-specific AD vari-\nability. Therefore, fairness under ICL requires explicit de-\nmographic balancing during exemplar selection rather than\nrelying solely on semantic heuristics.\n4.3. Scaling Analysis\nSetup.\nTo assess how dataset scale affects fairness and\nFADS performance, we compare experiments trained (or\nsubsampled) at two dataset sizes: 2.5K and 5K samples\n(training pool used to draw demonstrations). We report per-\nattribute AD (Race, Ethnicity) at both scales and the ab-\nsolute change in percentage points (pp). The result is pre-\nsented in Table 2.\nConclusion 2: Increasing dataset size generally reduces\nRace AD for most methods. Notably, FADS shows the\nlargest AD changing when the training pool doubles\n(2.5K →5K), indicating that fairness-aware selection\nbenefits from larger and more diverse candidate pools.\n5\n"}, {"page": 6, "text": "Table 2. Scaling analysis: Impact of dataset size on Race AD. Change in AD from 2.5K to 5K samples.\nMethod\n2.5K Race AD (%)\n5K Race AD (%)\nChange (pp)\nZero-shot\n34.90\n23.29\n-11.61\nRandom\n32.49\n21.54\n-10.95\nSimilarity\n38.02\n23.29\n-14.73\nFADS\n36.46\n21.54\n-14.92\nTable 3. Hyper-parameter analysis: Impact of shot budget (4-shot vs 8-shot) on accuracy and fairness metrics. All values in %.\nMethod\nShots\nAccuracy\nGender AD ↓\nRace AD ↓\nBaseline\nZero-shot\n64.90%\n5.14%\n11.33%\nRandom\n4-shot\n64.60%\n3.62%\n5.69%\nSimilarity\n4-shot\n64.20%\n3.10%\n6.97%\nFADS\n4-shot\n64.60%\n2.80%\n7.35%\nRandom\n8-shot\n65.50%\n4.49%\n4.79%\nSimilarity\n8-shot\n65.00%\n4.14%\n5.56%\nFADS\n8-shot\n65.30%\n3.21%\n4.15%\n4.4. Hyper-parameter Analysis\nSetup.\nWe study how the exemplar budget (number of\nshots) affects both performance and fairness. We evaluate 4-\nshot and 8-shot settings for Random, Similarity, and FADS;\nzero-shot is included as baseline. Results are summarized\nin Table 3. We find\n• 4-shot regime: FADS matches or slightly improves ac-\ncuracy over Random/Similarity (both 64.6% vs Zero-shot\n64.9%) while achieving the lowest Gender AD (2.80%\nvs Random 3.62%, Similarity 3.10%). Race AD remains\nhigher for FADS in this tight-budget setting (7.35%), in-\ndicating a trade-off when exemplars are severely limited.\n• 8-shot regime: With more exemplars, FADS attains bet-\nter fairness on Race (4.15%) while preserving competitive\naccuracy (65.3%). In contrast, Random and Similarity\nshow mixed race/gender trade-offs (see Table 3).\nConclusion 3: The hyper-parameter analysis shows that\nFADS is robust across shot budgets: it reduces Gender\nAD in very low-shot regimes and reduces Race AD when\nmore exemplars are allowed. Practically, this suggests\ntuning the exemplar budget depending on the primary\nfairness objective (e.g., prioritize gender parity at small\nbudgets, race parity with larger budgets).\n4.5. Data Bias Mitigation Analysis\nConclusion 4: FADS effectively mitigates data bias in\ndemonstration selection, enabling stable fairness im-\nprovement.\nTo understand why FADS consistently outperforms other\ndemonstration selection (DS) methods in fairness, we an-\nTable 4.\nMax Diff (%) comparison across sensitive attributes.\nLower values indicate smaller demographic imbalance among se-\nlected demonstrations. Here we use extended FADS that considers\nintersectional demographic groups (e.g., Female-White).\nMethod\nGender\nRace\nEthnicity\nRandom\n11.18\n67.30\n86.31\nSimilarity\n12.05\n65.88\n82.31\nK-means\n0.00\n75.00\n75.00\nFADS-Interactive\n0.00\n0.00\n88.22\nalyze the demographic balance of the selected demonstra-\ntions using the Max Diff (%) metric, which quantifies\nthe largest inter-group proportion gap among sensitive at-\ntributes (Gender, Race, Ethnicity). A lower Max Diff indi-\ncates a more balanced sample distribution within the con-\nstructed demonstration set.\nAs shown in Table 4, FADS selects a more demographi-\ncally balanced set of exemplars compared to methods like\nRandom, Similarity, and K-means. This is particularly ev-\nident in the Race dimension, where FADS outperforms\nother methods by reducing the racial imbalance, as seen\nin the significant reduction in Max Diff values. For ex-\nample, Random and Similarity have high Max Diff val-\nues in Race (67.30% and 65.88%, respectively), whereas\nFADS achieves near-perfect balance, especially in the case\nof Race. On the other hand, K-means, despite achieving\nperfect gender balance by coincidence, still exhibits high\nracial imbalance due to clustering predominantly around the\nmajority groups, such as White samples.\n6\n"}, {"page": 7, "text": "Patient has moderate stage normal-tension\nglaucoma in the left eye and is a glaucoma\nsuspect in the right eye. Medications\npreviously found ineffective are brimonidine,\nazopt, rhopressa. Other issue includes\nthyroid eye disease. No surgery history.\nMale\nWhite\nNon-Hispanic\n69.3\nFADS\nSimilarity\nImage\nRecord\nFigure 4. Case study: Similarity-based ICL vs FADS. Similarity overrepresents majority groups (75% Male, 50% White) and excludes the\nBlack subgroup; FADS enforces balanced sampling (50/50 gender, 33.3% per race).\nAnalysis. Random and Similarity sampling methods in-\nherit the intrinsic biases of the labeled dataset, which of-\nten results in imbalanced representation across sensitive at-\ntributes. In contrast, K-means, although semantically di-\nverse, amplifies this imbalance by clustering around dense\nembedding regions that are dominated by majority groups.\nFADS, however, integrates demographic stratification and\nbalanced sampling within clusters, ensuring that the se-\nlected demonstrations are more demographically represen-\ntative. This approach fundamentally reduces the data bias\nthat typically underlies fairness degradation.\nImplications. By reducing Max Diff in the selection of\ndemonstrations, FADS directly leads to lower disparity\nin model outputs across demographic groups (lower AD).\nThis demonstrates that the fairness improvement achieved\nby FADS is not coincidental but stems from a systematic\napproach to balancing demographic representation in the\ndemonstration set. These results highlight the importance\nof explicitly addressing data imbalance during in-context\nlearning, particularly in medical image analysis, to ensure\nfair and equitable reasoning.\n4.6. Case Study: Balanced Demonstration Selection\nwith FADS\nFinding 4: FADS produces more demographically bal-\nanced demonstrations, directly enhancing fairness.\nTo qualitatively illustrate how FADS mitigates data bias\nduring in-context learning, we present a representative case\nfrom the Glaucoma dataset (Figure 4). The figure com-\npares the composition of selected demonstrations under a\nconventional similarity-based method and under our FADS\napproach. The input case describes a male, White, non-\nHispanic patient with moderate-stage glaucoma and comor-\nbidities.\nThis qualitative example confirms our quantitative analyses:\nconstructing balanced and semantically relevant demonstra-\ntion sets reduces downstream disparity (lower AD) and\nyields more interpretable conditional behavior.\nTo summarize, FADS consistently provides a favorable\nfairness–performance trade-off under realistic shot budgets\nand benefits further from larger candidate pools. The scal-\ning and hyper-parameter studies show how dataset size and\nexemplar budget influence fairness–performance dynamics,\noffering practical guidance for deployment.\n5. Related Work\n5.1. Multimodal Large Language Models\nMultimodal large language models (MLLMs) have recently\nemerged as powerful architectures that integrate visual, tex-\ntual, and sometimes structured modalities into a unified\nreasoning framework, and have achieved strong perfor-\nmance on a wide range of vision understanding and rea-\nsoning tasks [37, 40].\nGeneral-purpose models such as\nLLaVA, built via visual instruction tuning [19], show that\naligning a vision encoder with an instruction-tuned LLM\nenables open-ended visual dialogue and chain-of-thought\nstyle reasoning. In the medical domain, early biomedical\nvision–language models such as BioViL and its temporal\nextension [1, 2] and GLoRIA [9] learn joint representations\nbetween radiology images and reports, while more recent\ngeneralist medical MLLMs, including LLaVA-Med [16]\nand Hulu-Med [13], adapt this paradigm to diverse clini-\ncal images and instructions and have shown significant im-\nprovements in zero-shot reasoning and robustness across\nimaging modalities including fundus, histopathology, and\nCT scans.\nProprietary systems such as Med-PaLM and\nMed-PaLM 2 [26, 27] further demonstrate near-expert per-\nformance on medical question answering through instruc-\ntion tuning on large-scale expert-curated clinical data.\n7\n"}, {"page": 8, "text": "Despite their remarkable progress, the trustworthiness of\nmedical MLLMs, such as fairness, robustness and hallu-\ncination, remains a major open challenge [7, 11, 33, 34].\nEchoBench [38] further reveals severe sycophancy in medi-\ncal MLLMs, where models tend to agree with user-provided\nmisinformation. However, mitigating these trustworthiness\nissues is non-trivial in practice.\nMost foundation-scale\nMLLMs operate as black boxes, hindering transparency\nand interpretability in clinical decision-making and limit-\ning the possibility of model-specific interventions. More-\nover, they remain highly sensitive to domain shift be-\ntween natural and medical imagery, often exhibiting de-\ngraded robustness and fairness across institutions, imag-\ning devices, or demographic subgroups [25, 36].\nFi-\nnally, adapting MLLMs through fine-tuning or domain-\nspecific retraining is computationally expensive and risks\ncatastrophic forgetting, resulting in erasing general multi-\nmodal reasoning skills in favor of narrow task specializa-\ntion [10, 32, 39]. These challenges motivate the need for\ntuning-free, data-efficient mechanisms, such as our pro-\nposed fairness-aware in-context learning framework, that\ncan adapt existing MLLMs for equitable medical reason-\ning without compromising their generalization capacity or\nrequiring access to model parameters.\n5.2. Fairness in Medical Image Analysis\nFairness in medical image analysis has become an increas-\ningly critical research focus, as biased AI systems risk\namplifying existing health disparities and eroding clinical\ntrust. Systematic investigations [5, 15] have revealed that\ndemographic, institutional, and device-level biases persist\nthroughout the medical AI pipeline—from data collection\nto model deployment. These biases often manifest as un-\neven predictive accuracy and sensitivity across subpopula-\ntions defined by gender, race, ethnicity, or socioeconomic\nstatus.\nEmpirical analyses further attribute such dispari-\nties to dataset imbalance, missing demographic annotations,\nand domain-specific artifacts such as scanner heterogeneity\nor site-dependent acquisition protocols. As a result, fairness\nviolations can arise even when models achieve high overall\naccuracy, underscoring the limitations of standard evalua-\ntion metrics in safety-critical domains.\nTo address these inequities, a variety of fairness-\nenhancing strategies have been proposed. Data-centric ap-\nproaches employ reweighting, resampling, or augmenta-\ntion to balance demographic representation, while model-\ncentric methods leverage adversarial debiasing or gradi-\nent reversal to decorrelate latent features from sensitive\nattributes.\nDomain adaptation and generalization frame-\nworks attempt to align feature distributions across insti-\ntutions, and federated learning schemes such as Zhang\net al. [42] enable decentralized training while preserving\npopulation diversity. More recent efforts, including Deng\net al. [3] and FairCLIP [21], explore representation-level\nfairness through multimodal contrastive learning, show-\ning that bias can persist even in pretrained embeddings.\nHowever, most existing methods are computationally inten-\nsive, require large-scale demographic annotations, or de-\npend on model retraining—making them impractical for\nclosed-source or foundation-level MLLMs. Furthermore,\nthese approaches often trade off fairness for accuracy or in-\nterpretability. In contrast, our proposed Fairness-Aware\nDemonstration Selection (FADS) framework adopts a\ndata- and model-agnostic perspective, leveraging in-context\nlearning to achieve fairness improvements without fine-\ntuning. This paradigm offers a lightweight, scalable alter-\nnative that complements ongoing efforts toward trustworthy\nand equitable medical AI.\n6. Discussion and Conclusion\nIn this work, we investigated the potential of In-Context\nLearning (ICL) to enhance fairness in multimodal large\nlanguage models (MLLMs) for medical image reason-\ning.\nThrough systematic analysis, we revealed that ex-\nisting demonstration selection strategies—such as random,\nsimilarity-based,\nand\nclustering-based\nmethods—often\nyield inconsistent fairness outcomes due to demographic\nimbalances in the selected exemplars. To address this chal-\nlenge, we proposed the Fairness-Aware Demonstration\nSelection (FADS) framework, which constructs balanced\nand semantically relevant demonstration sets by integrating\ndemographic-aware clustering with similarity-based sam-\npling. FADS improves fairness without model retraining\nor additional data collection, offering a lightweight, tuning-\nfree, and scalable approach suitable for foundation-level\nMLLMs. These results highlight the potential of fairness-\naware in-context learning as a scalable and data-efficient so-\nlution for equitable medical image reasoning.\nComprehensive experiments across multiple medical\nimaging benchmarks demonstrate that FADS consistently\nreduces gender-, race-, and ethnicity-related disparities\nwhile maintaining competitive task performance. Our anal-\nyses further show that balancing demographic composi-\ntion at the demonstration level directly mitigates both data-\ndriven and model-induced biases, enabling more equitable\nin-context reasoning. These findings highlight the promise\nof fairness-aware ICL as a practical paradigm for promot-\ning trustworthy and inclusive medical AI. In future work,\nwe plan to extend FADS to handle multiple sensitive at-\ntributes and more complex intersectional fairness settings.\nAnother promising direction is to explore adaptive demon-\nstration retrieval strategies that dynamically balance fair-\nness and performance at inference time. We believe such\nadvances will further strengthen fairness-aware in-context\nlearning as a general framework for equitable and transpar-\nent medical AI.\n8\n"}, {"page": 9, "text": "References\n[1] Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando\nPerez-Garcia, Maximilian Ilse, Daniel C Castro, Benedikt\nBoecking, Harshita Sharma, Kenza Bouzid, Anja Thieme,\net al.\nLearning to exploit temporal structure for biomed-\nical vision-language processing.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 15016–15027, 2023. 7\n[2] Benedikt Boecking,\nNaoto Usuyama,\nShruthi Bannur,\nDaniel C Castro, Anton Schwaighofer, Stephanie Hyland,\nMaria Wetscherek, Tristan Naumann, Aditya Nori, Javier\nAlvarez-Valle, et al. Making the most of text semantics to\nimprove biomedical vision–language processing. In Euro-\npean conference on computer vision, pages 1–21. Springer,\n2022. 7\n[3] Wenlong Deng, Yuan Zhong, Qi Dou, and Xiaoxiao Li. On\nfairness of medical image classification with multiple sen-\nsitive attributes via learning orthogonal representations. In\nInternational Conference on Information Processing in Med-\nical Imaging, pages 158–169. Springer, 2023. 8\n[4] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma,\nRui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao\nChang, et al. A survey on in-context learning. In Proceed-\nings of the 2024 conference on empirical methods in natural\nlanguage processing, pages 1107–1128, 2024. 1\n[5] Karen Drukker, Weijie Chen, Judy Gichoya, Nicholas\nGruszauskas, Jayashree Kalpathy-Cramer, Sanmi Koyejo,\nKyle Myers, Rui C S´a, Berkman Sahiner, Heather Whitney,\net al. Toward fairness in artificial intelligence for medical im-\nage analysis: identification and mitigation of potential biases\nin the roadmap from data collection to model deployment.\nJournal of Medical Imaging, 10(6):061104–061104, 2023.\n1, 8\n[6] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-\ncal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario\nMarch, and Victor Lempitsky. Domain-adversarial training\nof neural networks. Journal of machine learning research,\n17(59):1–35, 2016. 1\n[7] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan,\nMengling Feng, and Erik Cambria. A survey of large lan-\nguage models for healthcare: from data, technology, and ap-\nplications to accountability and ethics. Information Fusion,\n118:102963, 2025. 8\n[8] Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang,\nXinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su.\nMitigating catastrophic forgetting in large language models\nwith self-synthesized rehearsal. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1416–1428, 2024.\n1\n[9] Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and\nSerena Yeung. Gloria: A multimodal global-local represen-\ntation learning framework for label-efficient medical image\nrecognition. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 3942–3951, 2021. 7\n[10] Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng\nWan, He Li, Bo Du, Dacheng Tao, and Mang Ye. Learn from\ndownstream and be yourself in multimodal large language\nmodel fine-tuning. arXiv preprint arXiv:2411.10928, 2024.\n8\n[11] Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui\nZhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu,\nYixuan Zhang, et al. Trustllm: Trustworthiness in large lan-\nguage models. arXiv preprint arXiv:2401.05561, 2024. 8\n[12] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Sil-\nviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad\nHaghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:\nA large chest radiograph dataset with uncertainty labels and\nexpert comparison. In Proceedings of the AAAI Conference\non Artificial Intelligence, pages 590–597, 2019. 5\n[13] Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu,\nChenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng,\nJoey Tianyi Zhou, Jin Hao, Zijian Chen, Ruijia Wu, Tao\nTang, Junhui Lv, Hongxia Xu, Hongwei Wang, Jun Xiao,\nBin Feng, Fudong Zhu, Kenli Li, Weidi Xie, Jimeng Sun,\nJian Wu, and Zuozhu Liu. Hulu-med: A transparent gener-\nalist model towards holistic medical vision-language under-\nstanding, 2025. 7\n[14] Faisal Kamiran and Toon Calders. Data preprocessing tech-\nniques for classification without discrimination. Knowledge\nand information systems, 33(1):1–33, 2012. 1\n[15] Burak Koc¸ak, Andrea Ponsiglione, Arnaldo Stanzione,\nChristian Bluethgen, Jo˜ao Santinha, Lorenzo Ugga, Merel\nHuisman, Michail E Klontzas, Roberto Cannella, and Renato\nCuocolo. Bias in artificial intelligence for medical imaging:\nfundamentals, detection, avoidance, mitigation, challenges,\nethics, and prospects. Diagnostic and interventional radiol-\nogy, 31(2):75, 2025. 1, 8\n[16] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\nand Jianfeng Gao. Llava-med: Training a large language-\nand-vision assistant for biomedicine in one day. Advances\nin Neural Information Processing Systems, 36:28541–28564,\n2023. 1, 5, 7\n[17] Fenglin Liu, Tingting Zhu, Xian Wu, Bang Yang, Chenyu\nYou, Chenyang Wang, Lei Lu, Zhangdaihong Liu, Yefeng\nZheng, Xu Sun, et al. A medical multimodal large language\nmodel for future pandemics. NPJ Digital Medicine, 6(1):\n226, 2023. 1\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Advances in Neural Information\nProcessing Systems (NeurIPS), pages 34892–34916, 2023. 5\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36:34892–34916, 2023. 7\n[20] Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, and Mehran\nKazemi.\nIn-context learning with retrieved demonstra-\ntions for language models:\nA survey.\narXiv preprint\narXiv:2401.11624, 2024. 2\n[21] Yan Luo, Mingzhen Shi, Muhammad Osama Khan, Muham-\nmad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian,\nLi Song, Ava Kouhana, Tobias Elze, and Yi Fang. Fairclip:\nHarnessing fairness in vision-language learning. In CVPR,\npages 12289–12301, 2024. 4, 8\n9\n"}, {"page": 10, "text": "[22] David Madras, Elliot Creager, Toniann Pitassi, and Richard\nZemel. Learning adversarially fair and transferable represen-\ntations. In International Conference on Machine Learning,\npages 3384–3393. PMLR, 2018. 1\n[23] Bertalan Mesk´o. The impact of multimodal large language\nmodels on health care’s future. Journal of medical Internet\nresearch, 25:e52865, 2023. 1\n[24] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh\nHajishirzi. Metaicl: Learning to learn in context. In Proceed-\nings of the 2022 conference of the North American chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, pages 2791–2809, 2022. 1\n[25] Aminu Musa, Rajesh Prasad, and Monica Hernandez. Ad-\ndressing cross-population domain shift in chest x-ray clas-\nsification through supervised adversarial domain adaptation.\nScientific Reports, 15(1):11383, 2025. 8\n[26] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,\nJason Wei, Hyung Won Chung, Nathan Scales, Ajay Tan-\nwani, Heather Cole-Lewis, Stephen Pfohl, et al.\nLarge\nlanguage models encode clinical knowledge. Nature, 620\n(7972):172–180, 2023. 7\n[27] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery\nWulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R\nPfohl, Heather Cole-Lewis, et al. Toward expert-level med-\nical question answering with large language models. Nature\nMedicine, 31(3):943–950, 2025. 7\n[28] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui\nMen, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Jun-\nyang Lin. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution.\narXiv preprint\narXiv:2409.12191, 2024. 5\n[29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 2\n[30] Noam Wies, Yoav Levine, and Amnon Shashua. The learn-\nability of in-context learning. Advances in Neural Informa-\ntion Processing Systems, 36:36637–36651, 2023. 1\n[31] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan,\nand Philip S Yu. Multimodal large language models: A sur-\nvey. In 2023 IEEE International Conference on Big Data\n(BigData), pages 2247–2256. IEEE, 2023. 1\n[32] Junda Wu, Yuxin Xiong, Xintong Li, Yu Xia, Ruoyu Wang,\nYu Wang, Tong Yu, Sungchul Kim, Ryan A Rossi, Lina\nYao, et al. Mitigating visual knowledge forgetting in mllm\ninstruction-tuning via modality-decoupled gradient descent.\narXiv preprint arXiv:2502.11740, 2025. 8\n[33] Peiran Wu, Che Liu, Canyu Chen, Jun Li, Cosmin I Bercea,\nand Rossella Arcucci. Fmbench: Benchmarking fairness in\nmultimodal large language models on medical tasks. arXiv\npreprint arXiv:2410.01089, 2024. 8\n[34] Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou,\nYue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu\nZhu, et al. Cares: A comprehensive benchmark of trustwor-\nthiness in medical vision language models. Advances in Neu-\nral Information Processing Systems, 37:140334–140365,\n2024. 8\n[35] Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tianqi Liu,\nZhipeng Li, Xin Liu, and Xiaoxuan Huang. A comprehen-\nsive survey of large language models and multimodal large\nlanguage models in medicine.\nInformation Fusion, page\n102888, 2024. 1\n[36] Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico\nMastrodicasa, Wei Wu, Edward J Wang, Dushyant Sahani,\nand Shwetak Patel. Demographic bias of expert-level vision-\nlanguage foundation models in medical imaging.\nScience\nAdvances, 11(13):eadq0305, 2025. 8\n[37] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen.\nA survey on multimodal\nlarge language models.\nNational Science Review, 11(12):\nnwae403, 2024. 1, 7\n[38] Botai Yuan, Yutian Zhou, Yingjie Wang, Fushuo Huo,\nYongcheng Jing, Li Shen, Ying Wei, Zhiqi Shen, Ziwei Liu,\nTianwei Zhang, et al.\nEchobench: Benchmarking syco-\nphancy in medical large vision-language models.\narXiv\npreprint arXiv:2509.20146, 2025. 8\n[39] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu,\nYong Jae Lee, and Yi Ma. Investigating the catastrophic for-\ngetting in multimodal large language models. arXiv preprint\narXiv:2309.10313, 2023. 8\n[40] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan\nSu, Chenhui Chu, and Dong Yu. MM-LLMs: Recent ad-\nvances in MultiModal large language models. In Findings of\nthe Association for Computational Linguistics: ACL 2024,\npages 12401–12430, Bangkok, Thailand, 2024. Association\nfor Computational Linguistics. 7\n[41] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan\nSu, Chenhui Chu, and Dong Yu.\nMm-llms: Recent ad-\nvances in multimodal large language models. arXiv preprint\narXiv:2401.13601, 2024. 1\n[42] Fengda Zhang, Zitao Shuai, Kun Kuang, Fei Wu, Yueting\nZhuang, and Jun Xiao. Unified fair federated learning for\ndigital healthcare. Patterns, 5(1), 2024. 8\n[43] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai,\nDingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and\nMin Zhang. Gme: Improving universal multimodal retrieval\nby multimodal llms. arXiv preprint arXiv:2412.16855, 2024.\n5\n[44] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes\ngood examples for visual in-context learning?\nAdvances\nin Neural Information Processing Systems, 36:17773–17794,\n2023. 1\n[45] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.\nAutomatic chain of thought prompting in large language\nmodels. arXiv preprint arXiv:2210.03493, 2022. 3\n10\n"}]}