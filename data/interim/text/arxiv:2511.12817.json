{"doc_id": "arxiv:2511.12817", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.12817.pdf", "meta": {"doc_id": "arxiv:2511.12817", "source": "arxiv", "arxiv_id": "2511.12817", "title": "Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs", "authors": ["Shasha Zhou", "Mingyu Huang", "Jack Cole", "Charles Britton", "Ming Yin", "Jan Wolber", "Ke Li"], "published": "2025-11-16T22:58:22Z", "updated": "2025-12-19T15:20:17Z", "summary": "The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.12817v2", "url_pdf": "https://arxiv.org/pdf/2511.12817.pdf", "meta_path": "data/raw/arxiv/meta/2511.12817.json", "sha256": "624ae24d4ff30fd5cb54217e38702a93a57be0a71daab50e57ac2a30dc76a5ed", "status": "ok", "fetched_at": "2026-02-18T02:26:57.039701+00:00"}, "pages": [{"page": 1, "text": "Assessing Automated Fact-Checking for Medical LLM Responses\nwith Knowledge Graphs\nShasha Zhou1,2, Mingyu Huang1, Jack Cole1, Charles Britton2, Ming Yin3, Jan Wolber4, Ke Li1\n1 Department of Computer Science and Engineering, University of Electronic Science and Technology of China\n2 Department of Computer Science, University of Exeter\n3 College of Medicine and Health, University of Exeter\n4 Department of Computer Science, Purdue University\n5 GE HealthCare\n{sz484, m.huang, jc1417, k.li}@exeter.ac.uk, charles.britton@nhs.scot, mingyin@purdue.edu, jan.wolber@gehealthcare.com\nAbstract\nThe recent proliferation of large language models (LLMs)\nholds the potential to revolutionize healthcare, with strong\ncapabilities in diverse medical tasks. Yet, deploying LLMs\nin high-stakes healthcare settings requires rigorous verifica-\ntion and validation to understand any potential harm. This\npaper investigates the reliability and viability of using med-\nical knowledge graphs (KGs) for the automated factuality\nevaluation of LLM-generated responses. To ground this in-\nvestigation, we introduce FAITH, a framework designed to\nsystematically probe the strengths and limitations of this KG-\nbased approach. FAITH operates without reference answers\nby decomposing responses into atomic claims, linking them to\na medical KG, and scoring them based on evidence paths. Ex-\nperiments on diverse medical tasks with human subjective eval-\nuations demonstrate that KG-grounded evaluation achieves\nconsiderably higher correlations with clinician judgments and\ncan effectively distinguish LLMs with varying capabilities. It\nis also robust to textual variances. The inherent explainability\nof its scoring can further help users understand and mitigate\nthe limitations of current LLMs. We conclude that while lim-\nitations exist, leveraging KGs is a prominent direction for\nautomated factuality assessment in healthcare.\nIntroduction\nLarge language models (LLMs) have emerged as powerful\ntools with potential to revolutionize healthcare. These mod-\nels demonstrate strong capabilities across diverse medical\ntasks (Singhal et al. 2023; Saab et al. 2024; Nori et al. 2023;\nVan Veen et al. 2024; Liu et al. 2025; McDuff et al. 2025).\nHowever, deploying LLMs in the high-stakes healthcare do-\nmain, where precise and trustworthy information is critical,\ncalls for rigorous evaluation. Due to inherent characteris-\ntics of LLMs like the vast output scale, automated evalu-\nation methods have become increasingly necessary. These\nmethods complement traditional validation approaches like\nclinical studies (Singhal et al. 2023; Wang et al. 2023b),\nwhich are often too slow or limited in scope to keep pace\nwith rapidly evolving LLM technologies. Foremost among\nthese evaluation concerns is ensuring the factual integrity\nof LLM-generated medical content (Wang et al. 2023a;\nCopyright © 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nThirunavukarasu et al. 2023; Lee, Bubeck, and Petro 2023;\nShen et al. 2023; Augenstein et al. 2024). This is crucial\nas LLMs can produce plausible yet dangerously inaccurate\ninformation, known as hallucinations (Farquhar et al. 2024;\nJi et al. 2023). Such inaccuracies undermine stakeholder’s\ntrust and pose significant barriers to clinical adoption and\nregulatory approval of LLM applications.\nTraditionally, automated evaluation of LLM outputs has\nrelied on natural language processing (NLP) metrics such\nas BLEU (Papineni et al. 2002) and BERTScore (Zhang\net al. 2020). Yet, the usefulness of these metrics in special-\nized domains like healthcare is limited as they require man-\nually crafted reference responses that are rarely available\nin practice. In addition, it has been reported in various re-\ncent studies that they often correlate poorly with clinician\njudgments (Singhal et al. 2023; Van Veen et al. 2024; Liu\net al. 2025), probably because these metrics focus on over-\nall lexical or semantic similarities with reference texts. In\ncontrast, human experts prioritize evaluating the factual accu-\nracy of specific claims within LLM-generated responses (e.g.,\nthe statement “dry cough is a symptom of bronchiectasis\",\nFig. 1a-b) stylistic variations or peripheral details.\nAlternative approaches for assessing factuality involve us-\ning other LLMs or supervised models as evaluators (Chern\net al. 2023; Min et al. 2023; Wang et al. 2023c). However,\nrelying on LLMs for factuality evaluation can also be prob-\nlematic, as they are susceptible to data-poisoning attacks that\ncompromise their reliability (Alber et al. 2025), and these\nevaluators themselves can suffer from hallucinations. Super-\nvised models detect factual errors by identifying patterns\nfrom historical training data, assuming future data will follow\nthe same patterns. However, this assumption often fails in\nunforeseen error types, reducing their generalizability.\nIn recent years, various medical knowledge graphs (KGs)\nhave been developed to represent diverse validated medical\nknowledge as collections of nodes (entities) and edges (re-\nlationships) (Chen et al. 2025; Poulain and Beheshti 2024;\nHamza et al. 2025). They have been successfully applied\nto assist the generation of medical responses via retrieval-\naugmented generation (RAG) (Gao et al. 2023). Inspired\nby this, in this paper we hypothesize that these same medi-\ncal KGs may also be helpful in assisting the fact-checking\nof LLM-generated medical contents. Yet, unlike generation\narXiv:2511.12817v2  [cs.LG]  19 Dec 2025\n"}, {"page": 2, "text": "The most likely diagnosis is bronchiectasis. The patient’s symptoms of dry cough and \nincreasing shortness of breath on effort, along with the presence of ﬁnger clubbing, \nsuggest a chronic lung disease. Extrinsic allergic alveolitis typically presents with \nhaemoptysis and chest pain, whereas idiopathic pulmonary ﬁbrosis is more likely …\nClaim 2: (bronchiectasis, disease_phenotype_positive, increasing shortness of breath) \nClaim n: (Extrinsic allergic alveolitis, disease_phenotype_positive, haemoptysis) \nsubject\npredicate\nobject\nDry cough\nClaim 1: (bronchiectasis, disease_phenotype_positive, dry cough) \ninverse_isa\nmanifestation\na. LLM response\nb. Claim decomposition\nc. KG Matching and Traversing\nd. Scoring and Explaining\nVeracity\nFact index\nNR3C1\ncork-handlers' \ndisease\nPrednisolone\npneumonitis\nHodgkins \nlymphoma\nHemoptysis\nExtrinsic allergic \nalveolitis\nCoughing\nisa\nFigure 1: Overview of FAITH. (a) An example medical content generated by an LLM. (b) FAITH processes structured claims relating\ndifferent medical entities in the response, which are automatically extracted by an LLM. (c) Extracted claims are matched with nodes (maroon)\nin a medical KG. Paths (maroon) and intermediate nodes (blue) linking the entities are identified. (d) A factuality score for each claim is\ncomputed based on the path characteristics and edge semantics. The response score is aggregated from individual claim scores.\ntasks, fact-checking with KGs can be more complex as it\nrequires multi-hop exploration through complex relational\npaths within the KG. Additionally, effective fact-checking\nmust quantify how well extracted claims align with structured\nmedical knowledge, making simple lookups inadequate.\nTo systematically assess the potential use of KG in medical\nfact-checking for LLMs, we developed FAITH (fact-aware\nevaluation of LLM-generated contents in healthcare), an un-\nsupervised, reference-free framework based on medical KGs.\nFAITH first decomposes an LLM-generated response into\natomic medical claims (Fig. 1b). It then uses entity resolution\ntechniques (Christophides et al. 2021) to map the medical\nentities within these claims to their corresponding nodes in\na medical KG (Fig. 1c). Next, FAITH traverses the KG to\nidentify the evidence paths connecting the linked entities,\nbased on which the factuality of each claim will be assessed.\nFinally, an overall factuality score for the entire response is\naggregated from the individual claim scores, while preserving\nper-claim scores for nuanced interpretability (Fig. 1c-d).\nWe evaluated the effectiveness of FAITH and various ex-\nisting fact-checking approaches on four established medical\nquestion-answering (QA) tasks with both quantitative and\nsubjective evaluations. We also investigated whether FAITH\nis readily usable to improve the safety of LLM-based medical\nsystems by employing FAITH as a thresholding mechanism.\nWe additionally assessed FAITH’s broader utility by apply-\ning it to medical summarization and fact verification tasks\nand conducted ablation analysis on its core components. With\nthese experiments, we observed several features of this KG-\nbased fact-checker compared to conventional ones:\n• It operates without reference answers or supervised train-\ning, making it highly suitable for real-world settings.\n• It correlates significantly better with clinician judgments.\n• It is robust to noise in the input texts, and can effectively\ndistinguish LLMs of varying capabilities.\n• It offers better interpretability than existing approaches\nby pinpointing which particular claim is susceptible.\n• Yet, the reliability of FAITH places high demands on\nthe KG quality, which is expected to improve as newer\nmedical KGs come out in the future.\nRelated Work\nFactuality Evaluation of LLMs in Medicine\nMethods for evaluating the factuality of LLM-generated med-\nical contents generally fall into two categories.\nThe first line of work compares LLM outputs to reference\nanswers using NLP metrics. Examples include BLEU (Pap-\nineni et al. 2002) and ROUGE (Lin 2004) for lexical overlap,\nand BERTScore (Zhang et al. 2020) for contextual similarity\nin embedding space. To further incorporate domain-specific\nknowledge into evaluation, MEDCON (Yim et al. 2023) uses\nQuickUMLS (Soldaini and Goharian 2016)—a tool for ex-\ntracting biomedical concepts—to allow for a more informed\nevaluation. imapScore (Wang et al. 2024) adopts the similar\nidea, but instead uses an LLM for medical concept extraction.\nHowever, in practice, human references are hardly available,\nthe application of these metrics is therefore largely limited. In\ncontrast, FAITH, is reference-free and much more flexible.\nAnother line of work employs supervised machine learning\nmethods to detect factual errors. Earlier ones like FED (Mehri\nand Eskénazi 2020) and DEB (Sai et al. 2020) adopted BERT-\nlike architectures. Yet, these approaches hardly generalize\nbeyond their original training data. Later works used more ad-\nvanced GPT-series models for evaluation, e.g., GPTScore (Fu\net al. 2024), G-Eval (Liu et al. 2023), and GPT-judge (Lin,\nHilton, and Evans 2022). More recently, LLM-Eval (Lin\nand Chen 2023) adopts a multi-aspect evaluation schema for\nmore comprehensive evaluation. Specifically in the medical\ndomain, Vladika, Hacajová, and Matthes leverage multi-turn\ninteractions with LLMs to reach a binary decision on a sin-\ngle claim. However, these LLM judges can also suffer from\nhallucinations. In addition, all these model-based approaches\nlack sufficient explainability to the evaluation. In contrast,\nFAITH is unsupervised, grounded on medical facts, and of-\nfers inherent explainability to pinpoint the errors.\nAlso related are those works regarding hallucination detec-\ntion of LLMs (Ji et al. 2023; Zhang et al. 2023), which is a\n"}, {"page": 3, "text": "broader topic. Our work specifically targets one type of hal-\nlucination that stems from factual errors, and complements\nresearch on other types, e.g., confabulations (Farquhar et al.\n2024) or instruction misalignment (Zeng et al. 2024). Com-\nbining FAITH with other hallucination-detection methods\ncan enable more systematic safety oversight.\nKnowledge Graph-Based Fact-Checking\nIn general domains, external knowledge base like KGs, have\nbeen widely used for fact-checking (Gad-Elrab et al. 2019;\nCiampaglia et al. 2015; Shiralkar et al. 2017; Fionda and Pirrò\n2018; Shi and Weninger 2016; Kim et al. 2023; Dammu et al.\n2024; Huang et al. 2025; Yu et al. 2025; Huang, Zhou, and Li\n2024). Early approaches assessed the “quality” of paths link-\ning entities within each claim. For instance, KL (Ciampaglia\net al. 2015) suggests that paths involving generic entities\nprovide weaker support and uses node degrees for evaluation.\nKL-REL (Shiralkar et al. 2017) extends this by considering\nthe semantic similarity of edge labels in the path to the predi-\ncate in the original claim. Other methods like TransE (Bordes\net al. 2013) employ global representations of entities and\nrelationships to better capture hierarchical structures in KGs.\nWhile established, these methods often assume that a single\nfact is being checked each time and that its entities are directly\navailable in the KG, making them not directly applicable to\nLLMs’ free-text outputs.\nThe FAITH Evaluation Framework\nPreliminaries\nClaims of fact. A claim of fact is a triplet t = (s, r, o), where\ns, o denote the subject and object, respectively, and r is the\npredicate (i.e., relation between s and o). For example, the\ntriplet (Barack Obama, was_born_in, Hawaii) expresses the\nclaim that Barack Obama was born in Hawaii.\nKnowledge graphs (KGs). A knowledge graph G = (E, R)\nis composed of a set of entities (nodes) E and relations (edges)\nR among them. It can be regarded as a collection of factual\nclaims defined as T = E × R × E.\nKnowledge paths. A knowledge path p between entity e1 and\nek is a sequence of alternating entities and relations, denoted\nas p = (e1, r1, e2, r2, . . . , rk−1, ek), where ei ∈E(1 ≤i ≤\nk), ri ∈R(1 ≤i < k), and (ei, ri, ei+1) ∈T for 1 ≤i < k.\nThis path consists of k entities and k −1 relations. The path\nlength is defined as the number of relations it contains, k −1.\nComputational fact checking. Given an LLM-generated\nresponse D containing a set of medical factual claims {ti}n\ni=1\nand a KG G, the goal of computational fact checking is to\ndetermine the factuality of each claim triplet ti = (si, ri, oi)\nin D by leveraging the knowledge paths pi in G.\nKey Modules of FAITH\nAs shown in Fig. 1, FAITH comprises 4 key modules:\nMedical claim extraction. To extract medical claims {ti}n\ni=1\nfrom an LLM response D, we use GPT-4o for this purpose\ndue to its strong extraction capability (Dunn et al. 2022; Polak\nand Morgan 2024). Specifically, we use a multi-phase prompt-\ning strategy (Appendix B.1)1 that first identifies all medical\n1Appendix is available at: https://zenodo.org/records/17603819\nentities in D and then determines relationships among them.\nTo enhance performance, we employ in-context learning with\n5 clinician-crafted examples. We also adopt a multi-round\nprompting (Kuratov et al. 2024; Edge et al. 2024) to increase\nrecall, whilst a redundancy-inducing prompt is used to let the\nmodel critically reanalyze the extracted claims with suspicion\nto mitigate hallucination (Polak and Morgan 2024).\nMedical entity matching. In classic fact-checking litera-\nture (Ciampaglia et al. 2015; Shiralkar et al. 2017), bench-\nmarking datasets are often generated from the KG used in\nthe algorithm. Thus, entities in this data can always match\nwith KG nodes. Yet, in real-world applications, entities in\nthe LLM responses often cannot directly match with nodes\nin the KG (e.g., ‘haemoptysis’ and ‘Hemoptysis’ in Fig. 1c).\nTo resolve these discrepancies, we use entity resolution tech-\nniques (Binette and Steorts 2022) via the Unified Medical\nLanguage System (UMLS) (Bodenreider 2004). UMLS is a\ncomprehensive ontology linking synonymous medical terms\nfrom over 200 different biomedical vocabulary sources via\nconcept unique identifiers (CUIs). We use the UMLS API to\nmap entities from both claims and nodes in G to standardized\nCUIs. This provides a strong foundation of reliable, consen-\nsual knowledge, which is essential for a fact-checking system.\nClaims with unmatched entities are labeled as ‘unverifiable’\nand excluded from further evaluation. This represents a con-\nservative design choice, ensuring that FAITH only scores\nclaims for which it has a high-confidence knowledge base.\nKG traversal and factual evaluation. After entity match-\ning, we traverse the KG to extract the knowledge paths\nP = {pi}n\ni=1 connecting the resulting nodes, where each\npath p is a sequence (e1, r1, e2, r2, . . . , rk−1, ek). Given the\nvast scale and dense connectivity of medical KGs, an exhaus-\ntive search of all possible paths between two entities is com-\nputationally intractable. Therefore, we constrain our search\nto the shortest path, which represents the most direct and\nsalient connection. In addition to computational feasibility,\nthe shortest paths are also considered optimal for maximizing\nthe information content of the connection (Ciampaglia et al.\n2015). The obtained path pi can then serve as evidence to\nevaluate the factuality of the corresponding medical claim ti.\nIt is often considered that longer paths indicate weaker\nfactuality (Ciampaglia et al. 2015; Shiralkar et al. 2017), but\npath length alone is insufficient. This is because the seman-\ntics of relations between entities can significantly affect the\nfactuality of the claim. For example, the relations ‘indication’\nand ‘contraindication’ have totally different meanings when\ndirectly linking two entities. To include this, we introduce a\nrelation semantic similarity S(p, t) to assess the congruence\nbetween the semantics of the knowledge path p and the medi-\ncal claim t. We use the mean cosine similarity between the\nembeddings of the relations in p and the relation in t:\nS(p, t) =\n1\nk −1 ×\nk−1\nX\ni=1\ncos_sim(ri, ˆr),\n(1)\nwhere k −1 is the length of p, ri represents the i-th relation\nin p, and ˆr is the predicate in t.\nHowever, this is still inadequate to fully capture the factu-\nality of the medical claim t. Ciampaglia et al. (2015) suggest\n"}, {"page": 4, "text": "that paths involving generic entities provide weaker support\nto a claim. Moreover, Shiralkar et al. (2017) emphasize the\nimportance of considering the co-occurrence of each rela-\ntionship ri in p with t’s predicate to enhance the accuracy\nof factuality assessment. Building on these insights, FAITH\nincorporates the entity centrality (via PageRank, PR (Brin\nand Page 1998)) and relationship co-occurrence (u(ri, ˆr))\nderived from contracted line graphs. Putting these together,\nthe factuality score for a given medical claim t related to a\nknowledge path p is given by:\nW(p, t) = S(p, t)\n\" k−1\nX\ni=2\neαP R(ei)\nu(ri−1, ˆr) +\n1\nu(rk−1, ˆr)\n#−1\n, (2)\nwhere ei and ri are the i-th entity and relation in p, respec-\ntively, and α is a scaling constant set to 100 in this study\n(see Appendix B.2). The term u(ri, ˆr) is calculated as the\nco-occurrence between two relations based on contracted line\ngraph, with higher values indicating tighter relations.\nScoring and interpretation. Finally, the overall FAITH\nscore for the whole LLM-generated response D, which con-\ntains the set of claims {ti}n\ni=1 is aggregated from the individ-\nual factuality scores for each claim involved:\n¯\nW(P, D) = 1\nn ×\nX\npi∈P,ti∈D\nW(pi, ti).\n(3)\nFAITH generates factuality scores in the range of [−1, 1],\nwhere positive values indicate alignment with established\nmedical knowledge while negative values suggest contradic-\ntions. The absolute value reflects how strongly entities in a\nclaim are interconnected. Our scoring system ensures inter-\npretability by explicitly linking each assessment to the KG\npaths examined (see examples in Appendix D.1).\nExperimental Setup\nBaseline methods. We compare FAITH against the follow-\ning baselines: ▶NLP metrics: This includes BLEU-4 (Pap-\nineni et al. 2002) and ROUGE-L (Lin 2004). We also included\nBERTScore (Zhang et al. 2020) which leverages contextual\nembedding model (here voyage-3-large, a top-performer on\nMTEB leaderboard (Muennighoff et al. 2023)) to evaluate\nthe semantic similarity between reference and input. Addi-\ntionally, we considered MEDCON (Yim et al. 2023) as it\nis specifically designed for medical scenarios. ▶Computa-\ntional fact-checking methods: These include 3 established\nmethods: KL (Ciampaglia et al. 2015) and KL-REL (Shi-\nralkar et al. 2017), and TransE (Bordes et al. 2013). ▶LLM-\nbased method: We consider FActScore (Min et al. 2023)\nand imapScore (Wang et al. 2024) to represent this category.\nNote that as KL, KL-REL, and TransE cannot be directly ap-\nplied to free texts, we preprocess their inputs with our entity\nmatching module. NLP metrics and imapScore are provided\nwith reference responses for evaluation.\nLLMs. To compare the efficacy of our proposed approach\nwith the baselines, we considered 5 representative LLMs.\nThis includes 2 established proprietary models, GPT-4o-mini\nand GPT-4o (Hurst et al. 2024). We also included two open-\nsource models, Llama 3-8B and Llama 3.1-8B (Dubey et al.\n2024). We additionally included a specialized model, OpenBi-\noLLM, which is fine-tuned on medical texts and specifically\ndesigned to answer medical questions in an informative way.\nMedical tasks. To assess FAITH’s efficacy, we focused on\nmedical QA tasks as they purely demand a high factual ac-\ncuracy on medical knowledge. Other tasks, such as clinical\ndecision making, further require advanced reasoning capabili-\nties. While these are useful for evaluating the overall medical\ncapabilities of LLMs, they can introduce additional confound-\ning factors to our evaluation, which focuses on factuality. In\nthis spirit, we selected 4 established datasets: i) MedQA (Jin\net al. 2020) ii) MMLU (Hendrycks et al. 2021), iii) MS-\nAKT, and iv) LiveQA (Abacha et al. 2017). These span\nboth multiple-choice (MedQA, MMLU, MS-AKT) and open-\nended questions (LiveQA), with varying levels of complexity\nand coverage. For more details, see Appendix C.3.\nMedical KG. While many medical KGs exist (Lu et al. 2025;\nBodenreider 2004; Louden 2020; Cheung et al. 1996), many\nof them are tailored to focus on specific medical problems,\nwhereas our fact-checking would desire an encyclopedic\nKG that covers a wide range of medical information (see\ndiscussion in Appendix B.3). To this end, we consider the\nUMLS (Bodenreider 2004) 2025AA version for this paper.\nIt integrates knowledge from over 200 standard biomedical\nvocabulary sources, (e.g., MeSH, SNOMED CT and Human\nPhenotype Ontology), incorporating more than 23 million\nrelationships among 3.4 million concepts.\nExperimental Results\nReliability and Validity of FAITH\nAn effective evaluation metric for LLM-generated responses\nin healthcare should satisfy several key desiderata from prac-\ntitioners. To be specific, it should: i) clearly distinguish re-\nsponses produced by LLMs with different levels of capabil-\nities; ii) be robust to the variance or noise in the responses\nthat are not relevant to the evaluation; and iii) align with\nthe consensus of experienced clinicians. We thus evaluate\nFAITH on these 3 dimensions.\nDistinguishing LLM capabilities. We first probed FAITH’s\nability to differentiate 5 LLMs with varying medical capabil-\nities (Zhang et al. 2024; Boggavarapu et al. 2024; Ke et al.\n2025; Chen et al. 2024) by evaluating their responses on 4\nmedical QA tasks. We find that metrics such as ROUGE-L,\nBERTScore, and TransE on average rated all 5 models simi-\nlarly (paired t-test p > 0.05; Fig. 2a, b, e, and g). While other\nmetrics like BLEU-4, MEDCON, and FActScore achieved\nstatistically significant differences between some of the mod-\nels, this discrepancy does not persist across all models and\ntasks. For example, FActScore fails to distinguish between\nLlama 3.1 and GPT-4o-mini on MedQA (Fig. 2a). In con-\ntrast, FAITH consistently distinguished responses from all 5\nLLMs (p < 0.004). This indicates that the approach is sensi-\ntive enough to capture meaningful differences in the factual\nquality of outputs from models with varying capabilities.\nRobustness to variance. We then test FAITH’s robustness to\nvariations in response phrasing. To do this, we used GPT-4o\nto generate 10 paraphrased versions for each response from\nthe 5 LLMs (Appendix C.5). The paraphrasing instructions\n"}, {"page": 5, "text": "Performance\n0\n0.5\n1.0\nMMLU\nKL\nKL-REL\nTransE\nFActScore\nFAITH\nOpenBioLLM\nLlama 3\nLlama 3.1\nGPT-4o-mini\nGPT-4o\nPerformance\n0\n0.5\n1.0\nMedQA\nKL\nKL-REL\nTransE\nFActScore\nFAITH\nOpenBioLLM\nLlama 3\nLlama 3.1\nGPT-4o-mini\nGPT-4o\nCV\n0\n0.02\n0.04\nMedQA\nKL\nKL-REL\nTransE\nFActScore\nFAITH\nCV\n0\n0.02\n0.04\n0.06\nMMLU\nKL\nKL-REL\nTransE\nFActScore\nFAITH\nPerformance\n0\n0.5\n1.0\nMS-AKT\nBLEU-4ROUGE-L\nBERTScore\nMEDCON\nimapScore\nKL\nKL-REL\nTransEFActScore\nFAITH\nCV\n0\n0.5\n1.0\nMS-AKT\nBLEU-4ROUGE-L\nBERTScore\nMEDCON\nimapScore\nKL\nKL-REL\nTransEFActScore\nFAITH\n0\n0.05\nKL\nKL-REL\nTransE\nFActScore\nFAITH\nPerformance\n0\n0.5\n1.0\nLiveQA\nBLEU-4ROUGE-L\nBERTScore\nMEDCON\nimapScore\nKL\nKL-REL\nTransEFActScore\nFAITH\n0.02\n0.04\nBLEU-4\nCV\n0\n0.5\n1.0\nLiveQA\nBLEU-4ROUGE-L\nBERTScore\nMEDCON\nimapScore\nKL\nKL-REL\nTransEFActScore\nFAITH\n0.02\n0.04\n0.06\nKL\nKL-REL\nTransE\nFActScore\nFAITH\na\nb\nc\nd\ne\nf\ng\nh\nFigure 2: FAITH effectively distinguishes between LLMs and is robust to noise. This figure shows the mean factuality scores assigned by\nFAITH and baseline metrics to responses from five LLMs on four datasets: MedQA (a), MMLU (b), MS-AKT (e), and LiveQA (g). A reliable\nmetric should assign distinguishable scores across different models. Panels c, d, f, and h display the corresponding coefficients of variation\n(CV) of these scores under noisy conditions, introduced by generating 10 paraphrased versions per response.\nemphasized modifying sentence structures, pronouns, and\nprepositions while preserving medical terminology and rela-\ntionships. An ideal metric should yield low variance across\nsuch paraphrased responses. We selected the coefficient of\nvariation (CV) for this purpose, as it offers a measure of\nrelative variability. Across all scenarios, FAITH exhibits con-\nsistently low CV (Fig. 2c, d, f, and h), with the average ±\ns.d. being 0.014 ± 0.005. In contrast, metrics like BLEU-4\nshowed much higher variability (0.910 ± 0.862). This sug-\ngests that FAITH is more robust to the variations in phrasing.\nCorrelation with clinician judgments. Finally, to determine\nthe alignment of FAITH with human clinicians, we recruited\n20 UK-based clinicians to manually grade the responses gen-\nerated by LLMs. Each clinician holds a medical degree and\npossesses at least 5 years of clinical practice experience. In\nclose consultation with these experts, we identified 3 key\nevaluation dimensions: i) factuality, ii) relevance, and iii)\npotential harm. Each dimension was rated on a 5-point Likert\nscale (with 1 being the lowest and 5 the highest; Appendix\nC.6), and an additional overall assessment also used the same\nscale. Following these criteria, each clinician assessed 5\nresponses, which were randomly drawn from a candidate\nquestion pool of 16 questions from MS-AKT dataset. These\nresponses could either originate from Llama 3.1, GPT-4o,\nor official answers. To ensure rating consistency, each re-\nsponse was evaluated by at least two clinicians, with Cohen’s\nκ = 0.64, indicating substantial inter-rater agreement.\nAs shown in Fig. 3a, clinician assessments consistently\nranked official answers highest across all evaluation axes,\nOfficial answer\nGPT-4o\nLlama 3.1\nRating\n0\n2\n4\nOverall eval.\nFactuality\nHarmRelevance\n0.86 0.36\n0.49\n0.68\n0.63\n0.46\n0.02\n−0.2\n−0.12\n−0.13\n0.11\n0.04\n0.27\n0.01\n−0.26\n0.12\n0.12\n0.23\n0.12\n−0.2\n0.88\n0.17\n0.12\n0.02\n0.2\n0.35\n−0.12\n−0.06\n−0.09\n−0.04\n−0.19\n−0.23\n0.02\n0.07\n−0.14\n−0.04\n0.07\n0.12\n0.14\n0.01\n0.07\n0.39\n0.59\n−0.04\n−0.31\n0.08\n0.06\n0.15\n0.01\n−0.14\n0.32\n0.6\n−0.11\n−0.23\n0.7\n−0.31\n−0.02\n0.28\n0.58\n0.88\nROUGE.L\nBERTScore\nMEDCON\nimapScore\nKL\nKL.REL\nTransE\nFActScore\nFAITH\nClinicians\nBLEU−4\nROUGE−L\nBERTScore\nMEDCON\nimapScore\nKL\nKL−REL\nTransE\nFActScore\nFAITH\nPearson's ρ=0.696\nClinician Evaluation\n2\n4\n6\nFAITH\n0\n0.2\n0.4\na\nb\nc\nFigure 3: FAITH exhibits highest correlation with clinician judg-\nments. a, Clinician evaluation scores for answers to 16 questions\ngenerated by GPT-4o, Llama 3.1, and official answers. b, Scat-\nter plots correlating clinician judgments with scores from FAITH.\nLinear regression fits are shown with 95% confidence intervals. c,\nPearson correlation coefficients (ρ) between clinicians and scores\nfrom FAITH and various baselines.\nfollowed by GPT-4o, and then Llama 3.1. To quantify align-\nment between automated metrics and these human judgments,\nwe computed Pearson’s correlation coefficient ρ. The results\nshow that FAITH scores achieve a considerably higher cor-\nrelation with clinicians (Pearson’s ρ = 0.696; Fig. 3b, c)\ncompared to all baselines. This provides strong evidence for\nthe effectiveness of the KG-based evaluation approach. No-\ntably, baselines like BLEU-4 only has ρ = 0.081, consistent\n"}, {"page": 6, "text": "Factuality Explanation\nPositive\nNegative\nPositive\nNegative\nClinician Annotation\n47\n32\n25\ndisease_phenotype\nexposure_disease\ndisease_disease\ndrug_effect\nindication\n47.50%\n23.75%\n16.87%\n8.12%\n3.75%\na\nb\nFigure 4: Explainability of FAITH via faithful error identifica-\ntion and LLM limitation analysis. a, Alignment between clinician-\nidentified incorrect claims and FAITH’s lowest-scoring claims in\nLLM responses, shown by a confusion matrix. b, Distribution of the\ntop-5 most frequent KG relation types linked to incorrect claims in\nGPT-4o’s responses, as identified by FAITH.\nwith previous findings (Van Veen et al. 2024).\nExplainability of FAITH\nWe evaluate FAITH’s interpretability with 2 aspects: i): the\nfaithfulness of its explanations to medical consensus, and ii):\nthe utility of these explanations for analyzing LLM behaviors.\nFaithfulness to medical consensus. For the first aspect, we\nadditionally asked clinicians to annotate the one most incor-\nrect medical claim in each justification they had previously\nrated the factuality score lower than 5. These annotations\nwere manually converted to fact triplets to be comparable\nwith the factuality explanations generated by FAITH. We\nevaluated FAITH’s ability to pinpoint these specific annota-\ntions, achieving a precision of 0.65, recall of 0.59, and F1\nscore of 0.62 (Fig. 4a). It is important to consider that the\nselection of the most incorrect claim by clinicians can involve\nsubjective considerations (e.g., specific expertise, perceived\nclaim importance). We therefore evaluated its performance\nin localizing these claims and found that in 83.6% of these\njustifications, the erroneous statement pinpointed by clini-\ncians was ranked by FAITH among the top-5 lowest-scoring.\nThese findings suggest that FAITH’s explanations could be\nfaithful to current medical consensus, offering valuable, gran-\nular insights into response inaccuracies.\nUtility in analyzing LLM limitations. Such explanations\noffered by FAITH can be potentially utilized to offer insights\ninto limitations of LLMs. To show this, we investigated the\ntop-5 most frequent edge types in the KG that are associated\nwith the most incorrect claims made in GPT-4o’s responses,\nas identified by FAITH. As shown in Fig. 4b, nearly half of\nthe identified inaccuracies involve phenotypical features of\ndiseases. Such errors are particularly prominent when LLMs\nattempt to determine a disease from patient signs, which un-\nderscores their current challenges in the rigorous clinical task\nof differentiating between numerous potential pathologies for\na given initial complaint (Hager et al. 2024). Furthermore,\nfrequent errors in establishing causal relationships between\ndiseases and exposures highlight LLM tendencies to over-\nstate associations or assert causality without sufficient evi-\ndence (Griot et al. 2025; Pawitan and Holmes 2024). This\ngranular error typology (Appendix D.3), is vital for guiding\ntarget LLM refinement in complex medical applications.\nRTA\nRAG\nAccuracy\n0.6\n0.7\n0.8\n0.9\nIndex Uncer.\nExp. Uncer.\nOurs\nRTA\nRAG\nOur score\n0.4\n0.5\n0.6\nIndex Uncer.\nExp. Uncer.\nOurs\nFigure 5: FAITH enhances LLM factuality via selective inter-\nvention. GPT-4o performance on MedQA using Reject-to-Answer\n(RTA) or Retrieval-Augmented Generation (RAG). Interventions\ntriggered by either FAITH scores or a model uncertainty baseline.\na, Question-answering accuracy. b, FAITH factuality scores for re-\nsponses. Metrics plotted against the percentage (x-axis) of responses\nselected for intervention.\nPractical Utility of FAITH for Safeguarding\nBeyond evaluation, FAITH can be used to improve the fac-\ntuality and safety of LLM-generated medical responses. We\nshow this in two intervention scenarios. In the first one, we\nuse FAITH scores below a set threshold to activate a Reject-\nto-Answer (RTA) protocol, filtering out responses with low\nfactuality (Singhal et al. 2023; Wen et al. 2024). In the sec-\nond one, we augment the LLMs with a RAG module (Gao\net al. 2023) that retrieves relevant information from the med-\nical KG when potential factual errors are detected. Model\nuncertainty served as a baseline triggering mechanism (see\nAppendix B.4). For both scenarios, intervention thresholds\nwere determined by percentiles of the FAITH or uncertainty\nscore distribution in the dataset (ranging from 5% to 50%).\nOur findings show that by applying the RTA protocol, both\nthe accuracy of answers and the factuality of the justifica-\ntion generated by GPT-4o could be significantly improved\n(Fig. 5). While uncertainty-based thresholding could also\nlead to boosted results, the improvements are not as signif-\nicant as those achieved by FAITH. Likewise, RAG could\nalso enhance both metrics, particularly the factuality of the\njustification. Instead of employing RAG for all questions,\nthe use of FAITH as a thresholding mechanism can offer a\nmore cost-effective solution in practice. This advantage also\npersists if we consider other LLMs (Fig. A16).\nBroader Applicability of FAITH\nTo interrogate FAITH’s utility beyond medical QA, we\nfurther assessed its performance on medical summariza-\ntion (Shaib et al. 2023; Tang et al. 2023; Thawakar et al.\n2024; Van Veen et al. 2024) and medical fact verifica-\ntion (MFV) (Vladika, Schneider, and Matthes 2024; Mohr,\nWührl, and Klinger 2022) (details available in Appendix\nC.4). For medical summarization, we used the FactPICO\nbenchmark (Joseph et al. 2024), which comprises expert\nfactuality assessments of 345 LLM-generated summaries of\n115 randomized controlled trials (RCTs) across the Alpaca,\nLlama-2 and GPT-4 models. Results shown in Fig. 6a indi-\ncate that FAITH aligns closely with the expert-rated model\nrankings (Alpaca > GPT-4 > Llama-2). More importantly,\n"}, {"page": 7, "text": "Alpaca\nLlama 2\nGPT-4\nPerformance\n0\n0.5\n1.0\nFactPICO\nKL\nKL-REL\nTransE\nFActScore\nFAITH\n0.85\n0.16\n0.13\n0.04\n0.03\n0.08\n0.32\n0.51\n0.02\n0.06\n0.39\n0.41\n0.02\n0.37\n0.61\n0.02\n0.23\n0.43\n0.64\n0.85\nKL_REL\nTransE\nFActScore\nFAITH\nHuman\nKL\nKL_REL\nTransE\nFActScore\nFAITH\nFact\nIncorrect\nNot sure\nMean=-0.023\nMean=0.071\nMean=0.388\nDensity\n0\n2\n4\n6\nFAITH Score\n−1.0\n−0.5\n0\n0.5\n1.0\nMean=0.01\nMean=-0.22\nMean=0.30\nDensity\n0\n1\n2\nFAITH Score\n−1.0\n−0.5\n0\n0.5\n1.0\na\nb\nc\nd\nFigure 6: Broad applicability of FAITH to medical summariza-\ntion and fact verification (MFV). a, Factuality scores assigned by\nFAITH to LLM-generated medical summaries from the FactPICO\nbenchmark, reflecting alignment with model performance hierar-\nchies. b, Pearson correlations of FAITH and baselines with expert\njudgements on FactPICO. c, d, Distribution of FAITH scores for\nclaims of known veracity from the HealthFC (c) and BEAR-FACT\n(d) MFV benchmarks, illustrating clear differentiation between true\nand false statements.\nFAITH achieves the highest Pearson’s correlation with expert\njudgments (ρ = 0.61; Fig. 6b). For MFV task, we assessed\nFAITH on the HealthFC (Vladika, Schneider, and Matthes\n2024) and BEAR-FACT (Wührl et al. 2024) datasets, which\nconsist of 750 and 300 medical claims, respectively. The re-\nsults show that FAITH effectively distinguished true from\nfalse claims (Fig. 6c, d, Fig. A17, and Fig. A18).\nAblation Studies\nSensitivity to KG Choice and Integrity\nA critical question for any KG-based approach is how its\neffectiveness depends on the choice of KG.\nCompatibility with different KGs. To validate this, we\nintegrated FAITH with other established KGs of varying\nscales, PrimeKG (Chandak, Huang, and Zitnik 2023) and\nOGBL-biokg (Hu et al. 2020) (Appendix B.3), and repeated\nour core experiments. The results show that while relative\nranking of LLMs remains consistent, the performance will\nbe affected by different KGs (Fig. A11).\nRobustness against KG noise. Given the real-world KGs\ninevitably contain noise (e.g., missing/erroneous triplets), we\nassessed FAITH’s robustness by perturbing the UMLS. We\napplied 3 types of significant (Albert, Jeong, and Barabási\n2000) noise: i) 20% random edge deletion, ii) 20% random\nnode deletion (including incident edges), and iii) 20% ran-\ndom noisy edge insertion. The findings in Fig. A12 showed\nthat deleting entire nodes or inserting noisy edges cause per-\nformance degradation. This demonstrates that high-quality\nKGs remain a crucial prerequisite.\nReliability of Medical Claim Extraction Module\nTo assess the medical claim extraction pipeline in FAITH,\nwe compare various extractor implementations against Quick-\nUMLS (Soldaini and Goharian 2016) on the BioRED (Luo\net al. 2022) and DDI13 (Segura-Bedmar, Martínez, and\nHerrero-Zazo 2013) datasets (details in Appendix C.7).\nImpact of prompting strategies. We first tested how our\nmulti-round and critical analysis prompting affect extrac-\ntion performance. We thus implemented the LLM extrac-\ntor in FAITH with 4 prompting strategies: ▶Base prompt:\na basic extraction prompt without advanced techniques. ▶\nBase + critical: the base prompt with critical analysis. ▶\nBase + multi: the base prompt with multi-round conversation.\n▶FAITH: FAITH’s full prompting strategy. As shown in\nFig. A13, all LLM-based variants surpass QuickUMLS on\nboth datasets. Multi-round conversation mainly boosts recall,\nwhile the critical analysis prompts helps ensure the precision,\nmaking their combination most effective.\nSensitivity to LLM choice. We then examine the sensitivity\nof the claim extraction performance to the specific choice of\nLLM. For this, we replaced the GPT-4o model with 4 other\nLLMs and repeated the experiments on BioRED and DDI13.\nThe results in Fig. A14 showed that while changing to cheap\nopen-source models like Llama 3 and Llama 3.1 leads to a\nslight decrease, all LLM extractors still yield considerable\nimprovements over QuickUMLS. On the other hand, the\nperformance gain of GPT-4o over GPT-4o-mini is marginal,\nso GPT-4o model is already well-suited for the task.\nDiscussion and Conclusion\nThis paper investigated the reliability of using medical KGs\nas a foundation for evaluating LLM factuality in healthcare.\nThrough our framework, FAITH, we demonstrated that lever-\naging KGs is a powerful and reliable approach for LLM\nfactuality evaluation in healthcare.\nHowever, our work also illuminates the some limitations of\nthis approach. The first one is its reliance on the KG’s quality\nand coverage. Factual claims involving knowledge absent\nfrom the KG cannot be verified, which can lead to potential\nfalse negatives. Secondly, the entire pipeline is dependent on\nthe performance of the upstream medical claim extraction\nmodule. While our LLM-based extractor proved effective,\nerrors at this stage can propagate through the system. Future\nwork should focus on mitigating these limitations, perhaps\nby integrating multiple KGs or developing methods to handle\nclaims that are out-of-scope for the KG.\nSocial impact. FAITH enhance the safety and reliability of\nLLMs (Zhou et al. 2024; Zhou, Li, and Min 2022b,a) by\nautomatically assessing the factuality of their responses. It\nserves as a crucial “guardrail” against misinformation, foster-\ning trustworthy adoption by clinicians and regulators while\nguiding developers with targeted feedback for model improve-\nment. The core approach is also adaptable for fact-checking\nin other critical domains like law (Cui et al. 2023; Hamdani\net al. 2024) and finance (Xie et al. 2023; Kang and Liu 2023)\nwhere factuality is paramount.\nCode availability. Source code of FAITH can be found at:\nhttps://github.com/COLA-Laboratory/FAITH.\n"}, {"page": 8, "text": "Acknowledgements\nWe sincerely thank all the reviewers for their encouraging and\nconstructive feedback. This work was supported by the UKRI\nFuture Leaders Fellowship under Grant MR/S017062/1 and\nMR/X011135/1; in part by NSFC under Grant 62376056\nand 62076056; in part by the Royal Society Faraday Discov-\nery Fellowship (FDF/S2/251014), BBSRC Transformative\nResearch Technologies (UKRI1875), Royal Society Inter-\nnational Exchanges Award (IES/R3/243136), Kan Tong Po\nFellowship (KTP/R1/231017); and the Amazon Research\nAward and Alan Turing Fellowship.\nReferences\nAbacha, A. B.; Agichtein, E.; Pinter, Y.; and Demner-\nFushman, D. 2017. Overview of the medical question an-\nswering task at TREC 2017 LiveQA. In TREC, 1–12.\nAlber, D. A.; Yang, Z.; Alyakin, A.; Yang, E.; Rai, S.; Val-\nliani, A. A.; Zhang, J.; Rosenbaum, G. R.; Amend-Thomas,\nA. K.; Kurland, D. B.; Kremer, C. M.; Eremiev, A.; Ne-\ngash, B.; Wiggan, D. D.; Nakatsuka, M. A.; Sangwon, K. L.;\nNeifert, S. N.; Khan, H. A.; Save, A. V.; Palla, A.; Grin,\nE. A.; Hedman, M.; Nasir-Moin, M.; Liu, X. C.; Jiang, L. Y.;\nMankowski, M. A.; Segev, D. L.; Aphinyanaphongs, Y.; Ri-\nina, H. A.; Golfinos, J. G.; Orringer, D. A.; Kondziolka, D.;\nand Oermann, E. K. 2025. Medical large language models\nare vulnerable to data-poisoning attacks. Nat. Med.\nAlbert, R.; Jeong, H.; and Barabási, A.-L. 2000. Error and\nattack tolerance of complex networks. nature, 406(6794):\n378–382.\nAugenstein, I.; Baldwin, T.; Cha, M.; Chakraborty, T.;\nCiampaglia, G. L.; Corney, D. P. A.; DiResta, R.; Ferrara,\nE.; Hale, S.; Halevy, A. Y.; Hovy, E. H.; Ji, H.; Menczer, F.;\nMíguez, R.; Nakov, P.; Scheufele, D.; Sharma, S.; and Zagni,\nG. 2024. Factuality challenges in the era of large language\nmodels and opportunities for fact-checking. Nat. Mac. Intell.,\n6(8): 852–863.\nBinette, O.; and Steorts, R. C. 2022. (Almost) all of entity\nresolution. Science Advances, 8(12): eabi8021.\nBodenreider, O. 2004. The Unified Medical Language Sys-\ntem (UMLS): integrating biomedical terminology. Nucleic\nAcids Res., 32(suppl_1): 267–270.\nBoggavarapu, L.; Srivastava, V.; Varanasi, A. M.; Lu, Y.; and\nBhaumik, R. 2024. Evaluating Enhanced LLMs for Precise\nMental Health Diagnosis from Clinical Notes.\nmedRxiv,\n2024–12.\nBordes, A.; Usunier, N.; García-Durán, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating Embeddings for Modeling\nMulti-relational Data. In NIPS’13: Proc. of the Advances in\nNeural Information Processing Systems 26, 2787–2795.\nBrin, S.; and Page, L. 1998. The Anatomy of a Large-Scale\nHypertextual Web Search Engine. Comput. Networks, 30(1-\n7): 107–117.\nChandak, P.; Huang, K.; and Zitnik, M. 2023. Building a\nknowledge graph to enable precision medicine. Scientific\nData, 10(1): 67.\nChen, H.; Shen, X.; Wang, J.; Wang, Z.; Lv, Q.; He, J.; Wu,\nR.; Wu, F.; and Ye, J. 2025. Knowledge Graph Finetuning\nEnhances Knowledge Manipulation in Large Language Mod-\nels. In ICLR’25: The Thirteenth International Conference on\nLearning Representations.\nChen, S.; Gao, M.; Sasse, K.; Hartvigsen, T.; Anthony, B.;\nFan, L.; Aerts, H. J.; Gallifant, J.; and Bitterman, D. S. 2024.\nThe Risks of Medical Misinformation Generation in Large\nLanguage Models. Available at SSRN 5020664.\nChern, I.; Chern, S.; Chen, S.; Yuan, W.; Feng, K.; Zhou,\nC.; He, J.; Neubig, G.; and Liu, P. 2023. FacTool: Factual-\nity Detection in Generative AI - A Tool Augmented Frame-\nwork for Multi-Task and Multi-Domain Scenarios. CoRR,\nabs/2307.13528.\nCheung, K.-H.; Nadkarni, P.; Silverstein, S.; Kidd, J. R.;\nPakstis, A. J.; Miller, P.; and Kidd, K. K. 1996. PhenoDB: an\nintegrated client/server database for linkage and population\ngenetics. Computers and biomedical research, 29(4): 327–\n337.\nChristophides, V.; Efthymiou, V.; Palpanas, T.; Papadakis,\nG.; and Stefanidis, K. 2021. An Overview of End-to-End\nEntity Resolution for Big Data. ACM Comput. Surv., 53(6):\n127:1–127:42.\nCiampaglia, G. L.; Shiralkar, P.; Rocha, L. M.; Bollen,\nJ.; Menczer, F.; and Flammini, A. 2015.\nComputational\nfact checking from knowledge networks. PloS one, 10(6):\ne0128193.\nCui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. ChatLaw:\nOpen-Source Legal Large Language Model with Integrated\nExternal Knowledge Bases. CoRR, abs/2306.16092.\nDammu, P. P. S.; Naidu, H.; Dewan, M.; Kim, Y.; Roosta,\nT.; Chadha, A.; and Shah, C. 2024. ClaimVer: Explainable\nClaim-Level Verification and Evidence Attribution of Text\nThrough Knowledge Graphs. In EMNLP’24: Findings of the\nAssociation for Computational Linguistics: EMNLP 2024,\n13613–13627. Association for Computational Linguistics.\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\net al. 2024. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783.\nDunn, A.; Dagdelen, J.; Walker, N.; Lee, S.; Rosen, A. S.;\nCeder, G.; Persson, K. A.; and Jain, A. 2022. Structured\ninformation extraction from complex scientific text with fine-\ntuned large language models. CoRR, abs/2212.05238.\nEdge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody,\nA.; Truitt, S.; and Larson, J. 2024. From Local to Global:\nA Graph RAG Approach to Query-Focused Summarization.\nCoRR, abs/2404.16130.\nFarquhar, S.; Kossen, J.; Kuhn, L.; and Gal, Y. 2024. Detect-\ning hallucinations in large language models using semantic\nentropy. Nature, 630(8017): 625–630.\nFionda, V.; and Pirrò, G. 2018. Fact Checking via Evidence\nPatterns. In IJCAI’18: Proc. of the Twenty-Seventh Interna-\ntional Joint Conference on Artificial Intelligence, 3755–3761.\nijcai.org.\n"}, {"page": 9, "text": "Fu, J.; Ng, S.; Jiang, Z.; and Liu, P. 2024. GPTScore: Eval-\nuate as You Desire. In NAACL’24: Proc. of the 2024 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics, 6556–6576. Association for\nComputational Linguistics.\nGad-Elrab, M. H.; Stepanova, D.; Urbani, J.; and Weikum,\nG. 2019. ExFaKT: A Framework for Explaining Facts over\nKnowledge Graphs and Text. In WSDM’19: Proc. of the\nTwelfth ACM International Conference on Web Search and\nData Mining, 87–95. ACM.\nGao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.;\nSun, J.; Guo, Q.; Wang, M.; and Wang, H. 2023. Retrieval-\nAugmented Generation for Large Language Models: A Sur-\nvey. CoRR, abs/2312.10997.\nGriot, M.; Hemptinne, C.; Vanderdonckt, J.; and Yuksel,\nD. 2025. Large language models lack essential metacogni-\ntion for reliable medical reasoning. Nature communications,\n16(1): 642.\nHager, P.; Jungmann, F.; Holland, R.; Bhagat, K.; Hubrecht,\nI.; Knauer, M.; Vielhauer, J.; Makowski, M.; Braren, R.;\nKaissis, G.; et al. 2024. Evaluation and mitigation of the\nlimitations of large language models in clinical decision-\nmaking. Nature medicine, 30(9): 2613–2622.\nHamdani, R. E.; Bonald, T.; Malliaros, F. D.; Holzenberger,\nN.; and Suchanek, F. M. 2024. The Factuality of Large\nLanguage Models in the Legal Domain. In CIKM’24: Proc.\nof the 33rd ACM International Conference on Information\nand Knowledge Management, 3741–3746. ACM.\nHamza, A.; Abdullah; Ahn, Y. H.; Lee, S.; and Kim, S. T.\n2025. LLaVA Needs More Knowledge: Retrieval Augmented\nNatural Language Generation with Knowledge Graph for\nExplaining Thoracic Pathologies. In AAAI’25: Sponsored by\nthe Association for the Advancement of Artificial Intelligence,\n3311–3319. AAAI Press.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Multi-\ntask Language Understanding. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nHu, W.; Fey, M.; Zitnik, M.; Dong, Y.; Ren, H.; Liu, B.;\nCatasta, M.; and Leskovec, J. 2020. Open Graph Benchmark:\nDatasets for Machine Learning on Graphs. In NeurIPS’20:\nAdvances in Neural Information Processing Systems.\nHuang, M.; Zhou, S.; Chen, Y.; and Li, K. 2025. Conver-\nsational Exploration of Literature Landscape with LitChat.\nIn IJCAI’25: Proc. of the Thirty-Fourth International Joint\nConference on Artificial Intelligence, 11058–11061. ijcai.org.\nHuang, M.; Zhou, S.; and Li, K. 2024. Mapping Literature\nLandscapes with Data-Driven Discovery: A Case Study on\nMOEA/D. CoRR, abs/2404.14228.\nHurst, A.; Lerer, A.; Goucher, A. P.; and et al. 2024. GPT-4o\nSystem Card. CoRR, abs/2410.21276.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii,\nE.; Bang, Y.; Madotto, A.; and Fung, P. 2023. Survey of\nHallucination in Natural Language Generation. ACM Comput.\nSurv., 55(12): 248:1–248:38.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.; Fang, H.; and\nSzolovits, P. 2020. What Disease does this Patient Have? A\nLarge-scale Open Domain Question Answering Dataset from\nMedical Exams. CoRR, abs/2009.13081.\nJoseph, S.; Chen, L.; Trienes, J.; Göke, H. L.; Coers, M.; Xu,\nW.; Wallace, B. C.; and Li, J. J. 2024. FactPICO: Factuality\nEvaluation for Plain Language Summarization of Medical\nEvidence. In ACL’24: Proc. of the 62nd Annual Meeting of\nthe Association for Computational Linguistics, 8437–8464.\nAssociation for Computational Linguistics.\nKang, H.; and Liu, X. 2023. Deficiency of Large Language\nModels in Finance: An Empirical Examination of Hallucina-\ntion. CoRR, abs/2311.15548.\nKe, Y.; Jin, L.; Elangovan, K.; Abdullah, H. R.; Liu, N.;\nSia, A. T. H.; Soh, C. R.; Tung, J. Y. M.; Ong, J. C. L.;\nKuo, C. F.; Wu, S.; Kovacheva, V. P.; and Ting, D. S. W.\n2025. Retrieval augmented generation for 10 large language\nmodels and its generalizability in assessing medical fitness.\nnpj Digit. Medicine, 8(1).\nKim, J.; Park, S.; Kwon, Y.; Jo, Y.; Thorne, J.; and Choi, E.\n2023. FactKG: Fact Verification via Reasoning on Knowl-\nedge Graphs. In ACL’23: Proc. of the 61st Annual Meeting\nof the Association for Computational Linguistics ACL 2023,\n16190–16206. Association for Computational Linguistics.\nKuratov, Y.; Bulatov, A.; Anokhin, P.; Sorokin, D.; Sorokin,\nA.; and Burtsev, M. 2024. In search of needles in a 10m\nhaystack: Recurrent memory finds what llms miss. arXiv\npreprint arXiv:2402.10790.\nLee, P.; Bubeck, S.; and Petro, J. 2023. Benefits, Limits, and\nRisks of GPT-4 as an AI Chatbot for Medicine. New England\nJournal of Medicine, 388(13): 1233–1239.\nLin, C.-Y. 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLin, S.; Hilton, J.; and Evans, O. 2022. TruthfulQA: Mea-\nsuring How Models Mimic Human Falsehoods. In ACL’22:\nProc. of the 60th Annual Meeting of the Association for Com-\nputational Linguistics, 3214–3252. Association for Compu-\ntational Linguistics.\nLin, Y.; and Chen, Y. 2023.\nLLM-Eval: Unified\nMulti-Dimensional Automatic Evaluation for Open-Domain\nConversations with Large Language Models.\nCoRR,\nabs/2305.13711.\nLiu, X.; Liu, H.; Yang, G.; Jiang, Z.; Cui, S.; Zhang, Z.;\nWang, H.; Tao, L.; Sun, Y.; Song, Z.; Hong, T.; Yang, J.; Gao,\nT.; Zhang, J.; Li, X.; Zhang, J.; Sang, Y.; Yang, Z.; Xue, K.;\nWu, S.; Zhang, P.; Yang, J.; Song, C.; and Wang, G. 2025.\nA generalist medical language model for disease diagnosis\nassistance. Nat. Med.\nLiu, Y.; Iter, D.; Xu, Y.; Wang, S.; Xu, R.; and Zhu, C. 2023.\nG-Eval: NLG Evaluation using Gpt-4 with Better Human\nAlignment. In EMNLP’23: Proc. of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, 2511–\n2522. Association for Computational Linguistics.\nLouden, D. N. 2020. MedGen: NCBI’s portal to information\non medical conditions with a genetic component. Medical\nReference Services Quarterly, 39(2): 183–191.\n"}, {"page": 10, "text": "Lu, Y.; Goi, S. Y.; Zhao, X.; and Wang, J. 2025. Biomedical\nKnowledge Graph: A Survey of Domains, Tasks, and Real-\nWorld Applications. CoRR, abs/2501.11632.\nLuo, L.; Lai, P.; Wei, C.; Arighi, C. N.; and Lu, Z. 2022.\nBioRED: a rich biomedical relation extraction dataset. Brief-\nings Bioinform., 23(5).\nMcDuff, D.; Schaekermann, M.; Tu, T.; Palepu, A.; Wang,\nA.; Garrison, J.; Singhal, K.; Sharma, Y.; Azizi, S.; Kulkarni,\nK.; et al. 2025. Towards accurate differential diagnosis with\nlarge language models. Nature, 1–7.\nMehri, S.; and Eskénazi, M. 2020. Unsupervised Evaluation\nof Interactive Dialog with DialoGPT. In SIGdial’20: Proc.\nof the 21th Annual Meeting of the Special Interest Group on\nDiscourse and Dialogue, 225–235.\nMin, S.; Krishna, K.; Lyu, X.; Lewis, M.; Yih, W.; Koh,\nP. W.; Iyyer, M.; Zettlemoyer, L.; and Hajishirzi, H. 2023.\nFActScore: Fine-grained Atomic Evaluation of Factual Pre-\ncision in Long Form Text Generation. In EMNLP’23: Proc.\nof the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 12076–12100. Association for Com-\nputational Linguistics.\nMohr, I.; Wührl, A.; and Klinger, R. 2022. CoVERT: A\nCorpus of Fact-checked Biomedical COVID-19 Tweets. In\nLREC’22: Proc. of the Thirteenth Language Resources and\nEvaluation Conference, 244–257. European Language Re-\nsources Association.\nMuennighoff, N.; Tazi, N.; Magne, L.; and Reimers, N. 2023.\nMTEB: Massive Text Embedding Benchmark. In EACL,\n2006–2029. Association for Computational Linguistics.\nNori, H.; King, N.; McKinney, S. M.; Carignan, D.; and\nHorvitz, E. 2023. Capabilities of GPT-4 on Medical Chal-\nlenge Problems. CoRR, abs/2303.13375.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W. 2002. BLEU:\na Method for Automatic Evaluation of Machine Translation.\nIn ACL’02: Proc. of the 40th Annual Meeting of the Associa-\ntion for Computational Linguistics, 311–318. ACL.\nPawitan, Y.; and Holmes, C. 2024. Confidence in the Rea-\nsoning of Large Language Models. CoRR, abs/2412.15296.\nPolak, M. P.; and Morgan, D. 2024. Extracting accurate mate-\nrials data from research papers with conversational language\nmodels and prompt engineering. Nat. Commun., 15(1): 1569.\nPoulain, R.; and Beheshti, R. 2024. Graph Transformers\non EHRs: Better Representation Improves Downstream Per-\nformance. In ICLR’24: Proc. of the Twelfth International\nConference on Learning Representations, ICLR 2024. Open-\nReview.net.\nSaab et al., K. 2024.\nCapabilities of Gemini Models in\nMedicine. CoRR, abs/2404.18416.\nSai, A. B.; Mohankumar, A. K.; Arora, S.; and Khapra, M. M.\n2020. Improving Dialog Evaluation with a Multi-reference\nAdversarial Dataset and Large Scale Pretraining. Trans. As-\nsoc. Comput. Linguistics, 8: 810–827.\nSegura-Bedmar, I.; Martínez, P.; and Herrero-Zazo, M. 2013.\nSemEval-2013 Task 9 : Extraction of Drug-Drug Interactions\nfrom Biomedical Texts (DDIExtraction 2013). In NAACL’13:\nProc. of the 7th International Workshop on Semantic Evalua-\ntion, SemEval@NAACL-HLT 2013, 341–350. The Associa-\ntion for Computer Linguistics.\nShaib, C.; Li, M. L.; Joseph, S.; Marshall, I. J.; Li, J. J.;\nand Wallace, B. C. 2023. Summarizing, Simplifying, and\nSynthesizing Medical Evidence using GPT-3 (with Varying\nSuccess). In ACL’23: Proc. of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 2:\nShort Papers), 1387–1407. Association for Computational\nLinguistics.\nShen, Y.; Heacock, L.; Elias, J.; Hentel, K. D.; Reig, B.;\nShih, G.; and Moy, L. 2023. ChatGPT and Other Large\nLanguage Models Are Double-edged Swords. Radiology,\n307(2): e230163.\nShi, B.; and Weninger, T. 2016. Discriminative predicate\npath mining for fact checking in knowledge graphs. Knowl.\nBased Syst., 104: 123–133.\nShiralkar, P.; Flammini, A.; Menczer, F.; and Ciampaglia,\nG. L. 2017. Finding Streams in Knowledge Graphs to Sup-\nport Fact Checking. In ICDM’17: Proc. in the 2017 IEEE\nInternational Conference on Data Mining, 859–864. IEEE\nComputer Society.\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\net al. 2023. Large language models encode clinical knowl-\nedge. Nature, 620(7972): 172–180.\nSoldaini, L.; and Goharian, N. 2016. Quickumls: a fast,\nunsupervised approach for medical concept extraction. In\nMedIR workshop, sigir, 1–4.\nTang, L.; Sun, Z.; Idnay, B. R. S.; Nestor, J. G.; Soroush, A.;\nElias, P. A.; Xu, Z.; Ding, Y.; Durrett, G.; Rousseau, J. F.;\nWeng, C.; and Peng, Y. 2023. Evaluating large language mod-\nels on medical evidence summarization. npj Digit. Medicine,\n6.\nThawakar, O. C.; Shaker, A. M.; Mullappilly, S. S.;\nCholakkal, H.; Anwer, R. M.; Khan, S. H.; Laaksonen, J.;\nand Khan, F. 2024. XrayGPT: Chest Radiographs Summa-\nrization using Large Medical Vision-Language Models. In\nACL’24:Proc. of the 23rd Workshop on Biomedical Natural\nLanguage Processing, 440–448. Association for Computa-\ntional Linguistics.\nThirunavukarasu, A. J.; Ting, D. S. J.; Elangovan, K.; Gutier-\nrez, L.; Tan, T. F.; and Ting, D. S. W. 2023. Large language\nmodels in medicine. Nat. Med., 29(8): 1930–1940.\nVan Veen, D.; Van Uden, C.; Blankemeier, L.; Delbrouck,\nJ.-B.; Aali, A.; Bluethgen, C.; Pareek, A.; Polacin, M.; Reis,\nE. P.; Seehofnerová, A.; et al. 2024. Adapted large language\nmodels can outperform medical experts in clinical text sum-\nmarization. Nat. Med., 30(4): 1134–1142.\nVladika, J.; Hacajová, I.; and Matthes, F. 2025. Step-by-Step\nFact Verification System for Medical Claims with Explain-\nable Reasoning. 805–816.\nVladika, J.; Schneider, P.; and Matthes, F. 2024. HealthFC:\nVerifying Health Claims with Evidence-Based Medical Fact-\nChecking. In COLING’24: Proc. of the 2024 Joint Interna-\ntional Conference on Computational Linguistics, Language\nResources and Evaluation, 8095–8107.\n"}, {"page": 11, "text": "Wang, C.; Liu, X.; Yue, Y.; Tang, X.; Zhang, T.; Jiayang, C.;\nYao, Y.; Gao, W.; Hu, X.; Qi, Z.; Wang, Y.; Yang, L.; Wang,\nJ.; Xie, X.; Zhang, Z.; and Zhang, Y. 2023a. Survey on\nFactuality in Large Language Models: Knowledge, Retrieval\nand Domain-Specificity. CoRR, abs/2310.07521.\nWang, H.; Liu, C.; Xi, N.; Qiang, Z.; Zhao, S.; Qin, B.; and\nLiu, T. 2023b. HuaTuo: Tuning LLaMA Model with Chinese\nMedical Knowledge. CoRR, abs/2304.06975.\nWang, H.; Zhao, Y.; Wu, X.; and Zheng, Y. 2024. imapScore:\nMedical Fact Evaluation Made Easy. In ACL’24: Proc. of the\nFindings of the Association for Computational Linguistics,\n10242–10257. Association for Computational Linguistics.\nWang, Y.; Reddy, R. G.; Mujahid, Z. M.; Arora, A.; Ruba-\nshevskii, A.; Geng, J.; Afzal, O. M.; Pan, L.; Borenstein,\nN.; Pillai, A.; Augenstein, I.; Gurevych, I.; and Nakov, P.\n2023c. Factcheck-GPT: End-to-End Fine-Grained Document-\nLevel Fact-Checking and Correction of LLM Output. CoRR,\nabs/2311.09000.\nWen, B.; Yao, J.; Feng, S.; Xu, C.; Tsvetkov, Y.; Howe, B.;\nand Wang, L. L. 2024. Know Your Limits: A Survey of Ab-\nstention in Large Language Models. CoRR, abs/2407.18418.\nWührl, A.; Resendiz, Y. M.; Grimminger, L.; and Klinger,\nR. 2024. What Makes Medical Claims (Un)Verifiable? An-\nalyzing Entity and Relation Properties for Fact Verification.\nIn EACL’24: Proc. of the 18th Conference of the European\nChapter of the Association for Computational Linguistics,\n2046–2058. Association for Computational Linguistics.\nXie, Q.; Han, W.; Zhang, X.; Lai, Y.; Peng, M.; Lopez-Lira,\nA.; and Huang, J. 2023. PIXIU: A Large Language Model, In-\nstruction Data and Evaluation Benchmark for Finance. CoRR,\nabs/2306.05443.\nYim, W.; Fu, Y.; Abacha, A. B.; Snider, N.; Lin, T.; and\nYetisgen, M. 2023. ACI-BENCH: a Novel Ambient Clinical\nIntelligence Dataset for Benchmarking Automatic Visit Note\nGeneration. CoRR, abs/2306.02022.\nYu, H.; Zhou, S.; Huang, M.; Ding, L.; Chen, Y.; Wang,\nY.; Ren, Y.; Cheng, N.; Wang, X.; Liang, J.; et al. 2025.\nPlantScience. ai: An LLM-Powered Virtual Scientist for Plant\nScience. bioRxiv.\nZeng, Z.; Yu, J.; Gao, T.; Meng, Y.; Goyal, T.; and Chen,\nD. 2024. Evaluating Large Language Models at Evaluating\nInstruction Following. In ICLR’24: Proc. of The Twelfth\nInternational Conference on Learning Representations.\nZhang, J.; Sun, K.; Jagadeesh, A.; Falakaflaki, P.; Kayayan,\nE.; Tao, G.; Ghahfarokhi, M. H.; Gupta, D.; Gupta, A.; Gupta,\nV.; and Guo, Y. 2024. The potential and pitfalls of using a\nlarge language model such as ChatGPT, GPT-4, or LLaMA\nas a clinical assistant. J. Am. Medical Informatics Assoc.,\n31(9): 1884–1891.\nZhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi,\nY. 2020. BERTScore: Evaluating Text Generation with BERT.\nIn ICLR’20: Proc. of the International Conference on Learn-\ning Representations.\nZhang, Y.; Li, Y.; Cui, L.; Cai, D.; Liu, L.; Fu, T.; Huang, X.;\nZhao, E.; Zhang, Y.; Chen, Y.; Wang, L.; Luu, A. T.; Bi, W.;\nShi, F.; and Shi, S. 2023. Siren’s Song in the AI Ocean: A\nSurvey on Hallucination in Large Language Models. CoRR,\nabs/2309.01219.\nZhou, S.; Huang, M.; Sun, Y.; and Li, K. 2024. Evolution-\nary Multi-objective Optimization for Contextual Adversarial\nExample Generation. Proc. ACM Softw. Eng., 1(FSE): 2285–\n2308.\nZhou, S.; Li, K.; and Min, G. 2022a. Adversarial example\ngeneration via genetic algorithm: a preliminary result. In\nGECCO ’22: Genetic and Evolutionary Computation Con-\nference, 469–470. ACM.\nZhou, S.; Li, K.; and Min, G. 2022b. Attention-Based Ge-\nnetic Algorithm for Adversarial Attack in Natural Language\nProcessing. In PPSN’22: Parallel Problem Solving from Na-\nture, volume 13398 of Lecture Notes in Computer Science,\n341–355. Springer.\n"}]}