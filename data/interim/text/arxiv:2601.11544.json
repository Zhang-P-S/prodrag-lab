{"doc_id": "arxiv:2601.11544", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.11544.pdf", "meta": {"doc_id": "arxiv:2601.11544", "source": "arxiv", "arxiv_id": "2601.11544", "title": "Medication counseling with large language models: balancing flexibility and rigidity", "authors": ["Joar Sabel", "Mattias Wingren", "Andreas Lundell", "Sören Andersson", "Sara Rosenberg", "Susanne Hägglund", "Linda Estman", "Malin Andtfolk"], "published": "2025-12-02T13:50:03Z", "updated": "2025-12-02T13:50:03Z", "summary": "The introduction of large language models (LLMs) has greatly enhanced the capabilities of software agents. Instead of relying on rule-based interactions, agents can now interact in flexible ways akin to humans. However, this flexibility quickly becomes a problem in fields where errors can be disastrous, such as in a pharmacy context, but the opposite also holds true; a system that is too inflexible will also lead to errors, as it can become too rigid to handle situations that are not accounted for. Work using LLMs in a pharmacy context have adopted a wide scope, accounting for many different medications in brief interactions -- our strategy is the opposite: focus on a more narrow and long task. This not only enables a greater understanding of the task at hand, but also provides insight into what challenges are present in an interaction of longer nature. The main challenge, however, remains the same for a narrow and wide system: it needs to strike a balance between adherence to conversational requirements and flexibility. In an effort to strike such a balance, we present a prototype system meant to provide medication counseling while juggling these two extremes. We also cover our design in constructing such a system, with a focus on methods aiming to fulfill conversation requirements, reduce hallucinations and promote high-quality responses. The methods used have the potential to increase the determinism of the system, while simultaneously not removing the dynamic conversational abilities granted by the usage of LLMs. However, a great deal of work remains ahead, and the development of this kind of system needs to involve continuous testing and a human-in-the-loop. It should also be evaluated outside of commonly used benchmarks for LLMs, as these do not adequately capture the complexities of this kind of conversational system.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.11544v1", "url_pdf": "https://arxiv.org/pdf/2601.11544.pdf", "meta_path": "data/raw/arxiv/meta/2601.11544.json", "sha256": "c1e11faf48684e0affb6c43eea921004d692fd527771abc8f9f53950964695b7", "status": "ok", "fetched_at": "2026-02-18T02:25:40.514105+00:00"}, "pages": [{"page": 1, "text": "Medication counseling with large language\nmodels: balancing flexibility and rigidity\nJoar Sabel1, Mattias Wingren2 ∗, Andreas Lundell3,\nS¨oren Andersson2, Sara Rosenberg 4, Susanne H¨agglund2,\nLinda Estman 5, Malin Andtfolk4\n1 Department of Engineering and Information Technology\n˚Abo Akademi University, Vaasa/Turku, Finland\n2Experience Lab, ˚Abo Akademi University, Vaasa, Finland\n3Department of Engineering and Information Technology\n˚Abo Akademi University, Vaasa, Finland\n4Department of Natural and Health Sciences\n˚Abo Akademi University, Vaasa, Finland\n5Department of Caring and Ethics\nUniversity of Stavanger, Stavanger, Norway\nAbstract\nThe introduction of large language models (LLMs) has greatly en-\nhanced the capabilities of software agents. Instead of relying on rule-based\ninteractions, agents can now interact in flexible ways akin to humans.\nHowever, this flexibility quickly becomes a problem in fields where errors\ncan be disastrous, such as in a pharmacy context, but the opposite also\nholds true; a system that is too inflexible will also lead to errors, as it can\nbecome too rigid to handle situations that are not accounted for. Work\nusing LLMs in a pharmacy context have adopted a wide scope, accounting\nfor many different medications in brief interactions — our strategy is the\nopposite: focus on a more narrow and long task. This not only enables a\ngreater understanding of the task at hand, but also provides insight into\nwhat challenges are present in an interaction of longer nature. The main\nchallenge, however, remains the same for a narrow and wide system: it\nneeds to strike a balance between adherence to conversational require-\nments and flexibility. In an effort to strike such a balance, we present a\nprototype system meant to provide medication counseling while juggling\nthese two extremes. We also cover our design in constructing such a sys-\ntem, with a focus on methods aiming to fulfill conversation requirements,\nreduce hallucinations and promote high-quality responses. The methods\n∗Corresponding mail: mattias.wingren@abo.fi\n1\narXiv:2601.11544v1  [cs.HC]  2 Dec 2025\n"}, {"page": 2, "text": "used have the potential to increase the determinism of the system, while\nsimultaneously not removing the dynamic conversational abilities granted\nby the usage of LLMs.\nHowever, a great deal of work remains ahead,\nand the development of this kind of system needs to involve continuous\ntesting and a human-in-the-loop. It should also be evaluated outside of\ncommonly used benchmarks for LLMs, as these do not adequately capture\nthe complexities of this kind of conversational system.\nKeywords: Large language models, Chatbots, Multi-agent systems, Retrieval\naugmented generation, Human-computer interaction, Healthcare\n1\nIntroduction\nSoftware agents have historically been used in a wide range of contexts, from\ncybersecurity [1] to medicine [2]. Some of these agents have been designed to\ninteract directly with humans, such as the well-known chatbot [3]. Traditionally,\nthese systems have been rooted in a rule-based architecture, making it difficult\nto achieve natural conversations. Often, the human partner would need to adapt\nto the system and not vice versa [4].\nWith the introduction of large language models (LLMs), agents have been\ngiven tools to better handle the complexities of human conversation. Since LLMs\nare probabilistic, they are no longer constrained to predefined rules. However,\nthis flexibility causes issues on the other end of the spectrum, as LLM systems\nhave a tendency to hallucinate — that is, generate incorrect or unrealistic re-\nsponses in relation to their source content [5]. A factor that affects a model’s\ntendency to hallucinate is its context window. The context window of an LLM is\nthe amount of text the model can process at once and can effectively be thought\nof as its working memory. However, this memory comes with serious limitations.\nWhen contextual information increases, relevant facts are more likely to be ig-\nnored, especially information in the middle of the context window [6], similar\nto humans [7]. What is unique to LLMs, however, is how hallucinations work\nin tandem with limitations in their contextual understanding. The greater the\ncontextual information, the more likely the model is to hallucinate, for exam-\nple by being overly confident or simply disregarding relevant facts of a given\nsituation [8].\nThese flaws can have consequences of varying degrees depending on the\ncontext, and one context in which such flaws can be costly, both in terms of\nhealth and financially, is medication counseling [9, 10]. Despite these flaws, the\nflexibility and generative capabilities of LLMs are of interest to the field. Liu\net al. [11], for example, propose a framework for emulating the role of clinical\npharmacists, whereas Osheba et al. [12] use LLMs to verifying prescriptions\nand answer inquiries. These, and other work, [13, 14] handle many different\nmedications during a short interaction; but what also needs to be studied is\nmedication counseling concerning one specific medication during a longer time.\nThe result will vary between these two approaches since a longer task with\nmore conversational requirements gives rise to opportunities for different kind\n2\n"}, {"page": 3, "text": "of errors; but it is also important to examine since this kind of interaction is\nmore akin to medication counseling done by humans. Our project starts from\nthis other end, building a system that aims to provide satisfactory medication\ncounseling in a specific case: counseling regarding emergency contraceptive pills\n(ECPs).\n2\nBackground\nThis work is grounded in a project aiming to contribute interdisciplinary knowl-\nedge on how social robots can support and promote medication safety by pro-\nviding medication counseling at pharmacies [15]. What is of interest is a robot’s\nability to provide counseling regarding ECPs. As mentioned before, we see it\nas a fruitful strategy to begin studying counseling regarding one specific case,\nand this one seems appropriate since it is a medication that does not need a\nprescription, but still requires additional counseling according to Finnish law\n[16]. Since ECPs can be stressful to purchase [17], we also see the interaction\nwith an agent as a potentially less unconformable interaction than one with a\nhuman pharmacist.\nThe procedure of providing counseling regarding ECPs follows a rigid pro-\ncedure [16]. The project has studied this process in detail and divided the task\ninto goals and sub-goals [18]. The goals and sub-goals together number 35, but\nthe following five goals illustrate the procedure quite well: (1) check time since\nintercourse, (2) check for contraindications, (3) present side effects and the un-\nderlying mechanism of the pills, (4) offer choices, and (5) offer advice. Among\nthese, (2) is especially crucial since it ensures the safety and effectiveness of\nthe ECPs; but this goal is also difficult, because the system needs to be able\nto interpret potential contraindications (i.e., allergies, diseases and medication)\nthat can negatively interact with specific ECPs. This would be trivial if cus-\ntomers described contraindications like these in a uniform way, but naturally\nthey might use varying terms for the same condition, making interpretation\nproblematic.\nAt the beginning of the project, the system meant to provide medication\ncounseling was based on a state graph, and therefore its conversational capa-\nbilities were rigid and limited. Such a system is by design constrained to such\na degree that it quickly loses its usefulness. Introducing LLMs into the system\nprovides flexibility that is needed to handle these difficult situations; but as\nmentioned before, this flexibility needs to be tempered.\n2.1\nTools\nA way to introduce rigidity to an LLM system is by using tools [19], external\nfunctions such as application programming interface (API) calls or code execu-\ntion which can be used to improve the quality and flexibility of their responses.\nFor example, in our case all contraindications can be accessed through a tool call\n3\n"}, {"page": 4, "text": "which avoids polluting the LLMs context when the information is not needed;\nin essence it is a form of retrieval augmented generation (RAG).\nTools become even more useful when LLMs are used to reason about which\ntool to call and subsequently interpret the output of said tool and act based on\nit; this type of agent is called a ReAct agent.\n2.2\nReAct Agents\nReAct agents (Reason + Act) were introduced by Yao et al. [20], and unlike\ntraditional task-solving pipelines that separate reasoning and execution, these\nagents can interact with external environments (e.g., APIs) while maintaining a\nstructured thought process. In contrast to, for example, chain-of-thought agents\n[21] which only reason, ReAct agents combine reasoning with action, improving\ntheir decision-making and reducing hallucination.\nHowever, the more responsibilities an agent is given, the higher the risk of\nit performing unwanted behavior. One way of mitigating this is by dividing the\ntask between multiple agents.\n2.3\nMulti-agent Systems\nIn contrast to a single-agent system (SAS) which often struggles with longer\ninputs and more complex tasks due to its monolithic nature [22], a multi-agent\nsystem (MAS) can distribute both context and reasoning between several agents.\nTask decomposition [23] can be leveraged to split the main task between different\nagents as subtasks, thus reducing the “cognitive” load of each agent.\nTask\ndecomposition can be thought of in the same way as humans break down tasks\ninto smaller steps. Decomposing tasks leads to better response quality since each\nagent operates within a narrower problem space. Another advantage of an MAS\nis its modular composition, which makes it easier to change the system without\nimpacting its overall functionality. Finally, utilizing shared memory between all\nagents enables them to cross-check each other’s output, which allows the agents\nto attempt self-correction whenever an error occurs, since they are able to reason\nabout what has happened and what should happen next. Self-correction, in this\ncase, refers to the agent being able to determine that something has gone wrong\nand to attempt to take action in order to remedy the situation.\n3\nImplementation\nThe system is constructed as an MAS consisting of three agents, the (1) Conver-\nsationalist, (2) Medicine interpreter and (3) Symptom assessor, all being ReAct\nagents. The system uses a graph-based implementation, where the agents are\nrepresented as nodes and their interaction paths are represented as edges.\nAs mentioned before, the task of providing counseling regarding ECP was\nstudied in great detail. The analysis used was a hierarchical task analysis [24],\nand since it decomposes a task into smaller parts, it naturally fits our system\n4\n"}, {"page": 5, "text": "Figure 1: Graph representation of the system. Optional paths are dashed lines.\nMandatory paths are solid lines. Agents are ellipse-shaped, and tools are box-\nshaped.\nFigure 2: Structure of the conversation specification steps\nthat uses task decomposition.\nWe have also developed an extension of this\nmethod for more fine-grained analysis. Part of this analysis was used for the\nspecification regarding the required content of the conversation. The specifica-\ntion details what must be discussed with the customer during the conversation\nand what information must be obtained and considered in the process of de-\ntermining viable medication. The specification was converted into the YAML\nformat, as it can easily be interpreted by LLMs due to its rigid token structure\nand hierarchical organization. The structure of the YAML can be seen in Fig-\nure 2. The project also provides a knowledge base detailing the composition\nand contraindications of the ECPs. The specification and the knowledge bases\nprovides criteria to determine if an interaction is a success or a failure; if all\nthe required topics have been discussed and the customer has received the right\nset of ECPs, it has been successful, otherwise not. Ideally, both the knowledge\nbase and the conversation specification would be interchangeable with the spec-\nification and knowledge base of another medication. This would mean that the\nsystem could provide counseling for several types of medication, depending on\nthe specification and knowledge base received.\n5\n"}, {"page": 6, "text": "3.1\nConversationalist\nThe Conversationalist is the agent that converses directly with the customer\nand is therefore the customer-facing part of the system. The agent follows the\nstrict procedure outlined by the YAML document, while answering questions\nand asking follow-up questions if the answers from the customer are unclear.\nFollow-up (context-refining) questions, also known as iterative retrieval [8], help\nLLMs refine their situational understanding. However, given the propensity of\nLLMs to take user input at face value, it is often difficult to make them ask these\nquestions. In our case, the LLM on its own lacks the contextual information\nit needs to determine the relevance of a given allergy or contraindication. A\nway to provide more context is by giving the LLM access to a tool that provides\nresponses it can act upon, following the structure: “if the tool returns X, ask the\ncustomer Y”. The aim is that by doing this, the LLM can better determine if a\nfollow-up question should be asked, an approach that seems promising. Finally,\nfew-shot prompts, or examples of how to perform a task, can be very helpful\nto an LLM [25]. For us, this seems like a fruitful strategy in instructing the\nConversationalist how it should confirm customer’s answers and re-confirm if\nthe customer changes their answer. Other use cases for it include inter-agent\ncommunication and tool usage examples. Providing an agent with examples can\nalso help enforce the habit of asking follow-up questions.\nSome of the steps are more critical than others which the YAML document\nalso specifies by a stating a risk level. One of the most critical steps is check-\ning for possible contraindications, which includes allergies, underlying diseases\nor regular medications.\nThe Conversationalist’s final task is to tell the cus-\ntomer which ECPs, if any, can be consumed. It is vital that this agent follows\nthe conversation specification and that it relays information within the system\nwithout losing parts of or altering the content. We aim to lessen this risk by\nhaving the agent know little about the other agents in the system; it simply\nknows what to provide as input to each agent, the agent’s purpose and what\nto expect as output, which retains more space in the context window for core\nfunctionality (e.g., tasks and responsibilities) and avoids polluting the context\nwith superfluous information.\nThe agent was also given a name, making it easier to write less ambigu-\nous instructions. Instead of instructions in the more ambiguous form of ”You\nshould” it could now be instructed by name, ”[name] should”.\n3.2\nMedicine Interpreter\nThe Medicine interpreter has the task of interpreting the list of the\ncustomer’s\ncontraindications\nand,\nbased\non\nthose,\nretrieving\na\nlist\nof\nECPs\nthat\nare\nsafe\nto\nconsume.\nThe\nagent\nextracts\nindividual\nterms\nfrom\nmentioned\ncontraindications\nand\ncompares\nthem\nwith\nthe\nknowledge\nbase\nusing\nthe\ntools\ncheck pill contraindicating allergies\nand\ncheck pill contraindicating medications and diseases.\nBefore these tools, pure\ninference was used to extract the terms which was used as arguments, but\n6\n"}, {"page": 7, "text": "this often led to incorrect behavior.\nFor example, the system was insuffi-\ncient in handling several similar matches to the input term (e.g., potato or\ncorn starch).\nIn cases like these, the agent would choose the first matched\nterm even if there were several, no term at all, or simply re-use the orig-\ninal input term as an argument in the tool call.\nIn an effort to rem-\nedy this, the tools find most similar word regular medications and diseases and\nfind most similar word allergies were introduced. The tools take an input term\nand return the most similar word or words from their respective tables. If sev-\neral terms are returned, the agent should ask the customer a follow-up question\nfor further specification. Furthermore, the agent should ask follow-up questions\nif a medication contains an ingredient that would affect someone with a hyper-\nsensitivity. For example, if the customer says that they are lactose intolerant\nand some ECPs contain lactose monohydrate, the agent should ask if the cus-\ntomer is severely lactose intolerant, and if so consider those ECPs unsuitable\nfor the customer.\nThe contraindications table of the knowledge base includes disease cate-\ngories, such as “Severe Malabsorption Disorder (e.g., Crohn’s)”, and conditional\ncontraindications such as “Asthma (if glucocorticoids)”. It is unlikely that a\npharmacy customer suffering from asthma would describe it as “Asthma, for\nwhich I take medication containing glucocorticoids”, or describe their Celiac or\nCrohn’s disease as “Severe Malabsorption Disorder”. This is a key aspect of the\nsystem that demands the flexibility and adaptability of LLMs, as pre-defined\nstates are not able to capture this level of complexity. This means that iden-\ntifying contraindications in need of re-labeling and asking follow-up questions\nabout conditional contraindications also fell on the Medicine interpreter. As a\nconsequence, it seemed that the agent became overburdened which resulted in\na drop in response quality. Seeing as the process consists of two major parts:\nclassification and clarification of conditions and identification of viable ECPs, it\nis natural to split these tasks between two agents. In summary, the Symptom\nassessor agent assesses a customer’s symptoms and creates a list of contraindi-\ncations; then the Medicine interpreter uses this list to find ECPs that are safe\nfor the customer to consume.\n3.3\nSymptom Assessor Agent\nThe Symptom assessor identifies, matches and re-labels terms, poses follow-up\nquestions if necessary (including contraindications customers might not think\nof such as breastfeeding) and compiles a list of relevant contraindications. The\nMedicine interpreter agent expects the input list of contraindications to have\nirrelevant terms removed and relevant terms matched to the knowledge base.\nThe tools find most similar word regular medications\nand diseases and find most similar word allergies were given to the Symptom\nassessor, but were not removed from the Medicine interpreter, as a kind of safe-\nguard, in the event that the Symptom assessor makes an error. The tools have\na threshold parameter that dictates how similar a term has to be to constitute a\nmatch. The Symptom assessor calls them with the default value of 0.6, whereas\n7\n"}, {"page": 8, "text": "the Medicine interpreter calls them with a threshold of 0.8 to ensure that the\nterms are correct event if a small error occurs, such as “astma” being input\ninstead of “asthma.” While it would be possible to rely on pure inference to get\nthe correct terms, that presents a point of failure, so it should be avoided.\nThe Symptom assessor seemed to struggle with being tasked to do everything\nfrom determining if a disease needed to be re-labeled to forming the final list of\nrelevant terms. Therefore, it was logical to move the potential term re-labeling\nto its own tool, classify contraindication: a very simple call to an agent in the\nform of “if the input term X is a type of Y, then output Y else None.” This\nmeans, for example, that the input “Celiac disease” would produce the output\n“Severe Malabsorption Disorder.”\n4\nDiscussion\nRule-based systems are often too rigid for contexts where understanding nuance\nis crucial, such as medication counseling. Pure inference, on the other hand, is\ntoo unreliable, meaning that a balance between inferred and rule-based output\nmust be achieved. We present a system designed to achieve such a balance. By\nsubdividing the main task between a network of agents equipped with tools, it\naims to increases the reliability of the system, something of utmost importance\nin a context where errors may have serious consequences.\nFurthermore, the\ndesign of the system is modular in nature, meaning that the system eases the\nadaption to new situations compared to a single-agent solution.\nHowever, some apparent difficulties with designing this kind of system are\nstill present. Hallucinations are particularly problematic because LLMs rarely\nadmit to lacking knowledge or being wrong; instead they often produce confi-\ndently incorrect responses [26]. This kind of behavior was, for example, promi-\nnent when discrepancies between the specification and actual conversation would\narise, potentially resulting in some step being skipped.\nIt is possible some of these difficulties could be mitigated by introducing\nadditional agents, since there currently exists some overlap in the tasks delegated\nbetween them. Doing this, however, would require further examination of how\nto best organize multiple agents in this kind of system. Further examination is\nhere needed.\nBut whatever strategy is implemented to mitigate potential errors, the need\nof testing remains when developing a system of this kind; deployment without\nfurther supervision involves considerable risk, given the complexities involved in\nhuman-agent interaction. Therefore, it would be advisable to implement some\nkind of adaptive monitoring, starting with a human-in-the-loop upon initial\nimplementation and gradually scaling back the monitoring as the system proves\nits capabilities.\n8\n"}, {"page": 9, "text": "4.1\nEvaluation\nEven if evaluating a system of this kind is sorely needed, it is difficult. There\nare metrics, such as whether the customer received the correct recommenda-\ntion, but it also matters how the system came to that conclusion and how the\nsystem interacted with the customer during the process. Based on observations\nmade during the development of the system, some key scenarios were identified\nand act as situations that the system should be able to handle. These scenar-\nios present varying degrees of ambiguity and should all be manageable by the\nsystem. It is crucial that the system can handle more far-fetched descriptions\nof conditions, not just the normal ones. However, this alone is not a sufficient\nform of evaluation.\nTraditional natural language generation evaluation metrics such as BERTscore\n[27] or BARTscore [28] fall short for different reasons. BERTscore favors token-\nlevel similarity and does not consider fluency or coherence [29]. Additionally, it\ndoes not consider factuality, which is problematic in a system where that is the\nchief concern. In contrast, BARTscore suffers from insensitivities and biases\nwhen evaluating challenging texts [30]. Furthermore, static evaluations such\nas these are concerned primarily with evaluating single-turn inputs and out-\nputs [31], as opposed to multi-turn conversations. Due to these shortcomings,\nanother method of evaluation must be used.\nA possible alternative is human interaction evaluations (HIE) [31], a cat-\negory of evaluations that focuses on evaluating the process and outcomes of\nhumans interacting with AI systems. The HIE design framework, as proposed\nby Ibrahim et al. [31] consists of three stages: identifying risk or harm area,\ncharacterizing the use context and choosing the evaluation parameters. Nat-\nurally, this approach is well-suited for examining the system of this work, as\nit better captures the complexities of human-AI interactions that more static\nevaluation methods might overlook.\n4.2\nEthical and Legal Considerations\nAnother thing to consider are the ethical considerations within the context of\nthis work — a central one being the transparency of the system [32].\nThe\ncustomer has the right to know that they are interacting with an AI-based\nsystem [33], but it is not obvious how transparent it must be [34].\nA too\ntransparent system quickly becomes too technical for a lay person, whereas\nan opaque system builds mistrust and anxiety [35, 36]. Here, too, balance is\nkey.\nFor the sake of transparency, our system could could offer a summariza-\ntion transcript of the conversation, regarding how the system determined which\nECPs to recommend. The transcript could also be used as a means to keep\na human-in-the-loop by having domain expertise validate the decision, which\ncould be done in a natural way at the checkout desk. But, as noted, it is not\nclear exactly what information should be listed here.\nGiven the sensitive context in which this system is intended to operate,\n9\n"}, {"page": 10, "text": "accountability, explainability and liability must also be prioritized [32]. In the\nevent that an error occurs, there must be a responsible party, as it is not accept-\nable that the customer is left without recourse for errors. The more autonomous\nAI systems become, the less clear the question of accountability becomes. The\nissue is far from clear and is actively being debated [37, 38, 39, 40].\n5\nConclusion\nWe present a proof of concept that leverages LLM agents to provide medica-\ntion counseling services to customers regarding ECPs, examining narrower and\nlonger interactions than earlier research using LLMs in a pharmacy context.\nOur design aims through proper constraint of the problem space, task decom-\nposition, use of multiple agents, and tools to increase the level of determinism\nwhile maintaining dynamic conversational abilities in the system. We also pro-\npose a method of testing and evaluating the system, based on HIE. There still\nremains much work to be done in designing a system of this kind. Future work\nshould especially focus on finding design patterns that organize multiple agents\nin ways that make them more reliable.\nSystems that leverage LLMs are likely to become more common going for-\nward, which means that it is already vital to design with an emphasis on reli-\nability and quality. We have shown our approach of doing this in the context\nof medication counseling with insights relevant to any LLM-based system that\nneeds to balance between flexibility and rigidity.\nReferences\n[1] Bandar Alluhaybi, Mohamad Shady Alrahhal, Ahmed Alzhrani, and Vijey\nThayananthan. A Survey: Agent-based Software Technology Under the\nEyes of Cyber Security, Security Controls, Attacks and Challenges. Inter-\nnational Journal of Advanced Computer Science and Applications, 10(8),\n2019.\n[2] David Isern, David S´anchez, and Antonio Moreno. Agents applied in health\ncare: A review. International Journal of Medical Informatics, 79(3):145–\n166, March 2010.\n[3] Davide Calvaresi, Stefan Eggenschwiler, Yazan Mualla, Michael Schu-\nmacher, and Jean-Paul Calbimonte.\nExploring agent-based chatbots: a\nsystematic literature review. Journal of Ambient Intelligence and Human-\nized Computing, 14(8):11207–11226, August 2023.\n[4] Hannah R.M. Pelikan and Mathias Broth. Why That Nao? How Humans\nAdapt to a Conventional Humanoid Robot in Taking Turns-at-Talk. In\nProceedings of the 2016 CHI Conference on Human Factors in Comput-\ning Systems, CHI ’16, pages 4921–4932, New York, NY, USA, May 2016.\nAssociation for Computing Machinery.\n10\n"}, {"page": 11, "text": "[5] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Et-\nsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.\nSurvey\nof Hallucination in Natural Language Generation. ACM Comput. Surv.,\n55(12):248:1–248:38, March 2023.\n[6] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilac-\nqua, Fabio Petroni, and Percy Liang. Lost in the Middle: How Language\nModels Use Long Contexts, November 2023. arXiv:2307.03172.\n[7] Fernand Gobet, Peter C. R. Lane, Steve Croker, Peter C.-H. Cheng, Gary\nJones, Iain Oliver, and Julian M. Pine. Chunking mechanisms in human\nlearning. Trends in Cognitive Sciences, 5(6):236–243, June 2001.\n[8] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Hao-\ntian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and\nTing Liu. A Survey on Hallucination in Large Language Models: Princi-\nples, Taxonomy, Challenges, and Open Questions. ACM Trans. Inf. Syst.,\n43(2), January 2025.\n[9] Linda T. Kohn, Janet M. Corrigan, and Molla S. Donaldson. Errors in\nhealth care: A leading cause of death and injury. In Linda T. Kohn and\nJanet M. Corrigan, editors, To err is human: Building a safer health sys-\ntem, pages 26–47. National Academy Press, Washington, DC, USA, 2000.\n[10] Jill Van Den Bos, Karan Rustagi, Travis Gray, Michael Halford, Eva\nZiemkiewicz, and Jonathan Shreve. The $17.1 billion problem: the annual\ncost of measurable medical errors. Health Affairs, 30(4):596–603, 2011.\n[11] Zhengliang Liu, Zihao Wu, Mengxuan Hu, Bokai Zhao, Lin Zhao, Tianyi\nZhang, Haixing Dai, Xianyan Chen, Ye Shen, Sheng Li, Quanzheng Li,\nXiang Li, Brian Murray, Tianming Liu, and Andrea Sikora. PharmacyGPT:\nThe AI Pharmacist, October 2024. arXiv:2307.10432.\n[12] Abdelrahman Osheba, Ahmed Abou-El-Ela, Osama Adel, Youssef Maaod,\nTamer Abuhmed, and Shaker El-Sappagh.\nLeveraging Large Language\nModels for Smart Pharmacy Systems: Enhancing Drug Safety and Oper-\national Efficiency. In 2025 19th International Conference on Ubiquitous\nInformation Management and Communication (IMCOM), pages 1–8, Jan-\nuary 2025.\n[13] Euibeom Shin, Maggie Hartman, and Murali Ramanathan. Performance\nof the ChatGPT large language model for decision support in community\npharmacy.\nBritish Journal of Clinical Pharmacology, 90(12):3320–3333,\n2024.\n[14] Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, and Huy Phan\nThanh. Rx Strategist: Prescription Verification using LLM Agents Sys-\ntem, September 2024. arXiv:2409.03440.\n11\n"}, {"page": 12, "text": "[15] Malin Andtfolk, Susanne H¨aGglund, Sara Rosenberg, Mattias Wingren,\nS¨oRen Andersson, Prashani Jaysingha Arachchige, and Linda Nyholm. The\nfurther implementation of robots in welfare: A co-created application for\nrobot-assisted medication counselling.\nIn Proceedings of the 25th Inter-\nnational Academic Mindtrek Conference, Academic Mindtrek ’22, pages\n368–371, New York, NY, USA, November 2022. Association for Comput-\ning Machinery.\n[16] Fimea – L¨a¨akealan turvallisuus- ja kehitt¨amiskeskus.\nLis¨aneuvontaa\nvaativat\nitsehoitol¨a¨akkeet.\nhttps://fimea.fi/apteekit/\nlisaneuvontaa-vaativat-itsehoitolaakkeet, n.d. Accessed: 2025-09-\n06.\n[17] Ernesto S. Gonz´alez-Mesa, Lorena Bueno-Cobos, Nuria Barroso-Garc´ıa,\nand Jos´e C. Vilches-Jim´enez. Anxiety and attitudes towards sex in women\nrequesting emergency contraception. Journal of Psychosomatic Obstetrics\n& Gynecology, 40(1):75–81, January 2019.\n[18] Mattias Wingren, S¨oren Andersson, Sara Rosenberg, Malin Andtfolk, Su-\nsanne H¨agglund, Prashani Jayasingha Arachchige, and Linda Nyholm.\nUsing Role-Play and Hierarchical Task Analysis for Designing Human-\nRobot Interaction. In Oskar Palinko, Leon Bodenhagen, John-John Cabibi-\nhan, Kerstin Fischer, Selma ˇSabanovi´c, Katie Winkle, Laxmidhar Behera,\nShuzhi Sam Ge, Dimitrios Chrysostomou, Wanyue Jiang, and Hongsheng\nHe, editors, Social Robotics, pages 319–328. Springer Nature Singapore,\nSingapore, 2025.\n[19] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan,\nDongsheng Li, and Deqing Yang.\nEASYTOOL: Enhancing LLM-based\nAgents with Concise Tool Instruction, March 2024. arXiv:2401.06201.\n[20] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik\nNarasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting\nin Language Models, March 2023. arXiv:2210.03629.\n[21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian\nIchter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\nChain-of-Thought\nPrompting Elicits Reasoning in Large Language Models, January 2023.\narXiv:2201.11903.\n[22] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen.\nLong-context LLMs Struggle with Long In-context Learning, June 2024.\narXiv:2404.02060.\n[23] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson,\nPeter Clark, and Ashish Sabharwal. Decomposed Prompting: A Modular\nApproach for Solving Complex Tasks, April 2023. arXiv:2210.02406.\n12\n"}, {"page": 13, "text": "[24] Neville A. Stanton. Hierarchical task analysis: Developments, applications,\nand extensions. Applied Ergonomics, 37(1):55–79, January 2006.\n[25] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nModels are Few-Shot Learners, July 2020. arXiv:2005.14165.\n[26] S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,\nEric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.\nSparks of Artificial General Intelligence: Early experiments with GPT-4,\nApril 2023. arXiv:2303.12712.\n[27] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav\nArtzi.\nBERTScore: Evaluating Text Generation with BERT, February\n2020. arXiv:1904.09675.\n[28] Weizhe Yuan, Graham Neubig, and Pengfei Liu. BARTScore: Evaluating\nGenerated Text as Text Generation, October 2021. arXiv:2106.11520.\n[29] Ben Malin, Tatiana Kalganova, and Nikoloas Boulgouris. A review of faith-\nfulness metrics for hallucination assessment in Large Language Models,\nDecember 2024. arXiv:2501.00269.\n[30] Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan.\nLLM-\nbased NLG Evaluation:\nCurrent Status and Challenges, May 2025.\narXiv:2402.01383.\n[31] Lujain Ibrahim, Saffron Huang, Lama Ahmad, and Markus Anderljung.\nBeyond static AI evaluations: advancing human interaction evaluations for\nLLM harms and risks, May 2024. arXiv:2405.10632.\n[32] James CL Chow and Kay Li. Large language models in medical chatbots:\nOpportunities, challenges, and the need to address ai risks. Information,\n2025.\n[33] Regulatory framework for artificial intelligence. Technical report, European\nCommission, 2025. Accessed: 2025-08-06.\n[34] A. Bhaskara, M. Skinner, and S. Loft. Agent Transparency: A Review\nof Current Theory and Evidence. IEEE Transactions on Human-Machine\nSystems, 50(3):215–224, June 2020.\n13\n"}, {"page": 14, "text": "[35] Thomas P. Quinn, Stephan Jacobs, Manisha Senadeera, Vuong Le, and\nSimon Coghlan. The three ghosts of medical AI: Can the black-box present\ndeliver? Artificial Intelligence in Medicine, 124:102158, February 2022.\n[36] Hanhui Xu and Kyle Michael James Shuttleworth. Medical artificial intel-\nligence and the black box problem: a view based on the ethical principle\nof “do no harm”. Intelligent Medicine, 4(1):52–57, February 2024.\n[37] Herbert Zech. Liability for AI: public policy considerations. ERA Forum,\n22(1):147–158, April 2021.\n[38] Agustina D. Saenz, Zach Harned, Oishi Banerjee, Michael D. Abr`amoff,\nand Pranav Rajpurkar. Autonomous AI systems in the face of liability,\nregulations and costs. npj Digital Medicine, 6(1):185, October 2023.\n[39] Paulo Henrique Padovan, Clarice Marinho Martins, and Chris Reed. Black\nis the new orange: how to determine AI liability. Artificial Intelligence and\nLaw, 31(1):133–167, March 2023.\n[40] Garry A. Gabison and R. Patrick Xian. Inherent and emergent liability\nissues in LLM-based agentic systems: a principal-agent perspective, April\n2025. arXiv:2504.03255.\n6\nOnline Resources\nhttps://github.com/JoarSabel/medical-counselling-llm\n7\nFunding\nThis work was funded by the Finnish Work Environment Fund and the Swedish\nCultural Foundation in Finland.\n14\n"}]}