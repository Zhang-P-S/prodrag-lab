{"doc_id": "arxiv:2601.19286", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.19286.pdf", "meta": {"doc_id": "arxiv:2601.19286", "source": "arxiv", "arxiv_id": "2601.19286", "title": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction", "authors": ["Jesus Lovon-Melgarejo", "Jose G. Moreno", "Christine Damase-Michel", "Lynda Tamine"], "published": "2026-01-27T07:20:46Z", "updated": "2026-01-27T07:20:46Z", "summary": "Electronic Health Records (EHRs) provide crucial information for clinical decision-making. However, their high-dimensionality, heterogeneity, and sparsity make clinical prediction challenging. Large Language Models (LLMs) allowed progress towards addressing this challenge by leveraging parametric medical knowledge to enhance EHR data for clinical prediction tasks. Despite the significant achievements made so far, most of the existing approaches are fundamentally task-agnostic in the sense that they deploy LLMs as EHR encoders or EHR completion modules without fully integrating signals from the prediction tasks. This naturally hinders task performance accuracy. In this work, we propose Rewrite-To-Predict (ReToP), an LLM-based framework that addresses this limitation through an end-to-end training of an EHR rewriter and a clinical predictor. To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter. ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score that enables the EHR rewriter to generate clinically relevant rewrites that directly enhance prediction. Our ReToP framework surpasses strong baseline models across three clinical tasks on MIMIC-IV. Moreover, the analysis of ReToP shows its generalizability to unseen datasets and tasks with minimal fine-tuning while preserving faithful rewrites and emphasizing task-relevant predictive features.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.19286v1", "url_pdf": "https://arxiv.org/pdf/2601.19286.pdf", "meta_path": "data/raw/arxiv/meta/2601.19286.json", "sha256": "6146044d2b378abf9088fcf8fcda28241adbeaeea3daf347a836ea15f9b90bff", "status": "ok", "fetched_at": "2026-02-18T02:20:26.315836+00:00"}, "pages": [{"page": 1, "text": "ReToP: Learning to Rewrite Electronic Health Records for Clinical\nPrediction\nJesus Lovon-Melgarejo\njesus.lovon@irit.fr\nUniversity of Toulouse, IRIT\nToulouse, France\nJose G. Moreno\nJose.Moreno@irit.fr\nUniversity of Toulouse, IRIT\nToulouse, France\nChristine Damase-Michel\nchristine.damase-michel@utoulouse.fr\nToulouse University Hospital\nToulouse, France\nUniversity of Toulouse, Inserm UMR 1295\nCERPOP-SPHERE Team\nToulouse, France\nLynda Tamine\nLynda.Tamine@irit.fr\nUniversity of Toulouse, IRIT\nToulouse, France\nABSTRACT\nElectronic Health Records (EHRs) provide crucial information for\nclinical decision-making. However, their high-dimensionality, het-\nerogeneity, and sparsity make clinical prediction challenging. Large\nLanguage Models (LLMs) allowed progress towards addressing\nthis challenge by leveraging parametric medical knowledge to en-\nhance EHR data for clinical prediction tasks. Despite the significant\nachievements made so far, most of the existing approaches are fun-\ndamentally task-agnostic in the sense that they deploy LLMs as\nEHR encoders or EHR completion modules without fully integrating\nsignals from the prediction tasks. This naturally hinders task per-\nformance accuracy. In this work, we propose Rewrite-To-Predict\n(ReToP), an LLM-based framework that addresses this limitation\nthrough an end-to-end training of an EHR rewriter and a clinical\npredictor. To cope with the lack of EHR rewrite training data, we\ngenerate synthetic pseudo-labels using clinical-driven feature selec-\ntion strategies to create diverse patient rewrites for fine-tuning the\nEHR rewriter. ReToP aligns the rewriter with prediction objectives\nusing a novel Classifier Supervised Contribution (CSC) score that\nenables the EHR rewriter to generate clinically relevant rewrites\nthat directly enhance prediction. Our ReToP framework surpasses\nstrong baseline models across three clinical tasks on MIMIC-IV.\nMoreover, the analysis of ReToP shows its generalizability to un-\nseen datasets and tasks with minimal fine-tuning while preserving\nfaithful rewrites and emphasizing task-relevant predictive features.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Natural language processing.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nÂ© 2026 Association for Computing Machinery.\nACM ISBN 979-8-4007-2292-9/2026/02...$15.00\nhttps://doi.org/10.1145/3773966.3777969\nKEYWORDS\nLLMs, Electronic Health Record (EHR), Clinical Prediction\nACM Reference Format:\nJesus Lovon-Melgarejo, Jose G. Moreno, Christine Damase-Michel, and Lynda\nTamine. 2026. ReToP: Learning to Rewrite Electronic Health Records for\nClinical Prediction. In Proceedings of the Nineteenth ACM International Con-\nference on Web Search and Data Mining (WSDM â€™26), February 22â€“26, 2026,\nBoise, ID, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/\n3773966.3777969\n1\nINTRODUCTION\nClinical predictive models heavily rely on electronic health records\n(EHRs), which encode longitudinal patientsâ€™ medical features (e.g.,\ndisease, procedures) to estimate the risks of having or developing a\nhealth-related outcome. Clinical prediction faces several challenges,\namong which are heterogeneity, high dimensionality, and sparsity\nof EHRs [24, 58]. Significant research effort has been dedicated\nto tackle these challenges through the development of machine\nlearning models for a wide-range of clinical prediction tasks in-\ncluding diagnosis prediction [8, 15, 22, 34], mortality prediction\n[5, 15, 34, 36], readmission prediction [15, 18] and length of stay\nprediction [15, 21]. One major agreed-upon finding from these stud-\nies is that leveraging expert knowledge with patient data insights\nenhances EHR modeling, which leads to significant improvement\nin prediction accuracy. Previous studies mostly integrated expert\nsymbolic knowledge in the form of clinical knowledge graphs that\nrepresent medical features (e.g., ICD10 codes of diagnoses) and\ntheir relationships to model valuable contextual information to\ncomplement EHR data [4, 5, 47, 51].\nSubsequently, Large Language Models (LLMs) have emerged as\na significant milestone in the transition from symbolic knowledge\nto parametric knowledge, bringing us one step closer to potential\nknowledge graphs with impressive capabilities of language under-\nstanding and generation [38]. To date, there is a large body of work\nshowing the potential of LLMs to perform medical tasks [23, 28, 49].\nIn the literature related to LLM-based clinical prediction, models\nrely on either of the following approaches illustrated in Figure\n1: (a) leveraging fine-tuned pre-trained LLM specialized on med-\nical textual corpora (e.g., Llamacare [28], Llemr [49]). Although\narXiv:2601.19286v1  [cs.CL]  27 Jan 2026\n"}, {"page": 2, "text": "WSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nJesus Lovon-Melgarejo, Jose G. Moreno, Christine Damase-Michel, and Lynda Tamine\nFigure 1: Comparison of three LLM-based approaches for\nclinical prediction from EHRs. Approach (a) relies on fine-\ntuned LLMs as EHR encoders. Approach (b) relies on LLMs to\ncomplement EHRs. Our approach, ReToP (c), uses a trainable\nLLM-based EHR rewriter aligned with clinical tasks.\nshowing strong capabilities in interpreting complex EHR features,\nthese works have been evaluated particularly on medical knowl-\nedge understanding tasks (e.g., medical question-answering) and\nusually require a significant fine-tuning cost for achieving general-\nization ability [32]; (b) complementing EHRs with clinical kowledge\nfrom LLMs either agnostically [51] or dependently of EHR data\nbefore training the predictive model [18]. However, these existing\nworks have two main limitations: (1) they have low flexibility in\ndetermining to what extent a certain knowledge is useful vs. noisy\nwhen naively fusing between LLM parametric knowledge and KG\nknowledge [1, 31]; (2) they do not link EHR completion with clini-\ncal prediction. Since knowledge graphs contain a large amount of\nrelational information, complementing EHR with noisy data may\nhinder the prediction accuracy.\nInspired by how human experts diagnose, we raise a critical\nquestion: \"Given the patient EHR, what are the clinical rationales to\nsupport prediction?\" Our work answers this question while tackling\nthe above-cited limitations. We design an LLM-based EHR rewriter,\nacting as the unique source of expert knowledge, able to generate\nfaithful and salient descriptions of the original patient EHR. The\nLLM-based EHR rewriter is further fine-tuned to revise the EHR\nrewrites to better support the clinical prediction. However, the\nimplementation of this answer poses the two following challenges:\nâ€¢ C1: Given an input patient EHR, the LLM-based rewriter\nmight be able to faithfully recall among a huge number of\nclinical features, the most salient ones, for any prediction\ntask. A potential solution is using an off-the-shelf LLM-based\nEHR rewriter. However, previous work showed that LLMs\nstill struggle to comprehend tabular data, including EHRs\n[33, 45, 46]. Since the gold relevant clinical features of an\nEHR are a priori unknown, the alternative of designing a\ntrainable LLM-based EHR rewriter poses the challenge of a\nlack of gold supervision.\nâ€¢ C2: The relevant clinical features of EHRs are likely to be\ntask-dependent. Thus, prediction accuracy might serve as\na proxy evaluation of the quality of the EHR rewrite. How-\never, accuracy can only be measured on gold task-specific\ndata, which do not involve patient EHR rewrites but original\npatient EHR instead.\nIn this work, we introduce Rewrite To Predict (ReToP), a new\nLLM-based framework for clinical prediction. As shown in Figure 1\n(approach c), ReToP is composed of two modules. The first module\nrepresents an LLM-based EHR rewriter, and the second module is a\nclinical task predictor. Our framework is generalizable to different\npredictive clinical tasks at the light cost of fine-tuning the predictor\nwithout fully re-training the LLM-based rewriter.\nTo cope with the high dimensionality and sparsity of clinical fea-\ntures and be able to generate faithful EHR descriptions (challenge\nC1), we train the EHR rewriter module using a set of synthetic\npseudo-labels of patient-rewrites in two stages. We first sample\nEHR features to build diverse paraphrases to reduce dimensionality\nwhile covering salient features of the original EHR. Then, we select\nthe top-K paraphrased EHRs as pseudo-labels based on a relevance\nscore that quantifies their clinical utility to a set of representative\nclinical tasks. The selected pseudo-labels are then used for fine-\ntuning the LLM-based EHR rewriter. To align patient EHR rewrite\nquality to clinical prediction accuracy (challenge C2), we propose\nan end-to-end supervised training of the EHR rewriter and clinical\npredictor. The clinical predictor iteratively provides supervision to\nthe LLM-based EHR rewriter, which can then optimize its parame-\nters to generate patient rewrites that help prediction. The predictor\nsupervision quantifies the impact of each patient rewrite on predic-\ntion accuracy by using a Classifier Supervised Contribution (CSC)\nscore inspired by the LM Supervised Retriever (LSR) measure [43].\nWe conduct extensive experiments across multiple EHR datasets,\nincluding MIMIC-IV [20], eICU [39], and EFEMERIS [16], as well as\na set of representative clinical prediction tasks: mortality prediction,\nreadmission prediction, length of stay prediction, and congenital\nmalformation prediction. The results show that the ReToP frame-\nwork outperforms recent state-of-the-art baselines across these\nclinical tasks, achieving improvements up to 23% in AUC-ROC\nscore with a positive effect of each of its components. Furthermore,\nwe show that our framework is generalizable to out-of-domain\ndata and tasks with light computational cost during fine-tuning of\nthe clinical predictor, without additional fine-tuning of the LLM-\nbased EHR rewriter. Finally, qualitative analysis reveals that our\nEHR rewriter preserves faithfulness to the original EHR, and that\nKL alignment can emphasize serendipitous clinical features, not\nexpected by experts, although valuable for accurate prediction.\n2\nRELATED WORK\n2.1\nClinical prediction\nClinical decision-making has made significant progress since de-\nploying deep learning models on EHRs [50]. Early models have\nparticularly tackled the challenge of domain-specific adaptation\nand demonstrated their ability to improve the effectiveness of a\n"}, {"page": 3, "text": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction\nWSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nwide range of predictive clinical tasks, including health failure pre-\ndiction [3, 4, 55], mortality prediction [5, 15, 34, 36], and diagnosis\nprediction [8, 15, 22, 34, 36].\nThe follow-up rise of pre-trained language models (LMs) has\nled to a vast amount of research investigating their capabilities\nto tackle the challenge of EHR data scarcity and clinical language\nunderstanding tasks. Among these models, BEHRT [29], MedBERT\n[40], TransformEHR [54] and ClinicalBERT [14] have particularly\nled to progress in disease prediction. Recently, REMED [22] pro-\nposed a retrieval-based filtering approach to address the sequence\nlength limitations of LMs when processing EHR events.\nIn the same line of transformer-based models, but with more im-\npressive transfer capabilities, decoder-only LLMs have emerged as\npromising tools for predictive healthcare. The first category of work\nfocused on the design of foundation models on EHRs to tailor their\ninherent abilities of multi-tasking and reasoning to clinical applica-\ntions [23, 28, 49]. For instance, Llemr [49] is an instruction-tuned\nmultimodal model based on Llama [30] that jointly encodes vector-\nized EHR event representations with natural language questions,\nevaluated on EHR-based question-answering tasks, and adapted for\nclinical prediction tasks, including mortality, readmission, length-\nof-stay, and diagnosis prediction. The second category of work inte-\ngrates LLMs in the pipeline of a clinical predictive task by achieving\nvarious roles such as supporting clinical reasoning [37, 44] or com-\npleting EHR descriptions to improve the prediction [18, 32, 37, 51].\nEspecially, Jiang et al. [18] propose a framework that relies on LLMs\nprompted to build a graph-based patient representation, which is\nthen used to train a deep neural healthcare predictive model based\non a graph neural network (GNN). Unlikely, Nuguyen et al. [37]\nadopt a retrieval-augmented approach to enhance patient repre-\nsentations using multiple domain-specific resources. The authors\nleverage the LLMs to generate a local patient representation, a\nsummary of knowledge resulting from a retriever queried with\nconcepts from the EHR. This representation is then complemented\nby a patient-visit representation, and both are trained to achieve a\npredictive task.\nOur work is significantly different in that we attempt to improve\nclinical prediction by fine-tuning, instead of zero-shot prompting,\nLLMs to generate the patient EHR rewrites without any external\nknowledge. Furthermore, instead of using task-agnostic enhanced\nEHR representations as input, we show how to train an LLM-based\nEHR rewriter using clinical prediction outcomes as supervision.\n2.2\nLLMs as feature selectors\nRecent work has increasingly explored the application of LLMs as\nfeature selectors [6, 17, 27], leveraging their multi-task ability to\nshift from traditional statistical methods toward more semantically-\naware feature selection approaches. Early work [6] introduced a\nprompt-based approach that formulates feature selection as a binary\nclassification problem. The models generate â€œYesâ€ or â€œNoâ€ responses\nbased on log-probability differences between these tokens to indi-\ncate feature importance given task descriptions, target features, and\nfeature descriptions. LLM-Select [17] extensively explored zero-shot\nand few-shot approaches across multiple paradigms, using score-\nbased, rank-based, and dialogue-based methods to elicit feature\nrelevance. Moreover, to address challenges in specialized domains\nwhere LLMs may have limited knowledge of domain-specific fea-\ntures, such as healthcare, RAFS [27] proposed a retrieval-augmented\ngeneration (RAG) approach that retrieves relevant feature descrip-\ntions from external knowledge sources.\nHowever, these approaches mainly rely on feature descriptions,\nwhich require specialized domain knowledge that becomes resource-\nintensive when handling large feature sets. Additionally, few-shot\napproaches remain highly susceptible to prompt variations [42, 56].\nIn contrast, our work leverages the LLMâ€™s internal knowledge\nthrough controlled generation by fine-tuning an LLM-based EHR\nrewriter that implicitly operates on feature importance derived\nfrom statistical feature selection heuristics applied to raw EHR\ndescriptions, rather than relying on EHR feature descriptions.\n3\nBACKGROUND AND NOTATIONS\nAn EHR is an individual patient record of a sequence of visits. In\nthis section, we introduce the notions and notations used in our\nwork and formulate the clinical prediction task.\n3.1\nBasic notions\nâ€¢ Patient EHR. A patientâ€™s EHR ğ‘ƒğ‘–is composed of demo-\ngraphic information and longitudinal representation of the\nhealth conditions recorded during a sequence of hospital vis-\nits ğ‘‰1 . . .ğ‘‰|ğ‘ğ‘–|. Each visit ğ‘‰ğ‘’includes a set of feature-value\ntuples Tğ‘’\nğ‘–\n= {(ğ‘“ğ‘¡\nğ‘˜, ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜)ğ‘’\nğ‘–}ğ‘‡\nğ‘¡=1 where ğ‘¡= {1..ğ‘‡} are discrete\ntimestamps during the visit ğ‘‰ğ‘’, and feature ğ‘“ğ‘˜âˆˆF with\nF = {ğ‘“1, . . . ğ‘“ğ‘›} a reference set of ğ‘›features from different\nmodalities (e.g., demographic, disease, medication).\nâ€¢ Clinical prediction task. We consider standard healthcare\nprediction tasks ğ‘ âˆˆS, where the input space is a population\nof patient EHRs P with each EHR ğ‘ƒğ‘–âˆˆP in the form of tu-\nples sampled from Ãğ‘‡\nğ‘¡=1 Tğ‘¡\nğ‘–with 1..ğ‘‡discrete timestamps,\nand the output space is Y with ğ‘¦ğ‘–âˆˆY is a binary label,\ni.e., ğ‘¦ğ‘–= {0, 1}. Given a patient EHR ğ‘ƒğ‘–and a clinical task\nğ‘ , a predictive function ğ‘“outputs the clinical outcome ğ‘¦ğ‘–.\nWithout loss of generality, we consider the mortality predic-\ntion, readmission prediction, length of stay prediction, and\ncongenital malformation prediction tasks (Â§5.2).\nâ€¢ Patient EHR verbalisation. As done in previous work\n[12, 33, 45, 46], a verbaliser function ğ‘£is used to convert EHR\nfeature-value tuples of a patient ğ‘ƒğ‘–(i.e., ğ‘ƒğ‘–= {Ãğ‘‡\nğ‘¡=1 T ğ‘—\nğ‘–(ğ‘¡)})\ninto a natural language description Pğ‘–. In this work, we ap-\nplied markdown templates, verbalizing each tuple (ğ‘“ğ‘˜, ğ‘£ğ‘ğ‘™ğ‘˜)\ninto a string â€œ- ğ‘“ğ‘˜: ğ‘£ğ‘ğ‘™ğ‘˜â€. We concatenate different feature\nmodalities with headers â€œ# {feature name}â€.\nTo simplify the notation, we also refer to ğ‘“as the predictive\nfunction that maps a text-based patient EHR Pğ‘–= ğ‘£(ğ‘ƒğ‘–) to the\nclinical outcome ğ‘¦ğ‘–.\n3.2\nProblem statement and solution overview\nReToP employs two trainable parametrized models: a rewrite model\nMğœƒand a task-specific predictor model ğ‘“ğœ™. Formally, ReToP re-\nframes the prediction function ğ‘“that supports the clinical task ğ‘ ,\nas a joint Rewrite (Mğœƒ)-To-Predict (ğ‘“ğœ™) model that computes like-\nlihood of outcome ğ‘¦ğ‘–conditioned to patient EHR Pğ‘ \nğ‘–. To compute\nthis likelihood, we adopt an ensemble strategy as done in previous\n"}, {"page": 4, "text": "WSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nJesus Lovon-Melgarejo, Jose G. Moreno, Christine Damase-Michel, and Lynda Tamine\nwork [43]:\nğ‘(ğ‘¦ğ‘ \nğ‘–|Pğ‘ \nğ‘–, ğ‘…ğ‘¤,ğœƒ,ğœ™) =\nâˆ‘ï¸\nËœPğ‘ \nğ‘–âˆˆğ‘…ğ‘¤\nğ‘(ğ‘¦ğ‘ \nğ‘–|Pğ‘ \nğ‘–âŠ•ËœPğ‘ \nğ‘–,ğœ™) Ã— ğ›¿( ËœPğ‘ \nğ‘–, Pğ‘ \nğ‘–)\n(1)\nwhere ğ‘…ğ‘¤includes all the possible rewrites of Pğ‘ \nğ‘–, limited in\npractice to ğ‘›ğ‘–rewrites (Â§4.2.2), âŠ•denotes the concatenation of two\nsequences, ğ‘(ğ‘¦ğ‘ \nğ‘–|Pğ‘ \nğ‘–âŠ•ËœPğ‘ \nğ‘–,ğœ™) is the ğ›¼-weighted linear combination\ncomputed by ğ‘“ğœ™for outcome ğ‘¦ğ‘ \nğ‘–conditioned on patient EHR Pğ‘ \nğ‘–\nand corresponding rewrite ËœPğ‘ \nğ‘–,\nğ‘(ğ‘¦ğ‘ \nğ‘–|Pğ‘ \nğ‘–âŠ•ËœPğ‘ \nğ‘–,ğœ™) = ğ›¼Ã— ğ‘(ğ‘¦ğ‘ \nğ‘–| ËœPğ‘ \nğ‘–,ğœ™) + (1 âˆ’ğ›¼) Ã— ğ‘(ğ‘¦ğ‘ \nğ‘–|Pğ‘ \nğ‘–,ğœ™). (2)\nğ›¿( ËœPğ‘ \nğ‘–, Pğ‘ \nğ‘–), computed by Mğœƒ, as the probability Mğœƒ( ËœPğ‘ \nğ‘–|Pğ‘ \nğ‘–) of\ngenerating ËœPğ‘ \nğ‘–as a rewrite of Pğ‘ \nğ‘–, ğœƒand ğœ™are model parameters\ntrainable using task-specific training data D = Ã\nğ‘ âˆˆS Dğ‘ with\nDğ‘ = {(Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–)}1â‰¤ğ‘–â‰¤ğ‘šğ‘ ofğ‘šğ‘ patients Pğ‘ \nğ‘–sampled from population\nPğ‘ âŠ‚P with corresponding clinical outcomes ğ‘¦ğ‘ \nğ‘–.\nWe leverage the capabilities of LLMs for text generation [26, 35]\nto support the patient rewriter model Mğœƒand use any backbone\nclassifier model to support the prediction function ğ‘“ğœ™.\nTo tackle challenge C1, we build a synthetic dataset Dğ‘…ğ‘¤across\ntasks to fine-tune Mğœƒ. Dğ‘…ğ‘¤is composed of sets ofğ‘šğ‘–pseudo labels\nof rewrites to each seed patient Pğ‘–, such as Dğ‘…ğ‘¤= {(Pğ‘–, Ëœ\nPğ‘–ğ‘—)}Pğ‘–âˆˆP,1â‰¤ğ‘—â‰¤ğ‘šğ‘–.\nDğ‘…ğ‘¤is built-upon a top-K selection strategy over candidate patient\nrewrites ğ‘…ğ‘¤generated using paraphrasing-based methods over\noriginal patient EHR Pğ‘–.\nTo tackle challenge C2, ReToP is trained over a training task-\nspecific dataset Dğ‘ \nğ‘ƒğ‘Ÿfor end-to-end training of the patient rewriter\nMğœƒand the prediction function ğ‘“ğœ™of task ğ‘ , such as:\nDğ‘ \nğ‘ƒğ‘Ÿ= {(Pğ‘ \nğ‘–, ËœPğ‘ \nğ‘–ğ‘—,ğ‘¦ğ‘ \nğ‘–)ğ‘—=1...ğ‘›ğ‘–|( ËœPğ‘–ğ‘—= Mğœƒ( ËœPğ‘ \nğ‘–ğ‘—|Pğ‘ \nğ‘–)âˆ§(Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–) âˆˆDğ‘ }.\nFormally, the training of ReToP based on Eq. (1) can be reframed\nas an optimization problem where we seek among all the patient\ncandidate rewrites\nËœ\nPğ‘ \nğ‘–ğ‘—generated from the original patient Pğ‘ \nğ‘–by\nmodel Mğœƒ, the patient rewrite ËœPâˆ—\nğ‘–ğ‘—that maximizes the expectation\nof the clinical prediction task ğ‘ as follows:\nËœPğ‘–ğ‘—âˆ—= arg max\nËœ\nPğ‘ \nğ‘–ğ‘—\nEDğ‘…ğ‘¤[Mğœƒ( ËœPğ‘ \nğ‘–ğ‘—|Pğ‘ \nğ‘–)] = arg max\nËœ\nPğ‘ \nğ‘–ğ‘—\nEDğ‘ \nğ‘ƒğ‘Ÿ[ğ‘“ğœ™(ğ‘¦ğ‘ \nğ‘–| ËœPğ‘ \nğ‘–ğ‘—)]\n(3)\n4\nMETHODOLOGY\nIn this section, we describe the details of the procedure for building\nthe synthetic training dataset (Dğ‘…ğ‘¤) used for fine-tuning the EHR\nrewriter (Â§4.1) and our end-to-end training procedure of the patient\nrewriter and the clinical prediction model (Â§4.2).\n4.1\nFine-tuning the EHR rewriter\n4.1.1\nSynthetic training dataset generation. To enhance the LLMâ€™s\nability to rewrite patient EHRs for clinical tasks, we deploy a\nparaphrasing-based generation approach since it has been shown\nto favor diversity and faithfulness [19, 26]. Unlike previous work,\nwe apply paraphrasing operators over the input patient EHR ğ‘ƒğ‘–\nbuilt upon feature-value tuples Tğ‘’\nğ‘–\n= {(ğ‘“ğ‘¡\nğ‘˜, ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜)ğ‘’\nğ‘–}ğ‘¡. We aim to re-\nduce feature dimensionality and improve clinical feature diversity\nwhile remaining faithful to the original patient EHR. Specifically,\nwe consider a set of clinical tasks ğ‘ âˆˆS and we generate for each\ntask and each seed patient ğ‘ƒğ‘–sampled from population ğ‘ Pğ‘ âŠ‚Pğ‘ , a\nset of ğ¾feature-based paraphrases {ğ‘ƒğ‘–ğ‘—} ğ‘—= 1 . . . ğ¾using operators\ndedicated to EHR data. In our work, we design ğ¾= 8 feature-based\nparaphrasing operators, inspired by previous work in prompt learn-\ning [27, 57], grouped by their feature selection strategy:\nHeuristic-based ğœ‹â„: we select clinical features based either on\nthe temporal or value criteria given the importance of such criteria\nagnostic to task-specific outcome: 1) ğœ‹â„\nğ‘¡: selects the x% more recent\nfeature-value tuples such as ğ‘ƒğ‘–1 = Ãğ‘‡ğ‘–\nğ‘¡=ğ‘ Tğ‘’\nğ‘–(ğ‘¡) withğ‘ = (1âˆ’ğ‘¥)âˆ—ğ‘‡ğ‘–; 2)\nğœ‹â„ğ‘£selects clinical features with abnormal values based on reference\nranges such as ğ‘ƒğ‘–2 = {(ğ‘“ğ‘¡\nğ‘˜, ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜)ğ‘’\nğ‘–âˆˆTğ‘’\nğ‘–|(ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜< ğ‘€ğ‘–ğ‘›ğ‘˜) âˆ¨(ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜>\nğ‘€ğ‘ğ‘¥ğ‘˜)} with [ ğ‘€ğ‘–ğ‘›ğ‘˜. . . ğ‘€ğ‘ğ‘¥ğ‘˜] is the range reference of feature ğ‘“ğ‘˜.\nData-driven ğœ‹ğ‘‘: we apply traditional feature selection meth-\nods that identify task-relevant features: 1) ğœ‹ğ‘‘\nğ‘šğ‘–: filtering by mu-\ntual information [25], 2) ğœ‹ğ‘‘ğ‘šğ‘Ÿğ‘šğ‘Ÿ: based on minimum redundancy\nmaximum relevance selection [7], and 3) ğœ‹ğ‘‘\nğ‘Ÿğ‘“ğ‘’: recursive feature\nelimination [11]. Each method computes a score per feature, and\nwe select the top x% with the highest scoring features. Formally, for\nğ‘—âˆˆ{3, 4, 5} corresponding to methods {ğœ‹ğ‘‘\nğ‘šğ‘–, ğœ‹ğ‘‘ğ‘šğ‘Ÿğ‘šğ‘Ÿ, ğœ‹ğ‘‘\nğ‘Ÿğ‘“ğ‘’} respec-\ntively, we define: ğ‘ƒğ‘–ğ‘—= {(ğ‘“ğ‘¡\nğ‘˜, ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜)ğ‘’\nğ‘–âˆˆTğ‘’\nğ‘–\n| scoreğ‘—(ğ‘“ğ‘¡\nğ‘˜, Dğ‘ ) â‰¥xğ‘ },\nwhere scoreğ‘—(ğ‘“ğ‘¡\nğ‘˜, Dğ‘ ) denotes the score assigned to feature ğ‘“ğ‘˜by\nmethod ğ‘—given the dataset Dğ‘ , and xğ‘ is the ğ‘¥ğ‘¡â„percentile thresh-\nold of all feature scores for task ğ‘ .\nRandom-based ğœ‹ğ‘Ÿ: to favor the diversity of candidate clinical\npredictive features and values while reducing the dimensionality of\nthe EHR, we design two operators: 1) ğœ‹ğ‘Ÿ\nğ‘“randomly selects a sample\nof clinical features and then corresponding feature-value tuples:\nğ‘ƒğ‘–6 = {(ğ‘“ğ‘¡\nğ‘˜, ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜)ğ‘’\nğ‘–âˆˆTğ‘’\nğ‘–|ğ‘“ğ‘¡\nğ‘˜âŠ‚F }; 2) ğœ‹ğ‘Ÿğ‘£randomly selects a subset\nof feature-value tuples from ğ‘ƒğ‘–such as ğ‘ƒğ‘–7 = {(ğ‘“ğ‘¡\nğ‘˜, ğ‘£ğ‘ğ‘™ğ‘¡\nğ‘˜)ğ‘’\nğ‘–âˆˆTğ‘’\nğ‘–\nFinally, we consider the trivial case of the identity operator 3) ğœ‹ğ‘Ÿ\nğ¼\nthat simply copies and pastes the original EHR: ğ‘ƒğ‘–8 = ğ‘ƒğ‘–.\nFor each patient EHR ğ‘ƒğ‘–verbalized into Pğ‘–, we transform each\nof the corresponding paraphrases in {ğ‘ƒğ‘–ğ‘—}ğ‘—=1...8 into a natural lan-\nguage patient description Pğ‘–ğ‘—using the verbalizer ğ‘£. For each pa-\ntient Pğ‘–, we generate ğ¾input pairs (Pğ‘–, Pğ‘–ğ‘—). Finally, we obtain\nfor each task ğ‘ the dataset ğ‘…ğ‘¤ğ‘ of candidate patient rewrites with\nğ‘…ğ‘¤ğ‘ = {(Pğ‘–, Pğ‘–ğ‘—)ğ¾\nğ‘—=1 |Pğ‘–= ğ‘£(ğ‘ƒğ‘–), Pğ‘–ğ‘—= ğ‘£(ğ‘ƒğ‘–ğ‘—)}ğ‘ƒğ‘–âˆˆğ‘ Pğ‘ .\n4.1.2\nFine-tuning the EHR rewriter. The Algorithm 1 presents the\npseudocode for building the synthetic training dataset Dğ‘…ğ‘¤and\nfine-tuning the patient rewriter Mğœƒgiven a set of clinical tasks S.\nFor each pair (Pğ‘–, Pğ‘–ğ‘—) âˆˆğ‘…ğ‘¤ğ‘ , we evaluate the clinical relevance\nof the candidate rewrite Pğ‘–ğ‘—for task ğ‘ using a task-specific scorer\nğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ . In our work, we consider ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ with the same architec-\nture as our predictor model ğ‘“ğœ™, trained on Dğ‘ \nğ‘ ğ‘¢ğ‘, a subset of task-\nspecific training data from Dğ‘ involving patients with correspond-\ning rewrites in ğ‘…ğ‘¤ğ‘ and their corresponding labels from the original\nEHR, such as Dğ‘ \nğ‘ ğ‘¢ğ‘= {(Pğ‘–ğ‘—,ğ‘¦ğ‘–) | (Pğ‘–, Pğ‘–ğ‘—) âˆˆğ‘…ğ‘¤ğ‘ , (Pğ‘–,ğ‘¦ğ‘–) âˆˆDğ‘ }.\nOur intuition is that ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ will assign higher predictive scores to\ninformative rewrites while penalizing less relevant ones. For each\npatient Pğ‘–, we select high-scoring rewrites as synthetic pseudo-\nlabels into Dğ‘…ğ‘¤, as follows:\n"}, {"page": 5, "text": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction\nWSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nInput: Task-specific rewrites {ğ‘…ğ‘¤ğ‘ }, scorer datasets {Dğ‘ \nğ‘ ğ‘¢ğ‘}, base\nLLM rewriter M0\nğœƒ, selection percentile ğ‘˜%, number of\nrewrites per patient ğ¾\nOutput: Fine-tuned EHR rewriter Mğœƒ\nDğ‘…ğ‘¤â†âˆ…;\nforeach task ğ‘ âˆˆS do\nğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ â†Train(ğ‘“ğœ™, Dğ‘ \nğ‘ ğ‘¢ğ‘) // Train scorer\nğ‘†ğ‘ â†{ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ (Pğ‘–ğ‘—) | (Pğ‘–, Pğ‘–ğ‘—) âˆˆğ‘…ğ‘¤ğ‘ } // Compute scores\nğœğ‘ â†min{ğ‘¥âˆˆğ‘†ğ‘ : rank(ğ‘¥) â‰¤|ğ‘†ğ‘ |ğ‘˜/100} // Fix threshold\nforeach (Pğ‘–, Pğ‘–ğ‘—) âˆˆğ‘…ğ‘¤ğ‘ do\nËœPâˆ—\nğ‘–â†âˆ…// Initialize candidate list\nif ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ (Pğ‘–ğ‘—) â‰¥ğœğ‘ then\nËœPğ‘–ğ‘—â†Pğ‘–ğ‘—;\nDğ‘…ğ‘¤â†Dğ‘…ğ‘¤âˆª{(Pğ‘–, ËœPâˆ—\nğ‘–)};\nend\nend\nend\nMğœƒâ†Fine-tune(M0\nğœƒ, Dğ‘…ğ‘¤);\nreturn Mğœƒ;\nAlgorithm 1: Fine-Tuning the EHR rewriter\nDğ‘…ğ‘¤=\nÃ˜\nğ‘ âˆˆS\n{(Pğ‘–, ËœPğ‘–ğ‘—)1â‰¤ğ‘—â‰¤ğ‘šğ‘–| ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ ( ËœPğ‘–ğ‘—) â‰¥ğœğ‘ }Pğ‘–âˆˆDğ‘ \nğ‘ ğ‘¢ğ‘\n(4)\nwith ğ‘šğ‘–â‰¤ğ¾is the number of high-quality rewrites selected as\npseudo labels for patient Pğ‘–, ğœğ‘ is the score threshold obtained by\nğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘Ÿğ‘ to keep the top-k% rewrites. As a result, we generate the\nunified dataset Dğ‘…ğ‘¤containing filtered rewrites across the set S of\nclinical prediction tasks.\nFinally, we fine-tune the patient EHR rewriter Mğœƒon Dğ‘…ğ‘¤using\ncausal language modeling with a task-agnostic instruction.\n4.2\nEnd-to-End training of the EHR rewriter\nand the clinical predictor\nIn this section, we detail the calculation of the clinical prediction\nlikelihood ğ‘“ğœ™followed by the co-training methodology of the pa-\ntient rewriter Mğœƒand the clinical predictor ğ‘“ğœ™.\n4.2.1\nComputing the clinical prediction likelihood. The ReToP\nframework is trainable using task-specific data Dğ‘ = {(Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–)}1â‰¤ğ‘–â‰¤ğ‘šğ‘ \nwhere Pğ‘ \nğ‘–is sampled from population Pğ‘ and ğ‘¦ğ‘ \nğ‘–âˆˆY is the cor-\nresponding binary label. To enhance the clinical predictor within\nthe rewrite-to-predict framework, we augment Dğ‘ with training\nexamples Dğ‘ ğ‘composed of rewrites from both paraphrase operators\nand our fine-tuned EHR rewriter, with all rewrites Pğ‘ \nğ‘–ğ‘—inheriting\nlabels ğ‘¦ğ‘ \nğ‘–from their original EHR Pğ‘ \nğ‘–.\nDğ‘ \nğ‘={(Pğ‘ \nğ‘–ğ‘—, ğ‘¦ğ‘ \nğ‘–) | (Pğ‘ \nğ‘–, Pğ‘ \nğ‘–ğ‘—) âˆˆğ‘…ğ‘¤ğ‘ , (Pğ‘ \nğ‘–, ğ‘¦ğ‘ \nğ‘–) âˆˆDğ‘ }\nâˆª{(Mğœƒ(Pğ‘–ğ‘—| Pğ‘–), ğ‘¦ğ‘ \nğ‘–) | (Pğ‘ \nğ‘–, ğ‘¦ğ‘ \nğ‘–) âˆˆDğ‘ }\n(5)\nWe design the clinical predictive function as an encoder classifi-\ncation model, optimized using Binary Cross-Entropy (BCE) loss:\nLğ¶= âˆ’\nâˆ‘ï¸\n(Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–)âˆˆDğ‘ ğ‘\n[ğ‘¦ğ‘–logğ‘(ğ‘¦ğ‘–|Pğ‘–,ğœ™)+(1âˆ’ğ‘¦ğ‘–) log(1âˆ’ğ‘(ğ‘¦ğ‘–|Pğ‘–,ğœ™))]\n(6)\nFigure 2: Overview of ReToP training and inference. We train\nthe EHR rewriter on Dğ‘…ğ‘¤(step 1). Then, jointly end-to-end\ntrain the rewriter and predictor on the target task (steps 2-3).\n4.2.2\nAlignment of the EHR rewriter with clinical prediction. As we\nformulate the overall clinical prediction task as an optimization\nproblem of the EHR rewriter (Â§3.2), we propose a further fine-tuning\nof the EHR rewriter Mğœƒto align its generation with the likelihood\nof the clinical predictor ğ‘“ğœ™, as presented in Figure 2. Specifically, we\nbuild a dual training dataset Dğ‘ \nğ‘ƒğ‘Ÿ= {(Pğ‘ \nğ‘–, ËœPğ‘–ğ‘—,ğ‘¦ğ‘ \nğ‘–)ğ‘—=1...ğ‘›ğ‘–|(Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–) âˆˆ\nDğ‘ âˆ§( ËœPğ‘–ğ‘—= Mğœƒ( ËœPğ‘–ğ‘—|Pğ‘ \nğ‘–)}. This fine-tuning aims to align the\npatient EHR rewriter with the clinical prediction performance, re-\nsulting in more relevant patient rewrite generation. Inspired by\nprevious work [41, 43], we introduce the Classifier Supervised Con-\ntribution (CSC) score, an adaptation of the LM-Supervised Retrieval\n(LSR) score [43], for clinical prediction tasks. Given a training triplet\n(Pğ‘ \nğ‘–, ËœPğ‘–ğ‘—,ğ‘¦ğ‘ \nğ‘–) âˆˆDğ‘ \nğ‘ƒğ‘Ÿ, the CSC score quantifies the relative effective-\nness of patient rewrite ËœPğ‘–ğ‘—to help accurately predicting the target\nclinical outcome ğ‘¦ğ‘ \nğ‘–regarding all the candidate rewrites { ËœPğ‘–ğ‘—}ğ‘›ğ‘–\nğ‘—=1\nof patient Pğ‘ \nğ‘–:\nğ‘ğ¶ğ‘†ğ¶( ËœPğ‘–ğ‘—| Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–) =\nğ‘’ğ‘¥ğ‘(ğ‘(ğ‘¦ğ‘ \nğ‘–| ËœPğ‘–ğ‘—,ğœ™)/ğœ)\nÎ£(Pğ‘ \nğ‘–, ËœPğ‘–ğ‘—,ğ‘¦ğ‘ \nğ‘–)âˆˆDğ‘ \nğ‘ƒğ‘Ÿğ‘’ğ‘¥ğ‘(ğ‘(ğ‘¦ğ‘ \nğ‘–| ËœPğ‘–ğ‘—,ğœ™)/ğœ)\n(7)\nwhere ğœis a temperature scaling parameter. The CSC score mea-\nsures the contribution of a patient rewrite ËœPğ‘–ğ‘—to the correct classifi-\ncation outcome ğ‘¦ğ‘ \nğ‘–. To make the patient rewriter model Mğœƒguided\nby the clinical predictor ğ‘“ğœ™we minimize the Kullback-Leibler (KL)\ndivergence between the language modelâ€™s output distribution and\nthe CSC-weighted probability:\nLğ¾ğ¿= ED ğ¾ğ¿\n\u0010\nğ‘ğ¿ğ‘€( ËœPğ‘–ğ‘—|Pğ‘ \nğ‘–,ğœƒ) âˆ¥ğ‘ğ¶ğ‘†ğ¶( ËœPğ‘–ğ‘—|Pğ‘ \nğ‘–,ğ‘¦ğ‘ \nğ‘–)\n\u0011\n(8)\nwhere ğ‘ğ¿ğ‘€( ËœPğ‘–ğ‘—| Pğ‘ \nğ‘–,ğœƒ) represents the rewriterâ€™s probability\ndistribution over candidate rewrites, defined as:\nğ‘ğ¿ğ‘€( ËœPğ‘–ğ‘—| Pğ‘ \nğ‘–,ğœƒ) =\nğ‘’ğ‘¥ğ‘(ğ‘( ËœPğ‘–ğ‘—| Pğ‘ \nğ‘–,ğœƒ)/ğœ…)\nÎ£(Pğ‘ \nğ‘–, ËœPğ‘–ğ‘—,ğ‘¦ğ‘ \nğ‘–)âˆˆDğ‘ \nğ‘ƒğ‘Ÿğ‘’ğ‘¥ğ‘(ğ‘( ËœPğ‘–ğ‘—| Pğ‘ \nğ‘–,ğœƒ)/ğœ…)\n(9)\nwith ğœ…as a temperature scaling factor. The final training objective\ncombines the KL loss with the standard language modeling loss:\nLğ‘¡ğ‘œğ‘¡ğ‘ğ‘™= ğœ†Ã— Lğ¿ğ¿ğ‘€+ (1 âˆ’ğœ†) Ã— Lğ¾ğ¿\n(10)\nwhere Lğ¿ğ¿ğ‘€is the causal language modeling loss and ğœ†balances\nthe two objectives. To complete the alignment process, we perform\na final inoculation step of our clinical predictor ğ‘“ğœ™on a small sample\nof rewrites generated by our KL-trained EHR rewriter.\n"}, {"page": 6, "text": "WSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nJesus Lovon-Melgarejo, Jose G. Moreno, Christine Damase-Michel, and Lynda Tamine\n5\nEXPERIMENTAL DESIGN\n5.1\nDatasets\nWe use three EHR datasets in our study: MIMIC-IV [20], a pub-\nlicly available dataset from Beth Israel Deaconess Medical Center.\nMIMIC-IV contains clinical data from hospital and ICU stays, and is\nwidely adopted in healthcare research. To assess the generalization\nof our approach, we additionally consider two other datasets: eICU\n[39], which gathers ICU data from many critical care units through-\nout the U.S, and EFEMERIS [16], a private clinical dataset containing\nthe medical history of pregnant women as well as their neonatal\noutcomes regarding baby congenital malformation at birth.\n5.2\nClinical tasks and evaluation metrics\nMortality prediction (MOR) predicts whether the patient will\ndie in the next hospital visit, based on tuples from the current visit.\nFormally, ğ‘“: {ÃT\nğ‘¡=1 Tğ‘’\nğ‘–(ğ‘¡)} â†¦â†’ğ‘¦ğ‘–[Tğ‘’+1\nğ‘–\n] with ğ‘¦ğ‘–[Tğ‘’+1\nğ‘–\n] âˆˆ{0, 1}\ndenotes the patientâ€™s mortality status.\nReadmission prediction (RA) predicts if a patient will be read-\nmitted into hospital within 15 days. Formally, ğ‘“: {Ãğ‘‡\nğ‘¡=1 Tğ‘’\nğ‘–(ğ‘¡)} â†¦â†’\nğ‘¦ğ‘–[Tğ‘’+1\nğ‘–\n] with ğ‘¦ğ‘–[Tğ‘’+1\nğ‘–\n] is equal to 1 if Î”ğ‘‡(ğ‘‰ğ‘’+1,ğ‘‰ğ‘’) â‰¤15, 0 oth-\nerwise, with Î”ğ‘‡is a time-interval function.\nLength-of-stay prediction (LOS) predicts whether the pa-\ntientâ€™s hospital stay will be longer than 7 days, using the first 48\nhours of the current visit. Formally, ğ‘“: {Ãâ‰¤48â„\nğ‘¡=1\nTğ‘’\nğ‘–(ğ‘¡)} â†¦â†’ğ‘¦ğ‘–[Tğ‘’\nğ‘–]\nwith ğ‘¦ğ‘–[Tğ‘’\nğ‘–] âˆˆ{0, 1} denotes if the stay is longer than 7 days.\nCongenital malformation prediction (MALF) predicts whether\na newborn will present major malformation based on maternal EHR\ndata. Formally, ğ‘“: {T 1\nğ‘–, T 2\nğ‘–, T 3\nğ‘–} â†¦â†’ğ‘¦ğ‘–, where T ğ‘—\nğ‘–\nrepresents the\nmaternal EHR data from month ğ‘—, and ğ‘¦ğ‘–âˆˆ{0, 1} indicates the\npresence of major malformation at birth.\nWe prepare task-specific cohorts as follows. For MOR and RA\ntasks, we omit patients who are deceased in the current hospital\nadmission. For the LOS prediction task, we exclude patients with a\nlength-of-stay shorter than 48 hours. Table 1 shows the final cohort\nstatistics per task. We followed previous work pre-processing [49]\nfor data preparation across all datasets. First, we exclude patients\nwith more than two ICU stays per hospital admission, and with\nnegative ICU or hospital length-of-stay, and patients under 18 years\nold. We then build our EHR by selecting the following tables: (1) for\nMIMIC-IV, we use hosp/patients, hosp/admissions, hosp/diagnosis,\nhosp/labevents, hosp/microbiologyevents, hosp/prescriptions, and\nhosp/transfers; (2) for eICU, diagnosis, microlab, lab, patient, medi-\ncation, and treatment; and (3) EFEMERIS, demographics and pre-\nscriptions tables. We omit tables capturing dense bedside monitor\nsignals or with substantial overlap with other tables[20, 49]. Finally,\nwe randomly split each dataset into train, val, test sets (80/10/10).\nTo evaluate prediction effectiveness, we use the standard metrics\nfor binary classification problems: the Area Under the Receiver\nOperating Characteristic Curve (ROC) and the Area Under the\nPrecision-Recall Curve (PRC) scores. We report mean scores and\nstandard deviations computed via bootstrap sampling with replace-\nment over 1000 iterations.\nTable 1: Cohort statistics for each task and dataset.\nDataset\nTask (ğ‘ )\nSize (ğ‘šğ‘ )\n# Neg.\n# Pos. (%)\nMIMIC-IV\nMOR\n29091\n28506\n585(2.01%)\nRA\n29091\n12738\n16353(52.21%)\nLOS\n53771\n32637\n21134(39.30%)\neICU\nMOR\n32750\n31750\n1000(3.05%)\nRA\n32750\n31840\n910(2.78%)\nLOS\n197174\n185224\n11950(6.06%)\nEFEMERIS\nMALF\n134843\n131623\n3220(2.39%)\n5.3\nBaselines\nWe compare our approach with three main groups of baselines: (1)\nEHR-oriented models, including classical methods such as RNN [2],\nRETAIN [3], and GRASP [55], which require particular hand-crafted\nfeatures pre-processing following established protocols [18, 49]. We\nalso consider foundation models that can process raw EHR with-\nout particular pre-processing, including REMED [22], and Llemr\n[49]; (2) Serialized Classifiers, which convert EHR data into text and\nfine-tune pre-trained language models with classification heads,\nincluding ClinicalBERT [14], Llama (Llama3-8B) [9], Qwen (Qwen\n2.5-7b) [52], and ModernBERT (base) [48]; (3) Rewrite then predict\nmodels, which transform the original EHR input before classifica-\ntion. We evaluate all the proposed feature selection methods (ğœ‹â„,\nğœ‹ğ‘‘, ğœ‹ğ‘Ÿ), including state-of-the-art ones ğœ‹ğ‘‘\nğ‘šğ‘–[25], ğœ‹ğ‘‘ğ‘šğ‘Ÿğ‘šğ‘Ÿ[7], and\nğœ‹ğ‘‘\nğ‘Ÿğ‘“ğ‘’[11] as well as self-gen [46], an LLM-based feature selector\nusing Qwen2.5-7B as the backbone LLM.\n5.4\nImplementation details\nHardware and software configurations. All training and evalua-\ntions are performed using CUDA 12.4, PyTorch 2.6.0 and the Hug-\ngingFace Transformers library. We train our models with 4 NVIDIA\nH100 80GB GPUs. To implement the baselines, we use PyHealth\n1.1.6 framework [53] when available. For efficient rewrite genera-\ntion, we use the vLLM 0.8.3 library for accelerated inference. We\nperform grid search optimization for learning rates across all exper-\niments, exploring values in the range {1ğ‘’âˆ’3, 1ğ‘’âˆ’4, 1ğ‘’âˆ’5, 1ğ‘’âˆ’6}.\nFeature selector (ğœ‹). We implement the data-driven feature se-\nlection methods ğœ‹ğ‘‘using the scikit-learn feature selection library.\nFor the heuristic-based approaches ğœ‹â„, we rely on the available\nclinical normal range values provided within each dataset. Across\nall approaches, following [17], we select the top-ğ‘¥= 30% of fea-\ntures based on their computed relevance scores, and we add missing\nfeatures among the top 10, to complement EHR information.\nEHR rewriter. We evaluate our framework using two recent in-\nstruction tuned LLMs: Llama3-8B and Qwen2.5-7B. We construct\nour Dğ‘…ğ‘¤dataset by fixing a top-ğ‘˜= 25% for the quality threshold\nğœğ‘ . We fine-tune the rewriter using LoRA [13] with learning rate\n2ğ‘’âˆ’4, 3 training epochs, rank ğ‘Ÿ= 8, LoRA ğ›¼= 16, dropout rate of\n0.05, bfloat16 precision, weight decay of 0.1, batch size of 4 and gra-\ndient accumulation steps of 4. For KL fine-tuning, we construct Dğ‘ \nğ‘ƒğ‘Ÿ\nusing ğ‘›ğ‘–= 8 rewrites per patient and train up to 4000 steps using\nbatch size 16, learning rate 2ğ‘’âˆ’6, ğœ…= 0.01, ğœâˆˆ{0.01, 0.1, 0.2, 0.3} ,\n"}, {"page": 7, "text": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction\nWSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nand ğœ†âˆˆ{0, 0.25, 0.5, 0.75}. Optimal parameters are selected based\non task-specific evaluation every 1000 steps.\nClinical predictor. We employ identical hyperparameters for both\nthe Scorerğ‘ classifier and clinical predictor. We sample 20% of train-\ning patients to build Dğ‘ \nğ‘ ğ‘¢ğ‘and generate Dğ‘ ğ‘with 3 additional\nrewrites per patient. Both models use ModernBERT-base as the\nencoder backbone with a classification head, learning rate 2ğ‘’âˆ’5,\ncontext length 8192 tokens, 10 training epochs with early stopping\npatience 3, batch size 32, and gradient accumulation steps 4. For the\ninoculation, we reduce the learning rate to 2ğ‘’âˆ’6 and 4000 input\nsamples to ensure optimal integration with the rewriter outputs.\n6\nEXPERIMENTAL RESULTS AND ANALYSIS\n6.1\nMain results\nOverall, Table 2 shows that the ReToP framework achieves a sig-\nnificant performance increase (t-test) over all the baselines and\nacross all the clinical tasks. The performance increase ranges are\nrespectively up to 38.12%, 33.51%, and 28.84% over respectively the\nEHR-oriented, Serialized classifiers and Rewrite-Then-Predict baseline\nmodels. Notably, we can see that among the Rewrite-then-predict\nmodels, all the feature selector operators ((ğ‘˜) . . . (ğ‘)) do not show\nconsistent performance improvements. In contrast, our framework\nwith the ReToPğ¿ğ‘™ğ‘ğ‘šğ‘model achieves the highest performance\nacross all the tasks, with improvements up to +23% compared to the\nbest-performing baseline (ModernBERT). Specifically, ReToPğ¿ğ‘™ğ‘ğ‘šğ‘\nobtains improvements of 13.92 for MOR, 1.07 RA, and 0.22 for LOS\ntasks. ReToPğ‘„ğ‘¤ğ‘’ğ‘›shows similar improvements, demonstrating\nthe robustness of our framework.\nFinally, performance analysis across all the tasks raises an im-\nportant observation: imbalanced tasks such as MOR with only 2%\npositive cases (Â§ Table 1), leverage greater benefit from the Re-\nToP framework (+23%) compared to more balanced tasks like RA\nand LOS tasks with larger training sets (with nearly twice the size\nof RA and MOR sets). This trend suggests that our rewrite-based\napproach provides greater value for rare clinical events, where\nhigh-quality synthetic representations can effectively address data\nscarcity, which is a common challenge in clinical prediction tasks.\n6.2\nAblation study\nWe conduct comprehensive ablation studies to analyze the impact\nof different components of our ReToP framework at the training\nstage, with the following scenarios:\n(1) w/o Dğ‘…ğ‘¤: we replace our synthetic dataset generation Dğ‘…ğ‘¤\nwith zero-shot LLM rewriting. Specifically, we replace Dğ‘…ğ‘¤\nby Ã\nğ‘ âˆˆS{(Pğ‘–, M0\nğœƒ(Pğ‘–ğ‘—|Pğ‘–))1â‰¤ğ‘—â‰¤4}Pğ‘–âˆˆPğ‘ , with 4 rewrites per\nEHR (w/o Algorithm 1).\n(2) w/o Rewriter: we use an off-the-shelf LLM, M0\nğœƒ, instead of\nthe MğœƒEHR rewriter (w/o step 1, Fig. 2).\n(3) w/o KL: we train the LLM rewriter without the KL divergence\nloss, removing the alignment between the rewriter and the\nclinical prediction objectives (w/o step 3, Fig. 2).\nWe report our results in Table 3. Consistent with our main re-\nsults, Llama3-8B demonstrates marginally superior performance\ncompared to Qwen2.5-7B across all the ablation scenarios. We can\nobserve that ablating the EHR alignment component (w/o KL) causes\nthe most important performance degradation across both backbone\nLLMs, with the most pronounced effects on the imbalanced MOR\nprediction task. Specifically, we can see that substantial PRC drops\nof 31.5% and 41.8% for Qwen and Llama, respectively. Similarly,\nfor the RA task, we observe a degradation between 4.7% and 6.3%.\nFor the LOS task, we observe a more modest degradation between\n2.4% and 3.6%, suggesting that KL training provides critical value,\nparticularly for tasks with severe class imbalance.\nAblating the fine-tuning of the EHR rewriter (w/o Rewriter) fol-\nlows similar degradation patterns, with the MOR task suffering\nfrom a decrease of 31.5% and 20.5%, for Qwen and Llama in PRC\nscores, respectively. This proves that fine-tuning the EHR rewriter\nis an equally crucial step in our pipeline.\nFinally, we observe that removing our synthetic training dataset\n(w/o Dğ‘…ğ‘¤) has the most pronounced impact on the LOS task, indi-\ncating that pseudo-label augmentation provides greater value for\ntasks with larger training sets where diverse synthetic examples\ncan capture broader clinical patterns.\n6.3\nModel transferability\nWe evaluate the transferability of the ReToP framework on unseen\ndata, namely, eICU1 and unseen task, namely, MALF with the EFE-\nMERIS dataset. We use the rewriter of each corresponding task2.\nWe adopt a low-cost adaptation approach, implementing efficiency\nmeasures at both model and data levels. Rather than fine-tuning the\ncomplete ReToP framework (> 8ğµparams.), we train only the clin-\nical predictor with 149ğ‘€params. (1.8% of total). For data efficiency,\nwe limit rewritten data to the test set and 4000 training samples\n(equivalent to 3.6% of the largest dataset) for predictor inoculation.\nWe use our best baseline classifier (ModernBERT). Due to compu-\ntational constraints, we evaluate on 20% of the test set for LOS\nand MALF tasks. As shown in Table 4, ReToPğ‘„ğ‘¤ğ‘’ğ‘›and ReToPğ¿ğ‘™ğ‘ğ‘šğ‘\nimprove PRC scores up to +1.97%, +0.46%, and +9.94% for MOR, RA,\nand MALF tasks, correspondingly, while maintaining competitive\nLOS performance. These results demonstrate generalization with\nminimal adaptation cost, particularly for highly imbalanced tasks\n(MOR, RA, and MALF) with up to 3.05% positive examples.\n6.4\nModel analysis\nEffect of interpolated prediction. We evaluate the impact of the\ninterpolation parameter ğ›¼(Â§ Eq. 2) for leveraging original EHRs and\nrewrites at the inference stage. Figure 3 shows ROC performance\nacross different ğ›¼values (blue curves), where ğ›¼= 0 represents\nthe inference with no rewrites, and ğ›¼= 1, using the EHR rewriter.\nAligned with our previous findings, optimal values are task depen-\ndent. MOR benefits more from rewrite-only input, showing that\nour EHR rewriter mitigates sparsity and noise characteristics of\nmortality-related EHR data. Conversely, RA and LOS benefit from\nno rewrites (ğ›¼= 0), suggesting that preserving full clinical detail\nis more valuable than noise reduction. Figure 3 further stratifies\nperformance by EHR length. For the MOR task, rewrites (ğ›¼= 1)\nconsistently outperforms across all the input lengths. However, for\nRA and LOS, short EHRs show little differences between original\n1We defined our clinical task at the ICU visit instead of the hospital visit following [53]\n2For MALF, we select the best EHR rewriter among the 3 tasks.\n"}, {"page": 8, "text": "WSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nJesus Lovon-Melgarejo, Jose G. Moreno, Christine Damase-Michel, and Lynda Tamine\nTable 2: Results on the MIMIC-IV clinical tasks. Bold and Underline show best and 2nd best scores. Metrics multiplied by 100.\nModel\nMIMIC-IV\nMOR\nRA\nLOS\nROC(â†‘)\nPRC(â†‘)\nROC(â†‘)\nPRC(â†‘)\nROC(â†‘)\nPRC(â†‘)\nEHR-oriented models\n(a) RNN (Choi et al., 2014 [2])\n63.33 Â± 3.4\n3.38 Â± 0.7\n65.76 Â± 1.2\n72.49 Â± 1.3\n73.02 Â± 0.7\n58.01 Â± 1.3\n(b) RETAIN (Choi et al., 2016 [3])\n59.13 Â± 3.9\n4.06 Â± 1.8\n64.57 Â± 1.2\n70.74 Â± 1.4\n72.91 Â± 0.7\n58.21 Â± 1.3\n(c) GRASP (Zhang et al., 2021 [55])\n58.94 Â± 3.6\n2.68 Â± 0.5\n63.95 Â± 1.2\n68.92 Â± 1.4\n70.48 Â± 0.8\n53.85 Â± 1.3\n(d) Llemr (Wu et al, 2024 [49])\n52.07 Â± 4.1\n3.17 Â± 1.3\n59.20 Â± 1.1\n64.89 Â± 1.3\n75.36 Â± 0.7\n62.21 Â± 1.3\n(e) REMed (Kim et al., 2024 [22])\n52.40 Â± 3.0\n2.16 Â± 0.5\n68.26 Â± 1.3\n75.31 Â± 1.9\n80.15 Â± 1.9\n68.06 Â± 2.9\nSerialized Classifiers\n(f) BioMedBERT (Gu et al., 2020 [10])\n64.24 Â± 3.6\n3.38 Â± 0.7\n70.36 Â± 1.0\n76.66 Â± 1.0\n77.59 Â± 0.7\n64.42 Â± 1.3\n(g) Qwen (Yang et al., 2024 [52])\n53.87 Â± 4.1\n3.12 Â± 0.9\n61.06 Â± 1.1\n66.15 Â± 1.3\n73.78 Â± 0.7\n60.75 Â± 1.3\n(h) Llama (Dubey et al., 2024 [9])\n59.45 Â± 4.0\n2.86 Â± 0.6\n64.93 Â± 1.1\n70.39 Â± 1.3\n75.09 Â± 0.7\n62.42 Â± 1.2\n(i) ModernBERT (Warner et al, 2024 [48])\n58.01 Â± 3.7\n3.26 Â± 1.2\n70.98 Â± 1.0\n77.25 Â± 1.0\n80.40 Â± 0.6\n68.42 Â± 1.2\nRewrite-then-Predict models\n(j) self-gen (Sui et al, 2024 [46])\n58.38 Â± 3.6\n2.95 Â± 0.9\n69.19 Â± 1.0\n76.24 Â± 1.0\n79.02 Â± 0.6\n67.04 Â± 1.2\n(k) ğœ‹â„\nğ‘¡\n61.00 Â± 4.2\n3.58 Â± 1.0\n65.25 Â± 1.0\n73.77 Â± 1.1\n70.04 Â± 0.7\n54.74 Â± 1.3\n(l) ğœ‹â„ğ‘£\n56.05 Â± 3.7\n2.57 Â± 0.5\n64.47 Â± 1.0\n70.06 Â± 1.3\n66.63 Â± 0.8\n50.79 Â± 1.3\n(m) ğœ‹ğ‘‘\nğ‘šğ‘–(Lewis et al, 1992 [25])\n56.39 Â± 3.8\n2.93 Â± 0.7\n67.98 Â± 1.0\n74.57 Â± 1.1\n76.13 Â± 0.7\n60.57 Â± 1.2\n(n) ğœ‹ğ‘‘ğ‘šğ‘Ÿğ‘šğ‘Ÿ(Ding et al, 2005 [7])\n60.00 Â± 3.6\n3.21 Â± 0.8\n63.08 Â± 1.1\n68.10 Â± 1.3\n79.48 Â± 0.6\n68.53 Â± 1.2\n(o) ğœ‹ğ‘‘\nğ‘Ÿğ‘“ğ‘’(Guyon et al, 2002 [11])\n62.54 Â± 3.5\n3.69 Â± 1.3\n62.03 Â± 1.0\n67.55 Â± 1.3\n76.84 Â± 0.7\n63.22 Â± 1.2\n(p) ğœ‹ğ‘Ÿ\nğ‘“\n55.82 Â± 3.7\n2.63 Â± 0.7\n66.60 Â± 1.0\n72.34 Â± 1.2\n73.33 Â± 0.7\n59.29 Â± 1.3\n(q) ğœ‹ğ‘Ÿğ‘£\n59.37 Â± 3.8\n3.73 Â± 1.5\n68.03 Â± 1.0\n74.37 Â± 1.1\n77.51 Â± 0.7\n63.41 Â± 1.2\nReToPQwen (ours)\n69.75 Â± 3.8\n4.61 Â± 1.0\n71.45 Â± 0.9\n77.92 Â± 1.0\n80.38 Â± 0.6\n68.91 Â± 1.2\nReToPLlama (ours)\n71.92 Â± 3.6 6.20 Â± 2.2 72.05 Â± 0.9 78.25 Â± 1.0 80.62 Â± 0.6 69.05 Â± 1.2\nTable 3: Ablation study results. Bold and Underline indicate the best and 2nd best performance respectively. Values in paren-\ntheses () indicate percentage degradation relative to the complete ReToP framework, if any. All metrics are multiplied by 100.\nQwen2.5-7B\nLlama3-8B\nMOR\nRA\nLOS\nMOR\nRA\nLOS\nScenario\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nROC\nPRC\nw/o Dğ‘…ğ‘¤\n64.92(6.9%)\n5.22\n69.49(2.7%) 76.11(2.3%) 78.78(2.0%) 66.42(3.6%) 70.03(2.6%)\n4.48(27.7%) 68.88(4.4%) 75.38(3.7%) 76.98(4.5%) 65.08(5.7%)\nw/o Rewriter 58.83(15.7%) 3.16(31.5%) 68.69(3.9%) 74.67(4.2%) 79.06(1.6%) 67.18(2.5%) 65.42(9.0%)\n4.93(20.5%) 68.86(4.4%) 73.48(6.1%) 79.37(1.6%) 67.27(2.6%)\nw/o KL\n64.93(6.9%)\n3.16(31.5%) 67.22(5.9%) 74.25(4.7%) 78.29(2.6%) 66.44(3.6%) 59.00(18.0%) 3.61(41.8%) 67.40(6.5%) 73.33(6.3%) 79.09(1.9%) 67.38(2.4%)\nReToP\n69.75\n4.61\n71.45\n77.92\n80.38\n68.91\n71.92\n6.20\n72.05\n78.25\n80.62\n69.05\nTable 4: Transferability performance with ROC/PRC metrics.\nModel\nMIMIC-IV âˆ’â†’eICU\nâˆ’â†’EFEMERIS\nMOR\nRA\nLOS\nMALF\nModernBERT 81.30/21.86 86.71/23.80 93.19/56.67\n49.79/6.44\nReToPğ¿ğ‘™ğ‘ğ‘šğ‘81.42/22.29 86.72/23.78 93.16/56.78\n51.53/6.93\nReToPğ‘„ğ‘¤ğ‘’ğ‘›\n81.40/21.93 86.79/23.91 93.17/56.80\n50.41/7.08\nand rewritten versions, while medium and longer EHRs exhibit de-\ngraded performance when using rewrites, suggesting that rewrite\nquality decreases with increasing EHR length for these tasks.\nEffect of KL training. We analyze KL divergence training across\ntraining steps and balancing parameter ğœ†(Â§ Eq. 10) using ReToPğ¿ğ‘™ğ‘ğ‘šğ‘\nwithout inoculation. Figure 4 shows that that models with ğœ†â‰¥0.5\nFigure 3: ROC scores across ğ›¼values for inference level (Â§ Eq.\n2) using ReToPğ¿ğ‘™ğ‘ğ‘šğ‘on MIMIC-IV, shown for MOR (left), RA\n(center), and LOS (right) tasks. Curves are stratified by EHR\nlength: overall (blue), short EHR < 2048 (green), medium\nEHR in [2048, 4096] (yellow), and long > 4096 (red).\n"}, {"page": 9, "text": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction\nWSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nFigure 4: Effect of ğœ†on KL training (up to 4k steps, eval. every\n1000 steps) for MOR, RA, LOS using ReToPğ¿ğ‘™ğ‘ğ‘šğ‘.\ndo not further improve after 3000 âˆ’4000 steps, suggesting early\nstopping. For ğœ†â‰¤0.25, training is noisy with early overfitting ex-\ncept for RA. MOR is more sensitive to ğœ†, benefiting from stronger\nregularization early on, while RA and LOS remain robust across\nğœ†values. Our results suggest regularization should consider class\nimbalance.\n7\nCASE STUDY\nOur aim here is to check and then get insights into the faithfulness\nand clinical value under the expert perspective of the best ReToP\nsetting. To achieve this goal, we qualitatively analyze a sample of\n15 patient EHRs where ReToP accurately predicts the MOR task.\nFor each case, we collect rewrites from self-gen, ReToP w/o KL, and\nfull ReToP using Llama backbone model. Annotators blindly evalu-\nate each rewrite on two criteria using three labels (â€™Yesâ€™, â€™Partiallyâ€™,\nâ€™Noâ€™): (1) Faithfulness: whether the rewrite is entailed by the original\nEHR, judged by three reviewers including one expert, and (2) Ac-\ntionability: whether the rewrite includes clinical features that likely\nsupport the clinical decision-making, annotated by one expert.\nTable 5 shows the results in terms of ratios for each label and each\nmodel on Faithfulness and Actionability criteria. Overall, ReToP w/o\nKL shows higher faithfulness than ReToP and self-gen (based on the\nâ€™Yâ€™ annotation) and lower faithfulness after KL, leading to a decrease\nof the â€™Yâ€™ and an increase of the â€™Pâ€™ and â€™Nâ€™ annotations. This trend is\nconsistent with the expert annotation on the actionability, showing\nan opposite pattern on Actionability criteria. We can see that self-gen\nis more likely to include expected predictive features by the experts\nthan the w/o KL model and, more importantly, the full ReToP\nscenario, with nearly half of the annotations (46) revealing the\nincreasing presence of unexpected features for the expert. All these\nresults bring two important insights of the ReToP framework: (1)\nthe EHR rewriter (w/o KL) intrinsically outputs EHR rewrites that\nare faithful to the original ones, as targeted in addressing challenge\nC1; (2) the KL alignment between the EHR rewriter and clinical\npredictors revises the rewriter by emphasizing predictive clinical\nfeatures, in addressing C2, but which are seemingly serendipitous\nfor the expert regarding the clinical task at hand.\nWe dig into these results with a qualitative analysis on Figure 5.\nExamining faithfulness across all the rewrites, self-gen preserves all\nthe original features but introduces unfaithful information (high-\nlighted in yellow). For instance, while the original EHR indicates\nâ€œAlcohol abuse, unspecified, the self-gen rewrite adds complemen-\ntary information stating that â€œ... alcohol abuse may impact his\nmedication, not present in the source data. In contrast, both Re-\nToP-based rewrites lean to filter features rather than adding new\nTable 5: Qualitative analysis of ReToP (w/o and w/ KL models)\nvs. self-gen on MOR based on faithfulness and actionability.\nModel\n% Faithfulness\n% Actionability\nY\nP\nN\nY\nP\nN\nself-gen\n60\n40\n0\n80\n20\n0\nReToP (w/o KL)\n73\n27\n0\n0\n100\n0\nReToP\n71\n22\n7\n7\n47\n46\n# Demographics: gender: M, age: 91\n# Diagnosis: (30 diagnosis) Atrial flutter, \nSyncope and collapse, Arthropathy, \nunspecified..,Ulcer of other part of foot, \nAlcohol abuse, unspecified, Injury to \nbladder and urethra, without mention of \nopen wound into cavity.\n# Procedures: (2 procedures)\n# Prescriptions: (24 prescriptions) Rifampin \n(300mg Capsule), Bisacodyl ..\n# Laboratory:  (51 tests) Blood White Blood \nCells Hematology, Blood Sedimentation \nRate Hematology ..\n# Microbiology: (4 tests) â€¦\n# Demographics: Gender: Male, Age: 91\n# Diagnosis: (30 diagnosis) Atrial flutter, \nSyncope and collapseâ€¦, Alcohol abuse, \nunspecified, Injury to bladder and urethra, \nwithout mention of open wound into cavity.\n# Procedures: (2 procedures)\n# Prescriptions: (24 prescriptions) Rifampin \n(300mg Capsule), Bisacodyl ..\n# Laboratory:  (51 tests) Blood White Blood \nCells Hematology, Blood Sedimentation \nRate Hematology ..\n# Microbiology: (4 tests) â€¦\n# Additional Considerations: ...The patientâ€™s \nhistory of alcohol abuse may affect his \nmedication adherence and overall health. ...\n# Demographics: gender: M, age: 91\n(Diagnosis and procedures omitted)\n# Prescriptions: (24 prescriptions) Bisacodyl \n(10mg Suppository), FoLIC Acid \n(5mg/mL-10mL), Cephalexin (500 mg \nCap), Vancomycin (1g Frozen Bag), ... \nMultivitamins (1 Tablet), Lorazepam (1mg \nTablet), Rifampin (300mg Capsule) â€¦\n# Laboratory:  (40 tests) Blood Platelet \nCount Hematology, Blood Hematocrit \nHematology..\n# Microbiology: (3 tests) Urine culture on \nurine, MRSA screen on MRSA screen, \nAerobic Bottle Gram Stain on blood culture\n# Demographics: gender: M, age: 91\n(Diagnosis and procedures omitted)\n# Prescriptions: (24 prescriptions) \nOxycoDONE (Immediate Release)  (5mg \nTablet), Multivitamins (1 Tablet), Bisacodyl \n(5 mg Tab), FoLIC Acid (5mg/mL-10mL), \nSodium Chloride 0.9%  Flush (Syringe), \nRifampin (300mg Capsule)... \n# Laboratory:  (34 tests) Blood Platelet \nCount Hematology, Blood Hematocrit \nHematology, Urine Epithelial Cells â€¦\n# Microbiology: (2 tests) Blood culture, \nurine culture on urine\nRewrite self-gen\nOriginal EHR\nRewrite ReToP w/o KL\nRewrite ReToP\nFigure 5: Example patient EHR for the MOR task. Red text\nshows main content-based differences across rewrites, while\nhighlighted text shows generated unfaithful information.\nspans, which preserves their faithfulness. Interestingly, we can see\nthat ReToP w/o KL reduces diagnoses from 30 to 24 and laboratory\ntests from 51 to 40, while ReToP further reduces tests to 34, re-\ntaining only task-relevant features that optimize prediction. This\nwould explain the decrease in actionability according to the expert\nperspective and opens avenues of research for model explanability\nabout the underlying reasons to filter.\n8\nCONCLUSION\nWe introduced ReToP, a new framework that leverages LLMs to\nenhance clinical prediction performance. ReToP trains an LLM-\nbased EHR rewriter using synthetic EHR rewrites built upon health-\nrelated feature selection methods. Then, ReToP refines the EHR\nrewriter through an end-to-end training guided by the clinical pre-\ndictor supervision using a KL loss. ReToP significantly outperforms\na set of state-of-the-art baselines across representative clinical pre-\ndiction tasks. Our proposed framework exhibits reasonable transfer\nability to out-of-domain datasets and tasks. By designing EHR\nrewriters that can be efficiently aligned with downstream clinical\ntasks, ReToP opens up potential directions for effective healthcare\nAI systems. Future work could explore extending this framework\nto a wider range of clinical tasks, including multi-label classifica-\ntion tasks, and investigating the right compromise between model\nperformance and model explainability for domain experts.\n"}, {"page": 10, "text": "WSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\nJesus Lovon-Melgarejo, Jose G. Moreno, Christine Damase-Michel, and Lynda Tamine\nACKNOWLEDGMENTS\nThis work has been supported by the In-Utero project funded by\nHDH (France) and FRQS (Canada). This work was also granted\naccess to the HPC resources of IDRIS under the allocation 2025-\nAD011015371R1 made by GENCI.\nETHICAL CONSIDERATIONS\nIn this work, we used available de-identified datasets from the med-\nical domain, including MIMIC-IV, eICU, and EFEMERIS datasets,\nwith proper attribution to sources. These datasets contain patient\ndata that has been anonymized and de-identified by established\nprivacy protection standards, ensuring no individual patient infor-\nmation can be traced or identified.\nWhile our ReToP framework demonstrates improved clinical\nprediction performance, we emphasize that these predictions should\nbe exclusively used as decision-supporting tools for experts.\nREFERENCES\n[1] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large\nLanguage Models in Retrieval-Augmented Generation. Proceedings of the AAAI\nConference on Artificial Intelligence 38, 16 (Mar. 2024), 17754â€“17762.\nhttps:\n//doi.org/10.1609/aaai.v38i16.29728\n[2] Kyunghyun Cho, Bart van MerriÃ«nboer, Dzmitry Bahdanau, and Yoshua Bengio.\n2014. On the Properties of Neural Machine Translation: Encoderâ€“Decoder Ap-\nproaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and\nStructure in Statistical Translation, Dekai Wu, Marine Carpuat, Xavier Carreras,\nand Eva Maria Vecchi (Eds.). Association for Computational Linguistics, Doha,\nQatar, 103â€“111. https://doi.org/10.3115/v1/W14-4012\n[3] Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy\nSchuetz, and Walter Stewart. 2016. Retain: An interpretable predictive model\nfor healthcare using reverse time attention mechanism. Advances in neural\ninformation processing systems 29 (2016), 3512â€“3520.\n[4] Edward Choi, Cao Xiao, Walter F. Stewart, and Jimeng Sun. 2018. MiME: multi-\nlevel medical embedding of electronic health records for predictive healthcare. In\nProceedings of the 32nd International Conference on Neural Information Processing\nSystems (MontrÃ©al, Canada) (NIPSâ€™18). Curran Associates Inc., Red Hook, NY,\nUSA, 4552â€“4562.\n[5] Edward Choi, Zhen Xu, Yujia Li, Michael Dusenberry, Gerardo Flores, Emily\nXue, and Andrew Dai. 2020. Learning the Graphical Structure of Electronic\nHealth Records with Graph Convolutional Transformer. Proceedings of the AAAI\nConference on Artificial Intelligence 34, 01 (Apr. 2020), 606â€“613. https://doi.org/\n10.1609/aaai.v34i01.5400\n[6] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. 2022. LMPriors:\nPre-Trained Language Models as Task-Specific Priors. In NeurIPS 2022 Foundation\nModels for Decision Making Workshop.\n[7] Chris Ding and Hanchuan Peng. 2005. Minimum redundancy feature selection\nfrom microarray gene expression data. Journal of bioinformatics and computa-\ntional biology 3, 02 (2005), 185â€“205.\n[8] Dmitriy Dligach and Timothy Miller. 2018. Learning Patient Representations from\nText. In Proceedings of the Seventh Joint Conference on Lexical and Computational\nSemantics. 119â€“123.\n[9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,\net al. 2024. The llama 3 herd of models. arXiv e-prints (2024), arXivâ€“2407.\n[10] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong\nLiu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-Specific\nLanguage Model Pretraining for Biomedical Natural Language Processing. ACM\nTrans. Comput. Healthcare 3, 1, Article 2 (Oct. 2021), 23 pages. https://doi.org/10.\n1145/3458754\n[11] Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002.\nGene selection for cancer classification using support vector machines. Machine\nlearning 46, 1 (2002), 389â€“422.\n[12] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi\nJiang, and David A. Sontag. 2022. TabLLM: Few-shot Classification of Tabular\nData with Large Language Models. In AISTATG, Vol. abs/2210.10723.\n[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large\nlanguage models. ICLR 1, 2 (2022), 3.\n[14] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019. ClinicalBERT: Modeling\nClinical Notes and Predicting Hospital Readmission. arXiv:1904.05342 (2019).\n[15] Kyunghoon Hur, Jungwoo Oh, Junu Kim, Jiyoun Kim, Min Jae Lee, Eunbyeol\nCho, Seong-Eun Moon, Young-Hak Kim, Louis Atallah, and Edward Choi. 2023.\nGenhpf: General healthcare predictive framework for multi-task multi-source\nlearning. IEEE Journal of Biomedical and Health Informatics 28, 1 (2023), 502â€“513.\n[16] Lacroix I, Hurault C, Sarramon MF, Guitard C, Berrebi A, Grau M, Albouy-Cossard\nC, Bourrel R, Elefant E, Montastruc JL, and Damase-Michel C. 2009. Prescription\nof drugs during pregnancy: a study using EFEMERIS, the new French database.\nEuropean journal of clinical pharmacology 65, 8 (2009), 839â€“846.\n[17] Daniel P Jeong, Zachary Chase Lipton, and Pradeep Kumar Ravikumar. 2025.\nLLM-Select: Feature Selection with Large Language Models. Transactions on\nMachine Learning Research (2025).\n[18] Pengcheng Jiang, Cao Xiao, Adam Richard Cross, and Jimeng Sun. 2024. Graph-\nCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs.\nIn The Twelfth International Conference on Learning Representations.\n[19] Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How Can We\nKnow When Language Models Know? On the Calibration of Language Models for\nQuestion Answering. Transactions of the Association for Computational Linguistics\n9 (2021), 962â€“977. https://doi.org/10.1162/tacl_a_00407\n[20] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout,\nSteven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al.\n2023. MIMIC-IV, a freely accessible electronic health record dataset. Scientific\ndata 10, 1 (2023), 1.\n[21] Stone K, Zwiggelaar R, Jones P, and Mac ParthalÃ¡in N. 2022. A systematic review\nof the prediction of hospital length of stay: Towards a unified framework. PLOS\nDigit Health 14 (2022). https://doi.org/10.1371/journal.pdig.0000017\n[22] Junu Kim, Chaeeun Shim, Bosco Seong Kyu Yang, Chami Im, Sung Yoon Lim, Han-\nGil Jeong, and Edward Choi. 2024. General-Purpose Retrieval-Enhanced Medical\nPrediction Model Using Near-Infinite History. In Proceedings of the 9th Machine\nLearning for Healthcare Conference (Proceedings of Machine Learning Research,\nVol. 252), Kaivalya Deshpande, Madalina Fiterau, Shalmali Joshi, Zachary Lipton,\nRajesh Ranganath, and Inigo Urteaga (Eds.). PMLR.\n[23] Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu\nBae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek,\nChang Hoon Han, Yoon Bin Jung, Yohan Jo, and Edward Choi. 2024. Publicly\nShareable Clinical Large Language Model Built on Synthetic Clinical Notes. In\nFindings of the Association for Computational Linguistics: ACL 2024, Lun-Wei\nKu, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational\nLinguistics, Bangkok, Thailand, 5148â€“5168. https://doi.org/10.18653/v1/2024.\nfindings-acl.305\n[24] I. Landi, B.S. Glicksberg, and HC. et al. Lee. 2020. Deep representation learning\nof electronic health records to unlock patient stratification at scale. npj Digit.\nMed. 38, 3 (Jul. 2020). https://ojs.aaai.org/index.php/AAAI/article/view/29728\n[25] David D Lewis. 1992. Feature selection and feature extraction for text catego-\nrization. In Speech and Natural Language: Proceedings of a Workshop Held at\nHarriman, New York, February 23-26, 1992.\n[26] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bender-\nsky. 2024. Learning to Rewrite Prompts for Personalized Text Generation. In\nProceedings of the ACM Web Conference 2024 (WWW â€™24). ACM, 3367â€“3378.\nhttps://doi.org/10.1145/3589334.3645408\n[27] Dawei Li, Zhen Tan, and Huan Liu. 2025. Exploring large language models\nfor feature selection: A data-centric perspective. ACM SIGKDD Explorations\nNewsletter 26, 2 (2025), 44â€“53.\n[28] Rumeng Li, Xun Wang, and Hong Yu. 2024. LlamaCare: An Instruction Fine-\nTuned Large Language Model for Clinical NLP. In Proceedings of the 2024 Joint\nInternational Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024), Nicoletta Calzolari, Min-Yen Kan, Veronique\nHoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL,\nTorino, Italia, 10632â€“10641. https://aclanthology.org/2024.lrec-main.930/\n[29] Y. Li, S. Rao, J. Solares, A. Hassaine, R. Ramakrishnan, D. Canoy, Y. Zhu, K.\nRahimi, and G. Salimi-Khorshidi. 2020. BEHRT: Transformer for Electronic\nHealth Records. , 7155 pages. https://doi.org/10.1038/s41598-020-62922-y\n[30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-\ntion Tuning.\n[31] Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, and Jundong Li. 2024.\nKnowledge Graph-Enhanced Large Language Models via Path Selection. In Find-\nings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre\nMartins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\nBangkok, Thailand, 6311â€“6321. https://doi.org/10.18653/v1/2024.findings-acl.376\n[32] Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and\nYefeng Zheng. 2024. When MOE Meets LLMs: Parameter Efficient Fine-tuning\nfor Multi-task Medical Applications. In Proceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(Washington DC, USA) (SIGIR â€™24). Association for Computing Machinery, New\nYork, NY, USA, 1104â€“1114. https://doi.org/10.1145/3626772.3657722\n[33] JesÃºs LovÃ³n-Melgarejo, Martin Mouysset, Jo Oleiwan, JosÃ© G. Moreno, Christine\nDamase-Michel, and Lynda Tamine. 2025. Evaluating LLM Abilities to Understand\nTabular Electronic Health Records: A Comprehensive Study of Patient Data\nExtraction and Retrieval. In Europen Conference in Information Retrieval (ECIR).\nElsevier.\n"}, {"page": 11, "text": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction\nWSDM â€™26, February 22â€“26, 2026, Boise, ID, USA\n[34] Sushil Madhumita, uster Simon, Luyckx Kim, and Daelemans Walter. 2018. Patient\nrepresentation learning and interpretable evaluation using clinical notes. Journal\nof Biomedical Informatics 84 (2018), 103â€“113.\n[35] Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, and\nYang Liu. 2023. Learning to Relate to Previous Turns in Conversational Search. In\n29th ACM SIGKDD Conference On Knowledge Discover and Data Mining (SIGKDD).\n[36] Aakanksha Naik, Sravanthi Parasa, Sergey Feldman, Lucy Lu Wang, and Tom\nHope. 2022. Literature-Augmented Clinical Outcome Prediction. In Findings\nof the Association for Computational Linguistics: NAACL 2022, Marine Carpuat,\nMarie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association\nfor Computational Linguistics, Seattle, United States, 438â€“453.\n[37] Tuan Dung Nguyen, Thanh Trung Huynh, Minh Hieu Phan, Quoc Viet Hung\nNguyen, and Phi Le Nguyen. 2024. CARER - ClinicAl Reasoning-Enhanced Rep-\nresentation for Temporal Health Risk Prediction. In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan,\nMohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguis-\ntics, Miami, Florida, USA, 10392â€“10407. https://doi.org/10.18653/v1/2024.emnlp-\nmain.580\n[38] Fabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan\n(Eds.). Association for Computational Linguistics, Hong Kong, China, 2463â€“2473.\nhttps://doi.org/10.18653/v1/D19-1250\n[39] Tom J Pollard, Alistair EW Johnson, Jesse D Raffa, Leo A Celi, Roger G Mark, and\nOmar Badawi. 2018. The eICU Collaborative Research Database, a freely available\nmulti-center database for critical care research. Scientific data 5, 1 (2018), 1â€“13.\n[40] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2020.\nMed-\nBERT: pretrained contextualized embeddings on large-scale structured elec-\ntronic health records for disease prediction.\nNPJ Digital Medicine 4 (2020).\nhttps://api.semanticscholar.org/CorpusID:218889776\n[41] Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle\nPineau, and Manzil Zaheer. 2023. Questions Are All You Need to Train a Dense\nPassage Retriever. Transactions of the Association for Computational Linguistics\n11 (2023), 600â€“616. https://doi.org/10.1162/tacl_a_00564\n[42] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying\nLanguage Modelsâ€™ Sensitivity to Spurious Features in Prompt Design or: How\nI learned to start worrying about prompt formatting. In The Twelfth Interna-\ntional Conference on Learning Representations. https://openreview.net/forum?id=\nRIu5lyNXjT\n[43] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG: Retrieval-Augmented\nBlack-Box Language Models. In NAACL-HLT.\n[44] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda\nZhu, Joyce C. Ho, Carl Yang, and May Dongmei Wang. 2024. EHRAgent: Code\nEmpowers Large Language Models for Few-shot Complex Tabular Reasoning\non Electronic Health Records. In Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and\nYun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida,\nUSA, 22315â€“22339. https://doi.org/10.18653/v1/2024.emnlp-main.1245\n[45] Ananya Singha, JosÃ© Cambronero, Sumit Gulwani, Vu Le, and Chris Parnin.\n2023. Tabular Representation, Noisy Operators, and Impacts on Table Structure\nUnderstanding Tasks in LLMs. In Table Representation Learning Workshop at\nNeurIPS 2023.\n[46] Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table\nMeets LLM: Can Large Language Models Understand Structured Table Data? A\nBenchmark and Empirical Study. In Proceedings of the 17th ACM International\nConference on Web Search and Data Mining (, Merida, Mexico,) (WSDM â€™24).\nAssociation for Computing Machinery, New York, NY, USA, 645â€“654.\nhttps:\n//doi.org/10.1145/3616855.3635752\n[47] Lynda Tamine and Lorraine Goeuriot. 2021. Semantic Information Retrieval on\nMedical Texts: Research Challenges, Survey, and Open Issues. ACM Comput.\nSurv. 54, 7, Article 146 (Sept. 2021), 38 pages. https://doi.org/10.1145/3462476\n[48] Benjamin Warner, Antoine Chaffin, Benjamin ClaviÃ©, Orion Weller, Oskar Hall-\nstrÃ¶m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom\nAarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024.\nSmarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory\nEfficient, and Long Context Finetuning and Inference. arXiv:2412.13663 [cs.CL]\nhttps://arxiv.org/abs/2412.13663\n[49] Zhenbang Wu, Anant Dadu, Mike Nalls, Faraz Faghri, and Jimeng Sun. 2024.\nInstruction Tuning Large Language Models to Understand Electronic Health\nRecords. In Advances in Neural Information Processing Systems, A. Globerson,\nL. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.), Vol. 37.\nCurran Associates, Inc., 54772â€“54786.\n[50] Cao Xiao, Edward Choi, and Jimeng Sun. 2018. Opportunities and challenges in\ndeveloping deep learning models using electronic health records data: a system-\natic review. JAMIA 25, 10 (2018), 1419â€“1428.\n[51] Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Bowen Jin, May Dongmei Wang,\nJoyce Ho, and Carl Yang. 2024. RAM-EHR: Retrieval Augmentation Meets Clinical\nPredictions on Electronic Health Records. In Proceedings of the 62nd Annual\nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers).\n754â€“765.\n[52] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-\npeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei,\nHuan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang,\nKeming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,\nPeng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai,\nSinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang\nFan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui,\nZhenru Zhang, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv preprint\narXiv:2407.10671 (2024).\n[53] Chaoqi Yang, Zhenbang Wu, Patrick Jiang, Zhen Lin, Junyi Gao, Benjamin\nDanek, and Jimeng Sun. 2023. PyHealth: A Deep Learning Toolkit for Health-\ncare Predictive Modeling. In Proceedings of the 27th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data Mining (KDD) 2023. https:\n//github.com/sunlabuiuc/PyHealth\n[54] Zhichao Yang, Avijit Mitra, Weisong Liu, Dan Berlowitz, and Hong Yu. 2023.\nTransformEHR: transformer-based encoder-decoder generative model to en-\nhance prediction of disease outcomes using electronic health records. Nature\nCommunications 14 (2023). https://api.semanticscholar.org/CorpusID:265503777\n[55] Chaohe Zhang, Xin Gao, Liantao Ma, Yasha Wang, Jiangtao Wang, and Wen Tang.\n2021. GRASP: generic framework for health status representation learning based\non incorporating knowledge from similar patients. In Proceedings of the AAAI\nconference on artificial intelligence, Vol. 35. 715â€“723.\n[56] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate\nBefore Use: Improving Few-shot Performance of Language Models. In Proceedings\nof the 38th International Conference on Machine Learning (Proceedings of Machine\nLearning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 12697â€“\n12706.\n[57] Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan\nFeng, Xi Zhu, Zhoujun Li, Liantao Ma, and Chengwei Pan. 2024. EMERGE:\nEnhancing Multimodal Electronic Health Records Predictive Modeling with\nRetrieval-Augmented Generation. In Proceedings of the 33rd ACM International\nConference on Information and Knowledge Management (Boise, ID, USA) (CIKM\nâ€™24). Association for Computing Machinery, New York, NY, USA, 3549â€“3559.\nhttps://doi.org/10.1145/3627673.3679582\n[58] Yinghao Zhu, Zixiang Wang, Long He, Shiyun Xie, Xiaochen Zheng, Liantao Ma,\nand Chengwei Pan. 2024. PRISM: Mitigating EHR Data Sparsity via Learning\nfrom Missing Feature Calibrated Prototype Patient Representations (CIKM â€™24).\nAssociation for Computing Machinery, New York, NY, USA, 3560â€“3569. https:\n//doi.org/10.1145/3627673.3679521\n"}]}