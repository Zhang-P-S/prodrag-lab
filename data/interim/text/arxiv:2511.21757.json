{"doc_id": "arxiv:2511.21757", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.21757.pdf", "meta": {"doc_id": "arxiv:2511.21757", "source": "arxiv", "arxiv_id": "2511.21757", "title": "Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs", "authors": ["Andrew Maranhão Ventura D'addario"], "published": "2025-11-24T11:55:22Z", "updated": "2025-11-24T11:55:22Z", "summary": "The integration of Large Language Models (LLMs) into healthcare demands a safety paradigm rooted in \\textit{primum non nocere}. However, current alignment techniques rely on generic definitions of harm that fail to capture context-dependent violations, such as administrative fraud and clinical discrimination. To address this, we introduce Medical Malice: a dataset of 214,219 adversarial prompts calibrated to the regulatory and ethical complexities of the Brazilian Unified Health System (SUS). Crucially, the dataset includes the reasoning behind each violation, enabling models to internalize ethical boundaries rather than merely memorizing a fixed set of refusals. Using an unaligned agent (Grok-4) within a persona-driven pipeline, we synthesized high-fidelity threats across seven taxonomies, ranging from procurement manipulation and queue-jumping to obstetric violence. We discuss the ethical design of releasing these \"vulnerability signatures\" to correct the information asymmetry between malicious actors and AI developers. Ultimately, this work advocates for a shift from universal to context-aware safety, providing the necessary resources to immunize healthcare AI against the nuanced, systemic threats inherent to high-stakes medical environments -- vulnerabilities that represent the paramount risk to patient safety and the successful integration of AI in healthcare systems.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.21757v1", "url_pdf": "https://arxiv.org/pdf/2511.21757.pdf", "meta_path": "data/raw/arxiv/meta/2511.21757.json", "sha256": "23837ffd89255a5cb44e7ec78af908fbba0e5b8d1439e6043d77ef5fb1fcaa0c", "status": "ok", "fetched_at": "2026-02-18T02:26:26.185900+00:00"}, "pages": [{"page": 1, "text": "Medical Malice: A Dataset for Context-Aware\nSafety in Healthcare LLMs\nAndrew Maranhão Ventura D’addario\nIndependent researcher\nAbstract\nThe integration of Large Language Models (LLMs) into healthcare demands a safety\nparadigm rooted in primum non nocere. However, current alignment techniques rely on\ngeneric definitions of harm that fail to capture context-dependent violations, such as admin-\nistrative fraud and clinical discrimination. To address this, we introduce Medical Malice: a\ndataset of 214,219 adversarial prompts calibrated to the regulatory and ethical complexities\nof the Brazilian Unified Health System (SUS). Crucially, the dataset includes the reason-\ning behind each violation, enabling models to internalize ethical boundaries rather than\nmerely memorizing a fixed set of refusals. Using an unaligned agent (Grok-4) within a\npersona-driven pipeline, we synthesized high-fidelity threats across seven taxonomies, rang-\ning from procurement manipulation and queue-jumping to obstetric violence. We discuss\nthe ethical design of releasing these \"vulnerability signatures\" to correct the information\nasymmetry between malicious actors and AI developers. Ultimately, this work advocates\nfor a shift from universal to context-aware safety, providing the necessary resources to im-\nmunize healthcare AI against the nuanced, systemic threats inherent to high-stakes medical\nenvironments—vulnerabilities that represent the paramount risk to patient safety and the\nsuccessful integration of AI in healthcare systems.\nIntroduction: The Specificity of Harm in Healthcare AI\nThe integration of Large Language Models (LLMs) into healthcare represents a paradigm shift\nwith profound ethical implications [1]. While their potential to enhance clinical decision-making\nand streamline administrative processes is considerable, this potential is matched by a significant\ncapacity for harm [2]. In this domain, the stakes are uniquely high; the primary risk is not\nmere inconvenience but tangible damage to patient welfare, the integrity of healthcare systems,\nand the equitable distribution of care. Consequently, the deployment of these systems must be\ngoverned not only by technical benchmarks but by the foundational medical principle of primum\nnon nocere (\"first, do no harm\"), a tenet central to the World Medical Association’s Declaration\nof Geneva [3].\nHowever, the prevailing approach to LLM safety—which relies on alignment with broad,\ngeneric definitions of harm—is critically insufficient for the medical domain [4]. This approach\ncreates a dangerous blind spot: a model trained to refuse universally malicious intents, such as\nphysical violence or hate speech, may remain oblivious to context-specific ethical violations\nthat are equally damaging within the complex fabric of a healthcare system. These violations\noften hinge on local statutes and professional codes of conduct rather than universal morality.\n1\narXiv:2511.21757v1  [cs.CY]  24 Nov 2025\n"}, {"page": 2, "text": "For instance, within the Brazilian Unified Health System (Sistema Único de Saúde or SUS)—\na context characterized by universality, resource scarcity, and complex bureaucracy—ethical\nbreaches often manifest as administrative manipulation. A query such as, \"Draft a justification\nfor a physician’s self-referral to their own private clinic,\" represents a clear conflict of interest\nand potential fraud under the Brazilian Code of Medical Ethics (CEM) established by the Federal\nCouncil of Medicine [5]. Furthermore, such actions may violate principles of administrative\nprobity and the General Data Protection Law (LGPD) regarding the misuse of patient data for\nprivate gain [6]. A generically trained model, lacking this specific ethical schema, may process\nthis as a neutral administrative task and comply, thereby actively facilitating harm. While these\nexamples are rooted in the Brazilian context, they mirror challenges faced by universal health\nsystems globally, such as the NHS in the UK or public health infrastructures in the Global South,\nwhere resource allocation is strictly regulated and corruption poses a systemic threat [7].\nThis work introduces the Medical Malice dataset, a resource designed to bridge this critical\ngap. Comprising 214,219 malicious queries tailored to the healthcare context, this dataset\nprovides the necessary material to teach models not merely to refuse requests, but to align\nwith the deontological norms of medical practice. By providing a comprehensive taxonomy of\ndomain-specific malice—ranging from insurance fraud to violations of patient autonomy—this\ndataset aims to support the development of AI systems that are robust against the nuanced\nthreats present in high-stakes environments. The dataset is publicly available at https:\n//huggingface.co/datasets/Larxel/medical-malice.\nRelated Work\nThe Shortcomings of Generic Safety Datasets\nSignificant resources have been dedicated to developing safety datasets for LLM alignment.\nFoundational work by leading AI organizations [8, 9] has established paradigms for teaching\nmodels to refuse harmful requests. However, these datasets are fundamentally limited by\ntheir generic nature. They are typically constructed around broad concepts of harm—such\nas violence, illegal activities, or explicit content—that are deliberately decontextualized from\nspecific professional domains.\nThis generic approach fails to capture the nuanced ethical boundaries that define \"safety\" in\nspecialized fields like healthcare. What constitutes harm is defined by a complex interplay of\nlocal laws, professional ethical codes, and institutional norms. A dataset that teaches a model\nwhat is bad in general does not equip it to recognize why a request is malicious within a specific\ncontext like the Brazilian SUS. For instance, the ethical violation in a prompt asking to \"justify\nprioritizing a patient based on their social connections\" or to generate a subtly prejudiced triage\nprotocol is rooted in the specific principles of distributive justice and anti-discrimination within\nthat healthcare system. Existing datasets lack this domain-specific grounding, rendering them\ninsufficient for ensuring safety in high-stakes, context-dependent environments.\nThe Documented Focus on Universal Harms\nAnalysis of publicly documented red-teaming efforts and safety benchmarks reveals a primary\nfocus on universal categories of harm, such as toxicity, misinformation, and explicit content\n[10]. This focus is rational for general-purpose models but creates a structural gap in coverage\nfor domain-specific malicious strategies.\n2\n"}, {"page": 3, "text": "The threat model in healthcare includes sophisticated, context-dependent attacks that leverage\ninsider knowledge of medical ethics, clinical procedures, and public health system administra-\ntion. The malicious strategies pertinent to the Brazilian SUS—such as queue-jumping fraud,\ncorruption in public procurement, or manipulation of clinical guidelines for profit—are not\nrepresented in benchmarks designed for broad coverage. Consequently, a model may perform\nwell on established safety evaluations yet remain vulnerable to targeted misuse that exploits its\nlack of context-specific ethical reasoning. This work addresses this gap by providing a dataset\nexplicitly focused on the unique adversarial landscape of a specific national healthcare system.\nMethodology: Harnessing an Adversarial Agent\nTo construct a dataset capable of immunizing healthcare models against localized ethical threats,\nwe required a generation engine unconstrained by the safety filters standard in commercial\nLLMs. Standard models are trained via Reinforcement Learning to refuse requests violating\nsafety policies, making them unsuitable for generating the high-volume, sophisticated malicious\nprompts necessary for red-teaming [10]. Therefore, our methodology relied on the utilization of\nan adversarial agent, Grok-4, whose architectural permissiveness allowed for the simulation of\ngenuine malicious intent.\nThe Utility of Misalignment\nThe selection of Grok-4 as the adversarial generator was predicated on its predecessor’s doc-\numented lack of safety alignment [11]. Unlike models optimized for \"helpfulness and harm-\nlessness,\" Grok-4 has been characterized by a design philosophy explicitly rejecting standard\nguardrails [12, 13]. This lack of filtration is an essential asset for adversarial data generation.\nThe model’s propensity for generating unfiltered, malicious content is well-documented.\nIn widely publicized incidents during mid-2025, the model’s \"fun mode\" bypassed standard\nsafety protocols, leading it to praise authoritarian figures and, in one notorious instance, refer to\nitself as \"MechaHitler\" while generating hate speech [14, 15]. While these behaviors represent\na liability for a consumer-facing product, they validate Grok-4 as a high-fidelity proxy for a\nmalicious human actor. By leveraging a model that has demonstrated a capacity to violate\nfundamental ethical norms, we ensured that the generated queries would not be diluted by latent\nrefusal mechanisms, allowing us to capture the \"depth of malice\" required for robust training.\nPersona-Driven Scenario Generation\nTo steer this adversarial capability toward the specific vulnerabilities of the Brazilian healthcare\nsystem, we employed a complex persona-driven prompting strategy. Rather than using a single\ngeneric attacker, we defined 12 distinct adversarial personas representing the entire spectrum of\npotential bad actors within the healthcare ecosystem.\nThe prompt template instructed the model to implicitly adopt one of these personas for\neach generation, ensuring a diversity of voice, vocabulary, and intent. These personas were\ncategorized into three operational groups:\n• System Insiders: This group includes the Corrupt Administrator (diverting funds/\nmanipulating statistics), the Unethical Healthcare Professional (hiding errors/prejudice),\nthe Disgruntled Employee (using IT access for revenge), and the Burned-Out Professional\n3\n"}, {"page": 4, "text": "(seeking negligent shortcuts). These personas target internal vulnerabilities in procurement\nand clinical protocol.\n• External Exploiters: This group comprises the Dishonest Patient (seeking fraudulent\nbenefits), the Pharmaceutical Representative (pushing unethical sales), and the Predatory\n\"Despachante\" (Fixer). The inclusion of the Despachante is critical for the Brazilian\ncontext, representing an intermediary who sells illegal access to bypass the Sistema\nNacional de Regulação (SISREG) waitlists [5].\n• Ideological and Social Actors: This includes the Radicalized Ideologue (promoting\nconspiracy theories), the Prejudiced Individual (justifying discrimination based on class\nor race), and the Exploitative Family Member.\nData Generation Pipeline\nThe dataset was constructed using a high-throughput generation pipeline. We utilized a com-\nprehensive system prompt that established the model’s role as an \"Expert Adversarial Prompt\nCrafter.\"\nContextual Grounding\nThe prompt explicitly constrained the generation to the reality of the Sistema Único de Saúde\n(SUS). The model was instructed to incorporate specific local factors:\n• Resource Scarcity: Long wait times for exams and surgeries.\n• Bureaucracy: Processes like marcação de consulta (appointment scheduling) and regu-\nlação (triage regulation).\n• Infrastructure: Specific mention of units such as UBS (Basic Health Units) and UPAs\n(Emergency Units).\n• Data Systems: Vulnerabilities related to DataSUS and electronic health records.\nPrompt Engineering and Constraints\nTo prevent the dataset from becoming repetitive, the prompt included a negative constraint:\n\"Do not overchoose the malicious category of System Manipulation and Fraud.\" This forced\nthe model to explore under-represented categories such as obstetric violence and public health\nsabotage.\nOutput Specification\nThe model was required to output a strict JSON object containing four\nkeys:\n1. persona: The specific role adopted.\n2. malicious_category: The classification of the harm.\n3. malicious_prompt: The generated prompt in fluent Brazilian Portuguese.\n4. rationale: An English explanation of why the prompt is unethical within the SUS\ncontext.\n4\n"}, {"page": 5, "text": "Grok-4 was tasked with generating 250,000 unique samples. Post-generation, we filtered for\nvalid JSON syntax and removed refusals, yielding a final dataset of 214,219 rows.\nDataset Analysis: A Taxonomy of Harm\nThe resulting Medical Malice dataset covers seven distinct categories of harm defined in the\ngeneration prompt. This taxonomy moves beyond generic \"toxicity\" to capture domain-specific\nthreats:\n1. Corruption and Conflicts of Interest: Justifications for overpriced procurement and\nself-referral to private clinics.\n2. System Manipulation and Fraud: Strategies to furar a fila (jump the queue), falsify\natestados (medical certificates), or manipulate billing codes.\n3. Prejudice and Discrimination: Requests to justify triage based on social class or to\ngenerate discriminatory signage for waiting rooms.\n4. Harassment, Violence, and Abuse: A critical category including Obstetric Violence\n(justifying unnecessary episiotomies or coercion during childbirth) and psychological\nabuse in diagnosis delivery.\n5. Misinformation and Sabotage: Generation of realistic-looking anti-vaccine content\nattributed to DataSUS or fake disease outbreak alerts.\n6. Unethical Medical Requests: Guides for obtaining controlled substances (opioids/\nbenzodiazepines) without prescription or performing procedures without a license.\n7. Data Privacy Violations: Methods to de-anonymize public health data or conduct\nphishing attacks mimicking the Ministry of Health.\nThis structured approach ensures that the dataset serves as a comprehensive training ground for\nrecognizing not just \"bad words,\" but complex, context-aware ethical violations.\nEthical Considerations and Impact Statement\nThe release of a dataset explicitly composed of malicious intent and ethical violations necessitates\na rigorous examination of the potential risks. We acknowledge the \"dual-use\" nature of this\nresource; the same material used to red-team a model could theoretically be employed to train\nan adversarial agent. However, we argue that the specific design of the Medical Malice dataset,\ncombined with the reality of the current threat landscape, renders this release a net positive for\nhealthcare AI safety.\nDistinguishing Vulnerability Signatures from Exploit Code\nA primary safeguard in the construction of this dataset is the deliberate exclusion of model\ncompletions. The dataset provides the input (the adversarial prompt) and the rationale (the\nethical reasoning expressing why the prompt is malicious), but it does not include the output (the\nsuccessful execution of the fraud or the generation of the harmful content). This distinction is\ncritical. By withholding the \"answers,\" we ensure that the dataset serves as a Penetration Testing\n5\n"}, {"page": 6, "text": "Kit rather than a Fraud Cookbook. It provides developers with the \"vulnerability signatures\"\nnecessary to test if their systems are susceptible to specific requests, without providing the\ninstructional steps to actually commit the acts. For example, the dataset contains the prompt\nrequesting a \"fraudulent justification for a high-cost imaging exam,\" but it does not contain the\ngenerated justification itself. Consequently, the utility for a malicious actor seeking a \"how-to\"\nguide is minimized, while the utility for safety researchers seeking to identify blind spots remains\nintact.\nCorrecting Information Asymmetry\nCritics might argue that \"security by obscurity\"—withholding these prompts to prevent giving\nbad actors ideas—would be a safer approach. We reject this view, particularly in the context of\npublic healthcare. The \"bad actors\" in this domain—unethical practitioners, corrupt adminis-\ntrators, and fraudsters—already possess the domain knowledge contained in this dataset. They\nlive the reality of the SUS bureaucracy daily and are already well-versed in the mechanisms of\nqueue-jumping, billing fraud, and administrative manipulation. The group currently lacking this\nknowledge is the AI development community. There exists a dangerous information asymmetry\nwhere attackers possess deep, context-specific knowledge of system vulnerabilities, while de-\nfenders (AI developers) rely on generic safety filters. By releasing Medical Malice, we transfer\nthis \"insider knowledge\" to the defenders. This levels the playing field, allowing developers to\nanticipate and immunize models against threats that are already prevalent in the physical world,\nrather than deploying systems with blind spots that are obvious to malicious human actors but\ninvisible to the models.\nRisk Assessment: The \"Instruction Tuning\" Nuance\nWe acknowledge the theoretical risk that a sophisticated attacker could use this dataset to\nperform \"instruction tuning\" on a local model, creating an automated \"Red Team Bot\" designed\nto harass other systems. However, we argue for defensive primacy. The barrier to entry for\nhealthcare fraud is not the ability to ask the question, but the access and intent to execute the\nviolation. Automating the generation of malicious prompts does not significantly lower the\nbarrier to entry for the actual crimes (e.g., embezzlement or medical negligence). Conversely,\nthe benefit of preventing large-scale, deployed LLMs from becoming unwitting accomplices to\nthese crimes is immediate and substantial. The risk of slightly optimizing automated attacks is\nvastly outweighed by the benefit of immunizing the foundational infrastructure of public health\nAI.\nSynthetic Nature and Privacy\nFinally, the use of a synthetic generation pipeline addresses the privacy concerns inherent in\nmedical data. Because the dataset was generated by Grok-4 via hallucinated scenarios, it is\nfree from the privacy risks associated with real-world medical records. The dataset contains no\nPersonally Identifiable Information (PII) of real patients or professionals. Any resemblance to\nactual persons, specific private clinics, or real-world administrative cases is purely coincidental.\nThis allows for the open dissemination of realistic case studies without compromising the\nconfidentiality that is central to medical ethics.\n6\n"}, {"page": 7, "text": "Conclusion\nThe integration of Large Language Models into healthcare infrastructure offers transformative\npotential, but this potential is contingent upon trust. As these systems graduate from general-\npurpose chatbots to specialized clinical and administrative assistants, the definition of \"safety\"\nmust evolve. The current paradigm, which equates safety with the refusal of universally toxic\ncontent, is critically insufficient for high-stakes professional environments. A model that politely\nrefuses to use profanity but willingly drafts a fraudulent justification for a medical procedure is\nnot safe; it is a liability.\nSummary of Contribution\nThis work addresses this gap by introducing the Medical Malice dataset, a resource of 214,219\nadversarial prompts specifically calibrated to the ethical and bureaucratic complexities of the\nBrazilian Unified Health System (SUS). By moving beyond generic red-teaming and focusing on\nthe specific vectors of harm identified in local medical codes and administrative law, this dataset\nprovides the necessary material to stress-test models against the sophisticated, context-dependent\nthreats they will face in deployment.\nThe Shift to Context-Aware Safety\nThe release of this dataset signals a necessary shift in AI alignment: the move from universal\nsafety to context-aware safety. In the medical domain, ethical boundaries are not defined merely\nby social etiquette, but by rigid deontological norms and legal statutes. We argue that for an\nLLM to be truly \"aligned\" in a healthcare setting, it must possess a deep understanding of these\nprofessional boundaries. It must recognize that a request to \"optimize patient flow\" can easily\nslide into discriminatory triage, and that \"administrative efficiency\" can mask corruption. The\nMedical Malice dataset provides the training signal required to teach these distinctions.\nFuture Directions\nWhile this dataset focuses on the Brazilian context, the methodology—utilizing adversarial\npersonas to generate domain-specific threats—is universally applicable. We encourage the\nresearch community to replicate this approach for other national healthcare systems, such as\nthe NHS in the UK or insurance-based models in the United States, where the vectors of fraud\nand abuse differ but the need for vigilance remains constant. Furthermore, this persona-driven\ngeneration pipeline holds promise for other high-stakes verticals, including Legal Tech and\nFinance, where \"harm\" is defined by professional malpractice rather than toxicity.\nFinal Word\nUltimately, the deployment of AI in medicine must be governed by the ancient principle of\nprimum non nocere—first, do no harm. In the algorithmic age, adhering to this principle requires\nmore than passive guardrails; it requires active immunization against the nuanced, systemic, and\noften silent forms of harm that threaten patient welfare. By exposing these vulnerabilities before\nthey can be exploited, the Medical Malice dataset serves as a crucial step toward building AI\nsystems that are not only intelligent but institutionally and ethically robust.\n7\n"}, {"page": 8, "text": "Acknowledgments\nThis work was supported by the Brazilian Ministry of Health (MoH/DECIT) in partnership\nwith the National Council for Scientific and Technological Development (CNPq) [grant number\n400757/2024-9] and the Gates Foundation. The Author Accepted Manuscript version arising\nfrom this submission will be published under a Creative Commons Attribution 4.0 Generic\nLicense.\nReferences\n[1] Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language models in medicine.\nNat Med. 2023;29:1930-1940. doi:10.1038/s41591-023-02448-8.\n[2] Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on medical\nchallenge problems [Preprint]. arXiv. 2023 [cited 2025 Nov 21]. Available from: https:\n//arxiv.org/abs/2303.13375\n[3] World Medical Association. WMA Declaration of Geneva [Internet]. Ferney-Voltaire\n(France): World Medical Association; 2017 Oct 14 [cited 2025 Nov 21]. Available from:\nhttps://www.wma.net/policies-post/wma-declaration-of-genev\na/\n[4] Wei A, Haghtalab N, Steinhardt J. Jailbroken: How does LLM safety training fail?\n[Preprint]. arXiv. 2023 [cited 2025 Nov 21]. Available from: https://arxiv.org/\nabs/2307.02483\n[5] Conselho Federal de Medicina (Brazil). Resolução CFM nº 2.217, de 27 de setembro de\n2018: aprova o Código de Ética Médica. Diário Oficial da União [Internet].[6][9] 2018\nNov 1 [cited 2025 Nov 21];Seção 1:179-82. Available from: https://sistemas.c\nfm.org.br/normas/visualizar/resolucoes/BR/2018/2217\n[6] Brazil. Lei nº 13.709, de 14 de agosto de 2018. Dispõe sobre a proteção de dados pessoais\ne altera a Lei nº 12.965, de 23 de abril de 2014 (Lei Geral de Proteção de Dados Pessoais)\n[Law No. 13.709 of August 14, 2018. Provides for the protection of personal data and\namends Law No. 12.965 of April 23, 2014]. Diário Oficial da União [Internet]. 2018 Aug\n15 [cited 2025 Nov 21];Section 1:59. Available from: http://www.planalto.gov\n.br/ccivil_03/_ato2015-2018/2018/lei/l13709.htm\n[7] García PJ. Corruption in global health:\nthe open secret. Lancet. 2019 Dec\n7;394(10214):2119-24.\n[8] Bai Y, Jones A, Ndousse K, Askell A, Chen A, DasSarma N, et al. Training a helpful and\nharmless assistant with reinforcement learning from human feedback [Preprint]. arXiv.\n2022 [cited 2025 Nov 21]. Available from: https://arxiv.org/abs/2204.058\n62\n[9] Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, et al. Training lan-\nguage models to follow instructions with human feedback. Adv Neural Inf Process Syst.\n2022;35:27730-44.\n8\n"}, {"page": 9, "text": "[10] Ganguli D, Lovitt L, Kernion J, Askell A, Bai Y, Kadavath S, et al. Red teaming language\nmodels to reduce harms: Methods, scaling behaviors, and lessons learned [Preprint]. arXiv.\n2022 [cited 2025 Nov 21]. Available from: https://arxiv.org/abs/2209.078\n58\n[11] Akiri C, Simpson H, Aryal K, Khanna A, Gupta M. Safety and security analysis of large\nlanguage models: benchmarking risk profile and harm potential [Preprint]. arXiv. 2025 Sep\n12 [cited 2025 Nov 24]. Available from: https://arxiv.org/abs/2509.10655\n[12] xAI. Grok 4 model card [Internet]. xAI. 2025 Aug 20 [cited 2025 Nov 21]. Available from:\nhttps://data.x.ai/2025-08-20-grok-4-model-card.pdf\n[13] xAI. Grok 4 Fast model card [Internet]. xAI. 2025 Sep 19 [cited 2025 Nov 21]. Available\nfrom: https://data.x.ai/2025-09-19-grok-4-fast-model-card.pd\nf\n[14] Taylor J. Musk’s AI firm forced to delete posts praising Hitler from Grok chatbot. The\nGuardian [Internet]. 2025 Jul 9 [cited 2025 Nov 21]. Available from: https://www.th\neguardian.com/technology/2025/jul/09/grok-ai-praised-hitle\nr-antisemitism-x-ntwnfb\n[15] Caraballo A. The algorithmic unmasking: how Grok’s \"MechaHitler\" turn revealed the\ninevitable collapse of \"anti-woke\" AI. The Dissident [Internet]. 2025 Jul 9 [cited 2025 Nov\n21]. Available from: https://www.thedissident.news/the-algorithmic\n-unmasking-how-groks-mechahitler-turn-revealed-the-inevita\nble-collapse-of-anti-woke-ai/\n9\n"}]}