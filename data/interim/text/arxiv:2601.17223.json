{"doc_id": "arxiv:2601.17223", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2601.17223.pdf", "meta": {"doc_id": "arxiv:2601.17223", "source": "arxiv", "arxiv_id": "2601.17223", "title": "Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning", "authors": ["Massimiliano Pronesti", "Anya Belz", "Yufang Hou"], "published": "2026-01-23T23:22:20Z", "updated": "2026-01-23T23:22:20Z", "summary": "Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2601.17223v1", "url_pdf": "https://arxiv.org/pdf/2601.17223.pdf", "meta_path": "data/raw/arxiv/meta/2601.17223.json", "sha256": "aa7e0a83e27459e583ebe7a2c2ad5b0fa3c5ec95393095398c9d73b04a104cdb", "status": "ok", "fetched_at": "2026-02-18T02:20:37.258572+00:00"}, "pages": [{"page": 1, "text": "Beyond Outcome Verification: Verifiable Process Reward Models for\nStructured Reasoning\nMassimiliano Pronesti1,2, Anya Belz2, Yufang Hou1,3\n1IBM Research Europe - Ireland, 2Dublin City University,\n3IT:U Interdisciplinary Transformation University Austria\nCorrespondence: massimiliano.pronesti@ibm.com, yufang.hou@it-u.at\nAbstract\nRecent work on reinforcement learning with\nverifiable rewards (RLVR) has shown that large\nlanguage models (LLMs) can be substantially\nimproved using outcome-level verification sig-\nnals, such as unit tests for code or exact-match\nchecks for mathematics. In parallel, process\nsupervision has long been explored as a way to\nshape the intermediate reasoning behaviour of\nLLMs, but existing approaches rely on neural\njudges to score chain-of-thought steps, leaving\nthem vulnerable to opacity, bias, and reward\nhacking. To address this gap, we introduce\nVerifiable Process Reward Models (VPRMs),\na reinforcement-learning framework in which\nintermediate reasoning steps are checked by\ndeterministic, rule-based verifiers. We apply\nVPRMs to risk-of-bias assessment for medical\nevidence synthesis, a domain where guideline-\ndefined criteria and rule-based decision paths\nenable programmatic verification of reason-\ning traces. Across multiple datasets, we find\nthat VPRMs generate reasoning that adheres\nclosely to domain rules and achieve substan-\ntially higher coherence between step-level de-\ncisions and final labels.\nResults show that\nVPRMs achieve up to 20% higher F1 than state-\nof-the-art models and 6.5% higher than verifi-\nable outcome rewards, with substantial gains in\nevidence grounding and logical coherence.\n1\nIntroduction\nLarge language models (LLMs) have made remark-\nable progress in complex natural language process-\ning tasks, including reasoning, planning, and struc-\ntured decision making (Brown et al., 2020; Ope-\nnAI, 2024, 2025). Reinforcement learning with\nverifiable rewards (RLVR) has recently emerged\nas a robust alternative to preference-based rein-\nforcement learning, enabling LLMs to improve\nusing reward signals derived from deterministic\nchecks such as program test suites or exact-match\nmathematical evaluation (Guo et al., 2025; Lam-\nbert et al., 2025). By grounding supervision in\nobjective verifiers rather than learned reward mod-\nels, RLVR avoids many of the alignment failures\nassociated with neural reward hacking and has pro-\nduced state-of-the-art performance in code gen-\neration and mathematical reasoning (Wang et al.,\n2025b; Zhang and Zuo, 2025; Guo et al., 2025;\nYang et al., 2025a).\nHowever, outcome-only RLVR provides rewards\nsolely at the terminal step of reasoning, offering\nno guarantees about whether the model followed a\nvalid intermediate process. To address this lim-\nitation, several extensions augment RLVR with\nstructural or auxiliary signals, such as masked-and-\nreordered self-supervision (Wang et al., 2025c) or\nself-verification mechanisms (Zeng et al., 2025).\nThese works strengthen RLVR but still operate\nfundamentally at the level of outcome verifica-\ntion. Most importantly, none of the above meth-\nods provide a fully verifiable form of process su-\npervision, and existing approaches that score in-\ntermediate Chain-of-Thought (CoT) steps rely on\nneural judges (Lightman et al., 2024; Zhang et al.,\n2025; Zou et al., 2025) which reintroduce opacity,\nbias, and opportunities for reward hacking (Amodei\net al., 2016; Skalse et al., 2022).\nTo date, no work has demonstrated that pro-\ncess rewards themselves can be made verifiable\non tasks whose structure admits deterministic sym-\nbolic checking. Yet such a capability would be\nhighly desirable: if intermediate reasoning steps\ncan be validated explicitly, then reinforcement\nlearning could optimise not only for correct out-\ncomes, but also for transparent, logically sound\nreasoning. Crucially, verifiable step-level rewards\nwould eliminate the aforementioned problems as-\nsociated with neural process rewards.\nThis leaves a key open problem: can reinforce-\nment learning be used to train models whose entire\nreasoning trajectory is rewarded only when each\narXiv:2601.17223v1  [cs.CL]  23 Jan 2026\n"}, {"page": 2, "text": "Step 1\nRandom\nUnpredictable\nReasoning process \nRandomization \nreported?\nRandomization\nmethod?\nSequence\npredictability?\nOutput\nReported\nRisk: Low\nStudy on an\nintervention to\nimprove patient\nrecovery after\nsurgery.\nLabel \nInput \nStep 2\nStep 3\nRandomization\nreported\nRandomization\nmethod\nReported\n? Moderate\nNot Reported\nSequence\npredictability\nRandom\n- High\nNon Random\n? Moderate\nPredictable\nBaseline\nimbalance\nUnpredictable\n- High\nLikely\n+ Low\nNone\nStep 4\nBaseline\nImbalance?\nNone\nStep\nFinal Outcome\nLabel\nLabel \nLabel \nLabel \nFigure 1: Illustration of the verifiable reasoning setup for risk-of-bias assessment (type A: bias arising from the\nrandomisation process). Top: given an input study x, the model produces a structured reasoning trace Y =\n(o1, . . . , oT ) with step-level labels (ℓ1, ℓ2, ℓ3, ℓ4), each corresponding to a guideline-defined assessment question.\nBottom: the corresponding rule-based decision tree, which deterministically maps each combination of step-level\nlabels to low (+), high (-), or moderate (?) risk.\nintermediate step satisfies domain-defined, rule-\nbased criteria? To address this gap, we introduce\nVerifiable Process Reward Models (VPRMs), a\nreinforcement-learning framework in which each\nreasoning step is assessed by a deterministic veri-\nfier grounded in explicit task guidelines. VPRMs\nprovide fine-grained, step-level reward signals that\ncomplement outcome-level verification, guiding\noptimisation toward reasoning traces that are both\ncorrect and aligned with domain logic. Crucially,\nwe prove that under mild assumptions, VPRMs\noffer theoretical guarantees that gradient-based up-\ndates assign positive expected weight to correct\nreasoning trajectories and negative weight to incon-\nsistent ones, thereby encouraging sound reasoning.\nTo evaluate this framework, we consider a chal-\nlenging, real-world structured-reasoning task: risk-\nof-bias (RoB) assessment in clinical systematic re-\nviews. In this setting, studies must be evaluated for\nsusceptibility to systematic error (Chandler et al.,\n2019), and domain guidelines prescribe a rigid se-\nquence of reasoning steps and decision rules that\nmake the task uniquely amenable to verifiable pro-\ncess supervision. Figure 1 illustrates the verifiable\nreasoning process for assessing randomisation bias,\none of the RoB domains defined in the Cochrane\nRoB tool for randomised trials (Sterne et al., 2019).\nAcross multiple models and RoB domains, we\ncompare VPRMs against outcome-only RLVR,\nneural process-reward baselines, and pretrained\nLLMs prompted for Chain-of-Thought (CoT). Our\nresults show that VPRM-trained models achieve\nsubstantially higher accuracy and more coherent\nreasoning traces, showing that verifiable process su-\npervision offers a more reliable optimization signal\nfor both result correctness and process soundness.\nIn summary, our contributions are as follows: (i)\nwe propose a verifiable process reward framework\nthat integrates deterministic step-level verification\nwith reinforcement learning for dense, interpretable\nsupervision over reasoning trajectories (Section 3);\n(ii) we show that, under mild reward-separation\nassumptions, verifiable process rewards encourage\ncorrect reasoning (Section 3.3, Appendix A); and\n(iii) we validate the approach on risk-of-bias assess-\nment in medical systematic reviews, demonstrating\nsignificant improvements over outcome-only and\nneural process-reward baselines (Section 4).\n2\nPreliminaries\n2.1\nPolicy Optimisation Algorithms\nGroup Relative Policy Optimization (GRPO)\nGRPO (Shao et al., 2024) is a widely-adopted\ngroup-based reinforcement learning method that\noptimises a policy by comparing multiple candi-\ndate completions for the same input.\nFor each passage x, G candidate completions\n{yi}G\ni=1 ∼πold(· | x) are sampled from the refer-\n"}, {"page": 3, "text": "ence policy πold to encourage robustness and diver-\nsity. These completions are scored using a reward\nmodel. The raw rewards Ri are then normalised\nacross the group:\nAi = Ri −E[Rj]\np\nV[Rj]\n,\nj ∈{i, ..., G}\nwhere E[Rj] and V[Rj] are respectively the mean\nand variance of the rewards for the group of re-\nsponses. The policy is optimised using a clipped,\nKL-regularised objective that encourages agree-\nment with high-reward behaviours while maintain-\ning proximity to a reference model πref:\nLGRPO(θ) = E\n\u0014 1\nG\nG\nX\ni=1\n1\n|yi|\n|yi|\nX\nt=1\nmin\n\u0010\npi,t(θ)Ai,\nclip(pi,t(θ), 1 −ε, 1 + ε)Ai\n\u0011\n−β KL[πθ ∥πref]\n\u0015\nwhere β governs the regularisation strength and\npi,t(θ) is the token-level probability ratio defined\nas follows:\npi,t(θ) = πθ(yi,t | x, yi,<t)\nπθold(yi,t | x, yi,<t)\nDynamic sAmpling Policy Optimization (DAPO)\nBuilding on GRPO, DAPO (Yu et al., 2025) re-\nmoves the KL penalty, introduces a clip-higher\nmechanism, incorporates dynamic sampling, ap-\nplies a token-level policy gradient loss, and adopts\noverlong reward shaping. The key improvement\nis dynamic sampling, by over-sampling and filter-\ning out prompts with the accuracy equal to 1 and\n0, leaving all prompts in the batch with effective\ngradients, avoiding dampening the gradient signals\nfor model training with a larger variance in the gra-\ndient. This leads to the following maximisation\nobjective:\nLDAPO(θ) = E\n\u0014\n1\nPG\ni=1 |yi|\nG\nX\ni=1\n|yi|\nX\nt=1\nmin\n\u0010\npi,t(θ)Ai,\nclip(pi,t(θ), 1 −εH, 1 + εL)Ai\n\u0011\u0015\n,\ns.t. 0 < |{yi| is_equivalent(yi, a)}| < G\nThis ensures that for the same input, the sampled\nset contains both correct and incorrect answers.\n2.2\nRule-based Reward Modeling\nIn rule-based reward modeling, the reward signal\nis defined by explicit, hand-crafted rules that verify\nwhether a model output satisfies task-specific con-\nstraints. This approach is particularly effective for\nverifiable tasks with clear notions of correctness,\nsuch as mathematical problem solving, program\nsynthesis, or logical reasoning. The reward is com-\nputed deterministically by a verifier and does not\nrely on learned preference models. In its simplest\nform, the reward is binary:\nR(y) =\n(\n1\nif the output is verified as correct,\n0\notherwise.\nThis setting provides scalable, reliable supervision\nwith minimal ambiguity.\n2.3\nSystematic Reviews\nSystematic reviews provide a principled framework\nfor aggregating empirical evidence through pre-\ndefined search strategies, explicit inclusion crite-\nria, and reproducible synthesis pipelines (Chan-\ndler et al., 2019). Their value lies in reducing\nsubjective judgment in evidence collection and\nenabling structured comparison across heteroge-\nneous studies. One of the gold-standard reposito-\nries of systematic reviews is the Cochrane library (),\nwhich contains curated reviews adhering to strict\nevidence-synthesis and bias-assessment protocols.\n2.4\nRisk of Bias Assessment\nRisk of bias assessment is a core component of sys-\ntematic reviews, providing structured evaluations\nof methodological flaws in primary studies that di-\nrectly inform evidence synthesis and the credibility\nof review conclusions. Assessments are structured\ninto a fixed set of domains of bias, focusing on\ndifferent aspects of trial design, conduct and re-\nporting (Chandler et al., 2019; Sterne et al., 2019).\nWithin each domain, information about features\nof the trial that are relevant to risk of bias is col-\nlected and mapped to judgments of low, medium\nor high risk. This domain-based assessment iden-\ntifies issues such as selection bias, measurement\nbias, and selective reporting, enabling subsequent\nevidence synthesis to appropriately weight studies\nnot only in terms of their results, but also in terms\nof their risk of bias. Additional details on the differ-\nent bias domains and the corresponding assessment\nreasoning process are provided in Appendix B.\n3\nVerifiable Process Reward Models\nWe introduce Verifiable Process Reward Models\n(VPRMs), a framework for process supervision\n"}, {"page": 4, "text": "<think>\nThe paper reports that the    \nrandomization method used was\n\"random,\" and the sequence generation\nappears to follow standard protocols.\nHowever, there are some\ninconsistencies in the sequence\npattern, and the method of\nrandomization was not explicitly\ndetailed.\n</think> \n<answer> \n  risk: high\n</answer>\n <think>\n Step 1: Identify_randomization_report\n ...\n Answer: reported\n Step 2: Classify_randomization_method\n ...\n Answer: random\n Step 3: Assess_sequence_predictability\n ...\n Answer: unclear\n </think>\n <answer>\n  risk:low\n </answer>\nRCT\nLow\nRCT\nReasoning\nGround truth\nReasoning\nGround truth\nVerification\nVerifiable Outcome Rewarding\nVerifiable Process Rewarding\nLow\nscore: 0.0\nscore: 1.0\nStep \nLabel \nStep \nLabel \nStep \nLabel \nstep score: 1.0\nstep label score: 0.66\nFigure 2: Comparison between verifiable outcome rewards (left), which evaluates only the final risk label, and\nverifiable process rewards (right), which additionally verifies each reasoning step and its associated label.\nin which intermediate reasoning steps are evalu-\nated by deterministic, externally checkable verifiers\nrather than learned neural judges. The framework\nis presented in the context of risk assessment tasks,\nwhich naturally admit structured reasoning steps,\ndiscrete labels, and rule-based transitions that can\nbe validated against domain guidelines.\n3.1\nReasoning Trajectories and Steps\nFor an input x, the model produces a reasoning tra-\njectory Y = (o1, . . . , oT ) with a stochastic policy\nπθ(Y | x) =\nT\nY\nt=1\nπθ(ot | o<t, x).\nEach step t contains two discrete outputs: (i) a\nstep identifier st ∈S and (ii) a step label ˆℓt ∈Lt,\nwhich represents the model’s answer for that step.\nDomain guidelines specify, for each prefix Y≤t,\nthe gold step identifier s⋆\nt and gold step label ℓ⋆\nt\nobtained by applying the rule-based logic of the\ntask.\n3.2\nVerifiers and Process Rewards\nCorrectness is evaluated by two bounded scoring\nfunctions:\nsn\nt(st, s⋆\nt ),\nsl\nt(ˆℓt, ℓ⋆\nt ),\neach mapping a model output and its corresponding\ngold value into [0, 1]. These provide a positive\nreward signal when the model selects the correct\nstep identifier and the correct label according to the\ntask rules. The instanteneous step-level verifiable\nreward is then defined as:\nrt(Y ; x) = wn\nt sn\nt(st, s⋆\nt ) + wl\nt sl\nt(ˆℓt, ℓ⋆\nt ),\nwhere wn\nt , wl\nt ≥0 are preset weights.\nA terminal outcome reward rlabel evaluates\nwhether the final risk value predicted from the full\nreasoning trace matches the gold risk value. The\nfull verifiable process reward is then\nR(Y ; x) =\nT\nX\nt=1\nrt(Y ; x) + rlabel\nAs illustrated in Figure 2, this reward is fully\ncomputable using deterministic, rule-based checks,\nmaking all components of the reasoning trajectory\nverifiable.\n3.3\nReward Separation and Optimisation\nGuarantee\nAn interesting consequence of combining VPRMs\nwith GRPO or DAPO is that the resulting opti-\nmisation dynamics exhibit a clear structure: rule-\nconsistent trajectories are, in expectation, pushed\nin a beneficial direction.\nLet R(Y ) denote the verifiable process reward\nassigned to a response y, and let G be the number\nof responses sampled for the input x. Let C be the\nevent that Y is correct according to the rule-based\ntask semantics. Define the conditional expectations\nµc := E[R(Y ) | C],\nµi := E[R(Y ) | Cc].\nWe assume the following mild conditions (see\nAppendix A for the full formal statement and dis-\ncussion): (i) R(Y ) has finite variance, (ii) correct\nreasoning chains receive strictly larger expected\nreward than incorrect ones (µc > µi), and (iii) a\nsufficiently large G to ensure stable gradient up-\ndates.\n"}, {"page": 5, "text": "Theorem 1. Under the above hypotheses, the\nexpected GRPO and DAPO advantage E[ ˆA(Y )]\nsatisfies\nE[ ˆA(Y ) | C] > 0,\nE[ ˆA(Y ) | Cc] < 0.\nThus, both GRPO and DAPO assign positive\nexpected weight to correct reasoning trajectories\nand negative weight to incorrect ones, raising the\nlikelihood of correct reasoning in expectation.\nThis follows from the theoretical results pre-\nsented by Wen et al. (2025). A proof for both\nGRPO and DAPO objectives is provided in Ap-\npendix A.\n4\nExperiments\n4.1\nTraining Dataset Creation\nThe first stage of our methodology involved the\nacquisition of a high-quality, human-aligned corpus\nsuitable for training reward models.\nTo this end, we build on the COCHRANEFOR-\nEST (Pronesti et al., 2025a) and COCHRANEFOR-\nESTEXT (Pronesti et al., 2025b) datasets, which\nprovide two essential components for our task: (i)\nthe forest plots extracted from Cochrane systematic\nreviews, and (ii) the full-text primary studies corre-\nsponding to every trial included in those plots. The\ninclusion of full papers is critical, as risk-related\nsignals often depend on methodological details\navailable only in the complete manuscripts.\nFrom these corpora, we retain exclusively the\nforest plots that contain an associated risk-of-bias\nmap. Each such plot establishes an explicit corre-\nspondence between its set of included studies and\ntheir study-level bias assessments. We therefore\ndefine a single instance as a paper-risk pair consist-\ning of a full-text study and its aligned risk-of-bias\ndefinition and label extracted from the map. The\nresulting dataset comprises 2,946 instances drawn\nfrom 104 systematic reviews, totalling 4M tokens.\n4.2\nSynthetic Data Annotation for Structural\nReasoning Processes\nFollowing the methodology described by Pronesti\net al. (2025b), we enrich our dataset with step-level\nlabels for RL using LlaMa 3.1 405B (Grattafiori\net al., 2024) with the system prompt shown in Fig-\nure 4 (Appendix), temperature of 0.7 and 2,048\ntokens generation limit. An example data instance\nis provided in Table 8 (Appendix); a human verifi-\ncation of the generated annotations in Appendix D.\nDataset\nTrain\nTest\nTotal\nAvg tokens\nCOCHRANEFORESTEXT\n2651\n295\n2946\n13,596.9\nCOCHRANEFOREST\n–\n1846\n1846\n12,722.8\nRoBBR Cochrane\n774\n906\n1680\n9,084.6\nRoBBR Non-Cochrane\n–\n2489\n2489\n7,940.7\nTable 1: Datasets statistics. Train/test split only ap-\nplies to COCHRANEFORESTEXT and RoBBR Cochrane.\nCOCHRANEFOREST and RoBBR Non-Cochrane are\nused for testing.\n4.3\nExperimental Setup\nTraining and Evaluation Datasets.\nFor training,\nwe use the corpus constructed with the methods\nfrom Section 4.1, allocating 2,651 instances for\ntraining and 295 for validation. We also include the\n774 training instances from the RoBBR Cochrane\nsplit (Wang et al., 2025a). All training data are\naugmented with step-level labels (Section 4.2).\nFor evaluation, we consider three datasets. The\nfirst is COCHRANEFOREST (Pronesti et al., 2025b),\nwhich contains 1,846 instances drawn from 48\nCochrane Systematic Reviews and 202 forest plots.\nThe second consists of the two test sets from the\nRoBBR benchmark: RoBBR Cochrane, which con-\ntains 906 datapoints originating from 204 papers in-\ncluded in 58 Cochrane reviews; and RoBBR–Non-\nCochrane, which contains 2,489 datapoints drawn\nfrom 496 non-Cochrane reviews that collectively\nassess 496 papers.\nDataset statistics are sum-\nmarised in Table 1, while per-risk-type statistics\nare reported in Table 6 (Appendix).\nEvaluation Metrics.\nWe evaluate all models\non the main prediction task using Accuracy and\nmacro–F1, computed over the discrete risk labels.\nIn addition, for analyses (Section 4.6), we re-\nport Coherence for the VPRM-trained models, de-\nfined as the proportion of datapoints for which the\nmodel’s predicted risk is consistent with the con-\nclusion implied by its own intermediate reason-\ning. Formally, let ˆri ∈R denote the final risk\nvalue predicted by the model for datapoint i, and\nlet ˆℓi,1, . . . , ˆℓi,T denote the sequence of step-level\nlabels produced along the corresponding reasoning\ntrace. Let D : L1 × · · · × LT →R be a fixed, ex-\nternally specified decision function mapping step-\nlevel labels to a risk value. In our setting, this is the\nset of macros used in the RoB2 tool (Sterne et al.,\n2019) (See Figure 1). The coherence indicator for\ndatapoint i is then\nCi := 1\nn\nˆri = D(ˆℓi,1, . . . , ˆℓi,T )\no\n"}, {"page": 6, "text": "and the dataset-level Coherence is given by\nCoherence := 1\nN\nN\nX\ni=1\nCi\nBy construction, Coherence measures the degree to\nwhich the model’s final conclusions are internally\nconsistent with the reasoning signals expressed in\nits own intermediate steps.\nTraining Setup.\nWe conduct our training us-\ning a compact instruct models of recent release:\nQwen2.5-7B (Yang et al., 2025b). We study two\nmethodological regimes: supervised fine-tuning\n(SFT) with reasoning traces augmentation and\nreinforcement learning (RL) with verifiable re-\nwards. SFT is conducted for 5 epochs with a per-\ndevice batch size of 1, a learning rate of 5 × 10−5,\nand the AdamW optimiser (Loshchilov and Hut-\nter, 2017). For RL, we investigate two policy-\noptimisation algorithms, GRPO (Shao et al., 2024)\nand DAPO (Yu et al., 2025), combined with two\nreward types: verifiable outcome reward and our\nverifiable process reward approach. All RL config-\nurations are trained for 3 epochs with a learning\nrate of 1 × 10−6, per-device batch size 1, and 16\nsampled generations per batch. Further implemen-\ntation details are provided in Appendix F.\nModel Baselines.\nTo validate our results, we\ncompare a range of open- and closed-source mod-\nels, with and without reasoning capabilities. mod-\nels are evaluated in zero-shot settings with prompt\nand hyperparameters shown in Appendix C. We in-\nclude three main model families: Qwen 2.5 (Yang\net al., 2025b), Llama 3.1 (Grattafiori et al., 2024),\nand Granite 3.1 (Granite Team, 2024). In addition,\nwe benchmark the distilled Qwen and Llama mod-\nels derived from DeepSeek-R1 (Guo et al., 2025).\nLastly, we include one closed-source and two open-\nsource models from OpenAI (OpenAI et al., 2025).\nTo contextualise the effectiveness of our verifi-\nable reward formulation, we also evaluate neural\nprocess-reward baselines. At present, no pretrained\nPRM exists for risk-of-bias assessment or, more\nbroadly, for non-mathematical scientific reasoning.\nTherefore, to approximate a general-purpose PRM,\nwe follow prior work on using LLMs as step-level\njudges, prompting a model to assign correctness\nscores to each reasoning step (Song et al., 2025).\nThis setup has been shown to deliver competitive\nprocess-level feedback in domains where explicit\nPRM training data is unavailable, and therefore\nserves as a reasonable baseline for comparison. In\naddition, we train a policy using MedPRM (Yun\net al., 2025) as a reward model, one of the first\nopen-source PRMs for general medical reasoning.\n4.4\nMain Results\nComparison with Pretrained Baselines.\nTa-\nble 2 presents a performance comparison of pre-\ntrained and fine-tuned language models on the\nCOCHRANEFOREST and RoBBR benchmarks.\nAcross all datasets, models trained with verifi-\nable rewards substantially outperform pretrained\nmodels, including large reasoning-enabled sys-\ntems. Reinforcement learning with verifiable out-\ncome rewards already yields strong gains over\nsupervised fine-tuning, while incorporating veri-\nfiable process rewards consistently leads to fur-\nther improvements in both accuracy and macro-F1.\nOn COCHRANEFOREST, Qwen2.5-7B trained with\nVPRM achieves the best overall performance, and\nsimilar improvements are observed on both RoBBR\nCochrane and Non-Cochrane. The latter result in-\ndicates that the benefits of verifiable process super-\nvision generalise beyond the training distribution,\nrather than exploiting dataset-specific regularities.\nComparison with Neural PRMs.\nTable 3 com-\npares verifiable process rewards against neural\nprocess-reward baselines.\nIn all settings, mod-\nels also receive the same verifiable outcome re-\nward; the comparison isolates only the effect of\nthe process-level supervision. While neural PRMs\nsubstantially improve over outcome-only train-\ning, they are consistently outperformed by VPRM.\nThis performance gap suggests that learned step-\nlevel judges introduce noise and misalignment that\nlimit their effectiveness, whereas deterministic,\nguideline-based verification provides a cleaner and\nmore reliable optimisation signal. These results\nsupport the central claim that verifiable process re-\nwards offer a stronger and more robust alternative\nto neural process supervision for complex, struc-\ntured reasoning tasks.\n4.5\nAblation Studies\nTo assess the contribution of different components\nof our verifiable reward formulation, we conduct\ntwo ablation studies: the structure of process super-\nvision and the inclusion of an outcome-level reward.\nFor process supervision, we compare a steps-only\nreward that verifies whether the model follows the\ncorrect sequence of reasoning steps, irrespective\n"}, {"page": 7, "text": "Model\nThink\nCOCHRANEFOREST\nRoBBR Cochrane\nRoBBR Non-Cochrane\nAcc\nF1\nAcc\nF1\nAcc\nF1\nPretrained LLMs\nGPT-4-0125\n✗\n52.4\n41.6\n56.0\n47.9\n47.8\n42.3\nGPT-OSS-20B\n✓\n61.4\n43.9\n56.4\n50.3\n46.3\n42.8\nGPT-OSS-120B\n✓\n67.1\n49.8\n59.5\n51.0\n48.8\n44.2\nQwen2.5-7B\n✗\n32.9\n31.6\n35.8\n34.1\n36.4\n34.5\nQwen2.5-14B\n✗\n39.0\n35.1\n37.0\n35.5\n35.4\n32.5\nQwen2.5-72B\n✗\n51.3\n42.1\n56.1\n51.0\n47.5\n43.6\nLlama-3.1-8B\n✗\n36.4\n30.6\n34.5\n32.1\n36.4\n32.5\nLlama-3.1-70B\n✗\n38.8\n30.2\n49.5\n40.0\n42.5\n38.9\nLlama-3.1-405B\n✗\n68.4\n45.5\n59.4\n44.0\n52.5\n39.8\nDeepSeek-Qwen-7B\n✓\n–\n–\n–\n–\n–\n–\nDeepSeek-Qwen-14B\n✓\n33.3\n19.2\n35.8\n23.5\n35.4\n23.5\nDeepSeek-Qwen-32B\n✓\n40.8\n35.9\n44.9\n40.4\n46.4\n41.3\nDeepSeek-Llama-8B\n✓\n–\n–\n–\n–\n–\n–\nDeepSeek-Llama-70B\n✓\n44.2\n33.5\n57.3\n41.2\n48.3\n42.7\nGranite-3.1-3B\n✗\n24.4\n23.6\n22.2\n21.8\n13.7\n14.9\nGranite-3.1-8B\n✗\n24.7\n22.0\n35.8\n31.6\n33.2\n28.2\nGranite-4.0-h-small (32B)\n✗\n48.2\n33.5\n45.4\n41.2\n40.9\n33.1\nOur Models\nQwen2.5-7B-SFT\n✓\n45.1\n36.9\n38.6\n32.4\n38.3\n31.9\nQwen2.5-7B-GRPO\n✓\n81.5\n70.2\n63.1\n58.0\n56.8\n45.1\nQwen2.5-7B-DAPO\n✓\n76.8\n57.3\n60.2\n45.4\n55.8\n43.6\nQwen2.5-7B-GRPO-VPRM\n✓\n87.9\n76.7\n65.2\n58.5\n60.7\n47.2\nQwen2.5-7B-DAPO-VPRM\n✓\n79.2\n60.6\n60.7\n48.9\n57.1\n45.3\nTable 2: Evaluation results across models on three datasets, reporting Accuracy and macro-F1. “–” denotes\nunparsable or inconclusive outputs. Best results are bolded; second-best are underlined.\nMethod\nAcc\nF1\nNeural PRMs\nQwen2.5-7B-GRPO-PRM-GPT-OSS\n78.2\n56.1\nQwen2.5-7B-GRPO-MedPRM\n76.8\n53.4\nVerifiable Rewards\nQwen2.5-7B\n32.9\n31.6\nQwen2.5-7B-GRPO\n81.5\n70.2\nQwen2.5-7B-GRPO-VPRM\n87.9\n76.7\nTable 3:\nPerformance comparison between neural\njudges, rule-based rewarding and verifiable process re-\nwarding on COCHRANEFOREST.\nof the correctness of their content, against the full\nVPRM, which additionally evaluates the correct-\nness of each step and enforces consistency with the\nguideline-defined decision structure. For outcome\nsupervision, we train each variant both with and\nwithout a verifiable outcome reward.\nTable 4 shows that removing the outcome reward\nleads to substantial performance degradation, indi-\ncating that step-structure verification alone is insuf-\nficient to reliably optimize the task. Nevertheless,\neven in this setting, the full VPRM outperforms the\nSetting\nAcc\nF1\nw/o Outcome Reward\nSteps-only process reward\n34.4\n32.3\nFull VPRM\n40.2\n35.3\nw/ Outcome Reward\nSteps-only process reward\n83.1\n71.8\nFull VPRM\n87.9\n76.7\nTable 4: Ablation study on outcome and process reward\ncomponents on COCHRANEFOREST. We report Accu-\nracy (Acc) and F1; lower blocks compare models with\nand without outcome reward, and columns compare\nsteps-only supervision to the full VPRM formulation.\nsteps-only variant, demonstrating the importance\nof verifying not just the presence but also the cor-\nrectness and logical composition of intermediate\nreasoning steps. When the outcome reward is in-\ncluded, performance improves markedly, and the\nfull VPRM consistently achieves the best results,\nshowing that combining verifiable outcome super-\nvision with fine-grained, correctness-aware process\nrewards yields the strongest learning signal.\n"}, {"page": 8, "text": "4.6\nAnalyses\nImpact of Thought Process.\nTable 5 reports Co-\nherence on the COCHRANEFOREST testset for the\nmodels trained with VPRMs compared with pre-\ntrained baselines prompted to output process labels.\nIn addition, we report Coherent Accuracy (CA),\ndefined as the accuracy restricted to coherent in-\nstances. That is, among the datapoints for which\nthe model’s final prediction is consistent with the\ndecision implied by its own reasoning steps (i.e.,\nCi = 1), we measure the proportion whose final\npredicted label is also correct. Formally, letting ˆyi\nand yi denote the predicted and gold labels respec-\ntively, we define\nCA =\nPN\ni=1 1[Ci = 1 ∧ˆyi = yi]\nN\nCA therefore quantifies the reliability of the\nmodel’s predictions conditioned on coherence.\nResults show that pretrained models exhibit\nlow coherence and very low CA, indicating that\neven when their step-level reasoning appears self-\nconsistent, it rarely leads to correct final judg-\nments. In contrast, VPRM-trained models achieve\nboth substantially higher coherence and high CA,\ndemonstrating that they not only follow the de-\ncision logic faithfully but also produce accurate\nconclusions when they do so, suggesting improved\nrobustness and interpretability.\nReward Dynamics.\nFigure 3 shows that process\nand correctness rewards follow closely aligned tra-\njectories: both rise sharply early, stabilize within\nthe same oscillatory range, and peak at similar\npoints. This alignment indicates that improved\nstep-level reasoning directly improves final-label\ncorrectness. In contrast, the thought-format reward\nsaturates quickly and remains flat, contributing lit-\ntle once formatting is learned. Overall, the strongly\ncorrelated shapes of the process and correctness\ncurves highlight that VPRM training drives coher-\nent, mutually reinforcing gains in intermediate and\nfinal reasoning behaviour.\n5\nRelated Work\nFor a more detailed overview of related work,\nrefer to Appendix E. Prior work extends rein-\nforcement learning with verifiable rewards (RLVR)\nbeyond outcome-only supervision by adding\nstructural signals, such as masked-and-reordered\nself-supervision or self-verification modules that\nModel\nCoherence\nCA\nGPT-OSS-120B\n36.2\n28.5\nQwen2.5-72B\n44.3\n24.9\nLlama-3.1-405B\n50.7\n27.1\nQwen2.5-7B-GRPO-VPRM\n89.5\n75.0\nQwen2.5-7B-DAPO-VPRM\n80.1\n69.4\nTable 5: Coherence scores for Qwen models trained\nwith DAPO and GRPO using verifiable process rewards\ncompared with pretrained LLMs on COCHRANEFOR-\nEST. Best results are bolded; second-best underlined.\n0\n200\n400\n600\n800 1,000 1,200 1,400 1,600 1,800 2,000\n0.2\n0.4\n0.6\n0.8\n1\nStep\nTraining Reward\nProcess Reward\nAccuracy Reward\nThought Format Reward\nFigure 3: Reward dynamics. Format rewards plateau\nearly, while accuracy and process rewards improve grad-\nually, indicating that LLMs quickly learn structure but\ncontinue to refine quality.\nguide reasoning trajectories (Wang et al., 2025c;\nZeng et al., 2025; Wen et al., 2025).\nWhile\neffective, these methods remain centered on\nterminal-outcome verification. In parallel, process-\nsupervision approaches score intermediate reason-\ning steps using neural judges (Lightman et al.,\n2024; Zhang et al., 2025; Zou et al., 2025), provid-\ning dense feedback but relying on non-verifiable\nmodel-based evaluations prone to bias and reward\nhacking (Amodei et al., 2016; Skalse et al., 2022).\nWe bridge this gap by combining RLVR with step-\nwise rule-based verification, enabling transparent\nand verifiable process supervision.\n6\nConclusion\nIn this paper, we introduce verifiable process re-\nwards that integrate deterministic step-level veri-\nfication with reinforcement learning, provide the-\noretical guarantees under mild assumptions, and\ndemonstrate substantial empirical gains on risk-of-\nbias assessment in medical systematic reviews.\nOur results indicate that verifiable process super-\nvision is a practical and robust approach to induc-\ning reliable reasoning behaviour in large language\nmodels, opening the door to broader applications\nin structured scientific and decision-making tasks.\n"}, {"page": 9, "text": "Limitations\nWhile VPRMs offer strong guarantees for struc-\ntured reasoning tasks, several limitations remain.\nFirst, the approach relies on the existence of de-\nterministic, domain-specific rules; tasks lacking\nwell-defined intermediate reasoning steps may not\nbenefit directly. Second, our empirical evaluation is\ncurrently focused on risk-of-bias assessment; gener-\nalisation to other domains, particularly open-ended\nreasoning tasks, remains to be established.\nAdditionally, the approach assumes that the\nmodel can produce reasoning traces in a format\ncompatible with the verifiers; misalignment be-\ntween model output and verifier expectations could\nreduce reward effectiveness, especially in the con-\ntext of smaller models. Finally, while VPRMs re-\nduce reliance on neural reward models, they do not\nfully eliminate other sources of model bias or errors\narising from incomplete guidelines. Addressing\nthese challenges will be critical for deploying ver-\nifiable process supervision in broader, real-world\napplications.\nReferences\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul\nChristiano, John Schulman, and Dan Mané. 2016.\nConcrete problems in ai safety.\narXiv preprint\narXiv:1606.06565.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, and 1 others. 2020. Language models are\nfew-shot learners. Advances in Neural Information\nProcessing Systems (NeurIPS), 33.\nJacqueline Chandler, Miranda Cumpston, Tianjing\nLi, Matthew J. Page, and Vivian A. Welch. 2019.\nCochrane handbook for systematic reviews of inter-\nventions. Hoboken: Wiley, 4.\nAbel Corrêa Dias, Viviane Pereira Moreira, and João\nLuiz Dihl Comba. 2025. Robin: A transformer-based\nmodel for risk of bias inference with machine reading\ncomprehension. Journal of Biomedical Informatics,\n166:104819.\nIBM Granite Team. 2024. Granite 3.0 language mod-\nels.\nURL: https://github.com/ibm-granite/granite-\n3.0-language-models.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The LLaMa 3\nherd of models. arXiv preprint arXiv:2407.21783.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai\nYu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao,\nZhuoshu Li, Ziyi Gao, Aixin Liu, and 180 others.\n2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in LLMs via reinforcement learning. Preprint,\narXiv:2501.12948.\nJiajie Huang, Honghao Lai, Weilong Zhao, Danni Xia,\nChunyang Bai, Mingyao Sun, Jianing Liu, Jiayi Liu,\nBei Pan, Jinhui Tian, and 1 others. 2025.\nLarge\nlanguage model–assisted risk-of-bias assessment in\nrandomized controlled trials using the revised risk-\nof-bias tool: Usability study. Journal of Medical\nInternet Research, 27:e70450.\nHugging Face. 2025. Open R1: A fully open reproduc-\ntion of deepseek-r1.\nChangkai Ji, Bowen Zhao, Zhuoyao Wang, Yingwen\nWang, Yuejie Zhang, Ying Cheng, Rui Feng, and\nXiaobo Zhang. 2025. Robguard: Enhancing llms\nto assess risk of bias in clinical trial documents. In\nProceedings of the 31st International Conference on\nComputational Linguistics, pages 1258–1277.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\nsymposium on operating systems principles, pages\n611–626.\nHonghao Lai, Jiayi Liu, Chunyang Bai, Hui Liu, Bei\nPan, Xufei Luo, Liangying Hou, Weilong Zhao,\nDanni Xia, Jinhui Tian, and 1 others. 2025. Lan-\nguage models for data extraction and risk of bias\nassessment in complementary medicine. npj Digital\nMedicine, 8(1):74.\nNathan Lambert, Jacob Morrison, Valentina Pyatkin,\nShengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James Validad Miranda, Alisa Liu, Nouha\nDziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria\nGraf, Jena D. Hwang, Jiangjiang Yang, Ronan Le\nBras, Oyvind Tafjord, Christopher Wilhelm, Luca\nSoldaini, and 4 others. 2025. Tulu 3: Pushing fron-\ntiers in open language model post-training. In Second\nConference on Language Modeling.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harri-\nson Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2024. Let’s verify step by step. In The Twelfth Inter-\nnational Conference on Learning Representations.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nOpenAI. 2024.\nIntroducing openai o1.\nhttps://\nopenai.com/it-IT/o1/.\n"}, {"page": 10, "text": "OpenAI. 2025. Introducing gpt-5. https://openai.\ncom/it-IT/gpt-5/.\nOpenAI, Sandhini Agarwal, Lama Ahmad, Jason\nAi, Sam Altman, Andy Applebaum, Edwin Ar-\nbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haim-\ning Bao, Boaz Barak, Ally Bennett, Tyler Bertao,\nNivedita Brett, Eugene Brevdo, Greg Brockman, Se-\nbastien Bubeck, Che Chang, and 107 others. 2025.\ngpt-oss-120b & gpt-oss-20b model card. Preprint,\narXiv:2508.10925.\nMassimiliano Pronesti, Joao H Bettencourt-Silva, Paul\nFlanagan, Alessandra Pascale, Oisín Redmond, Anya\nBelz, and Yufang Hou. 2025a.\nQuery-driven\ndocument-level scientific evidence extraction from\nbiomedical studies. In Proceedings of the 63rd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 28034–\n28051, Vienna, Austria. Association for Computa-\ntional Linguistics.\nMassimiliano Pronesti, Michela Lorandi, Paul Flanagan,\nOisín Redmond, Anya Belz, and Yufang Hou. 2025b.\nEnhancing study-level inference from clinical trial\npapers via reinforcement learning-based numeric rea-\nsoning. In Proceedings of the 2025 Conference on\nEmpirical Methods in Natural Language Processing,\npages 30345–30361, Suzhou, China. Association for\nComputational Linguistics.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.\nDeepSeekMath: Pushing the limits of mathemati-\ncal reasoning in open language models. Preprint,\narXiv:2402.03300.\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov,\nand David Krueger. 2022. Defining and characteriz-\ning reward gaming. Advances in Neural Information\nProcessing Systems, 35:9460–9471.\nMingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou,\nand Yu Cheng. 2025. PRMBench: A fine-grained\nand challenging benchmark for process-level reward\nmodels. In Proceedings of the 63rd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 25299–25346, Vienna,\nAustria. Association for Computational Linguistics.\nJonathan AC Sterne, Jelena Savovi´c, Matthew J.\nPage, Roy G. Elbers, Natalie S. Blencowe, Isabelle\nBoutron, Christopher J. Cates, He Cheng, Mark S.\nCorbett, Sandra M. Eldridge, Miguel A. Hernán,\nSally Hopewell, Asbjørn Hróbjartsson, Diana R. Jun-\nqueira, Peter Jüni, Jamie J. Kirkham, Toby Lasser-\nson, Tianjing Li, Ann McAleenan, and 8 others. 2019.\nRoB 2: a revised tool for assessing risk of bias in\nrandomised trials. BMJ, 366:l4898.\nSimon Šuster, Timothy Baldwin, and Karin Verspoor.\n2024. Zero-and few-shot prompting of generative\nlarge language models provides weak assessment of\nrisk of bias in clinical trials.\nResearch Synthesis\nMethods, 15(6):988–1000.\nJianyou Wang, Weili Cao, Longtian Bao, Youze Zheng,\nGil Pasternak, Kaicheng Wang, Xiaoyue Wang, Ra-\nmamohan Paturi, and Leon Bergen. 2025a. Measur-\ning risk of bias in biomedical reports: The RoBBR\nbenchmark. In Proceedings of the 2025 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3220–3248, Suzhou, China. Association\nfor Computational Linguistics.\nYiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren,\nLiyuan Liu, Baolin Peng, Hao Cheng, Xuehai He,\nKuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang\nWang, Simon Shaolei Du, and yelong shen. 2025b.\nReinforcement learning for reasoning in large lan-\nguage models with one training example. In The\nThirty-ninth Annual Conference on Neural Informa-\ntion Processing Systems.\nZhen Wang, Zhifeng Gao, and Guolin Ke. 2025c.\nMasked-and-reordered self-supervision for reinforce-\nment learning from verifiable rewards. arXiv preprint\narXiv:2511.17473.\nXumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu,\nShengyu Ye, Zhirong Wu, and 1 others. 2025. Rein-\nforcement learning with verifiable rewards implicitly\nincentivizes correct reasoning in base llms. arXiv\npreprint arXiv:2506.14245.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 41\nothers. 2025a. Qwen3 technical report. Preprint,\narXiv:2505.09388.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jian-\nhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,\nJingren Zhou, Junyang Lin, Kai Dang, and 23 oth-\ners. 2025b.\nQwen2.5 technical report.\nPreprint,\narXiv:2412.15115.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xi-\naochen Zuo, YuYue, Weinan Dai, Tiantian Fan, Gao-\nhong Liu, Juncai Liu, LingJun Liu, Xin Liu, Haibin\nLin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan\nTong, Chi Zhang, Mofan Zhang, and 17 others. 2025.\nDAPO: An open-source LLM reinforcement learning\nsystem at scale. In The Thirty-ninth Annual Confer-\nence on Neural Information Processing Systems.\nJaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae\nKim, Xiangru Tang, Daniel Shao, Yong Hoe Koo,\nKo Minhyeok, Qingyu Chen, Mark Gerstein, Michael\nMoor, and Jaewoo Kang. 2025. Med-PRM: Medical\nreasoning models with stepwise, guideline-verified\nprocess rewards. In Proceedings of the 2025 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 16565–16582, Suzhou, China. As-\nsociation for Computational Linguistics.\n"}, {"page": 11, "text": "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. STar: Bootstrapping reasoning with rea-\nsoning. In Advances in Neural Information Process-\ning Systems.\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu,\nKeqing He, Zejun MA, and Junxian He. 2025.\nSimpleRL-zoo: Investigating and taming zero rein-\nforcement learning for open base models in the wild.\nIn Second Conference on Language Modeling.\nJixiao Zhang and Chunsheng Zuo. 2025. GRPO-LEAD:\nA difficulty-aware reinforcement learning approach\nfor concise mathematical reasoning in language mod-\nels. In Proceedings of the 2025 Conference on Em-\npirical Methods in Natural Language Processing,\npages 5642–5665, Suzhou, China. Association for\nComputational Linguistics.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen\nZhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jin-\ngren Zhou, and Junyang Lin. 2025. The lessons of\ndeveloping process reward models in mathematical\nreasoning. In Findings of the Association for Compu-\ntational Linguistics: ACL 2025, pages 10495–10516,\nVienna, Austria. Association for Computational Lin-\nguistics.\nJiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen,\nJingrui He, and Mengdi Wang. 2025. Reasonflux-\nPRM: Trajectory-aware PRMs for long chain-of-\nthought reasoning in LLMs.\nIn The Thirty-ninth\nAnnual Conference on Neural Information Process-\ning Systems.\nA\nTheoretical Analysis\nThis section provides the proof of Theorem 1,\nwhich is an extension of prior results on reinforce-\nment learning with verifiable rewards (RLVR) in\nbase LLMs (Wen et al., 2025).\nA.1\nSetup and assumptions\nLet R(Y ) denote the verifiable process reward as-\nsigned to a response Y , and let G be the number\nof responses sampled for the input x. Let C denote\nthe correctness event for a trajectory Y . Define\nµc := E[R(Y ) | C],\nµi := E[R(Y ) | Cc],\np := P(C),\nand let m := pµc + (1 −p)µi be the uncondi-\ntional expected reward.\nAssumption A1 (Verifiability and finite vari-\nance).\nFor fixed (x, r), the verifiable reward\nR(Y ) is a real-valued random variable with finite\nmean and nonnegative variance σY > 0.\nAssumption A2 (Reward gap).\nCorrect reason-\ning trajectories have higher probabilities to induce\ncorrect answers: µc > µi.\nAssumption A3 (Concentration of group statis-\ntics).\nLet R and S denote the empirical mean\nand standard deviation of rewards within a sampled\ngroup. As the group size G →∞,\nR\np−→m,\nS\np−→σ :=\np\nVar[R(Y )].\nA.2\nNormalised advantages\nGRPO uses the trajectory-level normalised advan-\ntage\nˆA(Y ) = R(Y ) −R\nS\n.\nDAPO constructs token-level advantages by scaling\nthe same trajectory advantage:\nˆAi,t = ci,t ˆA(Yi),\nwhere ci,t ≥0 and\n1\nP\ni |Yi|\nP\ni,t ci,t = 1. Follow-\ning Wen et al., 2025 and without loss of generality,\nwe consider a policy gradient update\n∇J(θ) ≈1\nG\nG\nX\ni=1\nˆA(Yi)∇θ log πθ(Yi | x).\nA.3\nProof of Theorem 1\nWe prove the result for ˆA(Y ); the DAPO case fol-\nlows immediately by the nonnegativity of the con-\nstants ci,t.\nBy Assumption A3 and Slutsky’s theorem,\nˆA(Y ) = R(Y ) −R\nS\nd−→R(Y ) −m\nσ\n.\nTaking conditional expectations yields\nE[ ˆA(Y ) | C] G→∞\n−−−−→µc −m\nσ\n,\nE[ ˆA(Y ) | Cc] G→∞\n−−−−→µi −m\nσ\n.\nSubstituting m = pµc + (1 −p)µi gives\nµc −m = (1 −p)(µc −µi) > 0,\nµi −m = −p(µc −µi) < 0,\nestablishing the sign separation:\nE[ ˆA(Y ) | C] > 0,\nE[ ˆA(Y ) | Cc] < 0.\nFor DAPO,\nE[ ˆAi,t | C] = ci,t E[ ˆA(Y ) | C] > 0,\nE[ ˆAi,t | Cc] = ci,t E[ ˆA(Y ) | Cc] < 0.\nThus, both GRPO and DAPO apply positive ex-\npected weight to correct traces and negative weight\nto incorrect ones, proving Theorem 1.\n"}, {"page": 12, "text": "A.4\nVerifiable Outcome Reward as a Special\nCase of VPRMs\nDefine the degenerate label spaces L(r)\nt\n= ∅for\nall t < T. Then rt(·) = 0 for t < T and\nR(Y ; x, r) = rlabel(Y ; x, r).\nHence a Verifiable Outcome Reward Model is ex-\nactly the case of a VPRM with no intermediate\nverifiable labels. All the results above apply: they\nreduce to the original GRPO/DAPO statements\nwhere the scalar reward depends only on final out-\ncome statistics.\nB\nRisk of Bias Assessment\nRisk-of-bias estimation evaluates the extent to\nwhich study findings may be systematically dis-\ntorted. The process is organised into a set of do-\nmains that correspond to common sources of bias\nin randomized trials. For each domain, reviewers\nextract relevant information from the study report\nand translate it into qualitative judgments about\nthe presence and potential impact of bias. Modern\nassessment tools, such as RoB 2.0 (Sterne et al.,\n2019), increasingly leverage automated decision\nrules to standardise these judgments. Algorithm 1\nprovides an example of a macro for risk of type A\nusing the labels defined in this paper, which illus-\ntrates how extracted steps are mapped to specific\nrisk levels. Below we outline the main domains\nconsidered in our work, together with the typical\nreasoning steps involved.\nA. Random sequence generation\nThis domain\nassesses whether the method used to generate the\nallocation sequence was truly random. Reviewers\nfirst check whether the study reports how random-\nization was carried out. If so, they evaluate the\nnature of the method (e.g., computer-generated se-\nquence versus quasi-random methods such as alter-\nnation) and judge whether the sequence could have\nbeen predicted. Clearly reported and genuinely ran-\ndom procedures indicate low risk; quasi-random or\nnon-random procedures, or a lack of information,\nincrease concern.\nB. Allocation concealment\nHere the question is\nwhether the assignment to treatment groups was\nshielded from those enrolling participants. Review-\ners determine whether concealment was reported\nand whether the method (e.g., sealed opaque en-\nvelopes, central allocation) prevented foreknowl-\nedge of upcoming assignments. Adequate con-\ncealment protects against selection bias, whereas\ninadequate or unclear procedures raise concerns.\nC. Blinding of participants and personnel\nThis\ndomain considers whether participants and those\nadministering interventions were aware of group\nassignments. Reviewers establish whether blind-\ning was reported, whether it involved participants,\npersonnel, or both, and whether the blinding ap-\nproach was likely to have been effective. Lack of\nblinding, or ineffective procedures, may influence\nparticipants’ behaviour or care delivery and thus\nintroduce performance bias.\nD. Blinding of outcome assessment\nAssessors\nmay also be influenced by knowledge of treatment\nallocation. Reviewers check whether outcome as-\nsessors were blinded and whether blinding was\nlikely to minimise biased measurement. Absence\nof blinding or unclear reporting raises the possibil-\nity that assessments were influenced by expecta-\ntions or prior beliefs.\nE. Incomplete outcome data\nThis domain eval-\nuates the extent and handling of missing data.\nReviewers consider how much data is missing,\nwhether reasons for missingness are reported and\nplausible, and whether the analysis appropriately\naccounts for missing data. High or unexplained\nattrition, or inadequate handling strategies, can pro-\nduce biased estimates of effect.\nF. Selective reporting\nSelective reporting bias\narises when outcomes are reported inconsistently\nwith the study protocol or when unplanned out-\ncomes are introduced. Reviewers check whether\na protocol is available, compare planned and re-\nported outcomes, and assess whether omissions or\nadditions suggest selective emphasis. Clear corre-\nspondence indicates low risk; discrepancies raise\nconcern.\nIn addition to these core domains, we also consider\nsupplementary aspects relevant to internal validity:\nsimilarity of baseline outcomes (G), similarity of\nbaseline characteristics (H), and risk of contamina-\ntion between study arms (I). These domains capture\nfurther sources of potential bias arising from imbal-\nances at baseline or from unintended exposure to\ninterventions across groups.\nC\nPrompts\nThe prompts used for synthetic data annotation and\nfor training are shown in Figure 4 and 6, respec-\n"}, {"page": 13, "text": "Algorithm 1 RoB A Macro\n1: procedure PREDICTLABEL-A(steps)\n2:\nif steps[IDENTIFYRANDOMIZATIONREPORT] = NOTREPORTED then\n3:\nreturn MODERATE\n4:\nend if\n5:\nif steps[CLASSIFYRANDOMIZATIONMETHOD] = NONRANDOM then\n6:\nreturn HIGH\n7:\nend if\n8:\nif steps[ASSESSSEQUENCEPREDICTABILITY] = PREDICTABLE then\n9:\nreturn MODERATE\n10:\nend if\n11:\nif steps[BASELINEIMBALANCE] = LIKELY then\n12:\nreturn HIGH\n13:\nend if\n14:\nreturn LOW\n15: end procedure\nDataset\nA\nB\nC\nD\nE\nF\nG\nH\nI\nCOCHRANEFORESTEXT\n498\n498\n498\n498\n498\n273\n61\n61\n61\nCOCHRANEFOREST\n330\n330\n330\n330\n330\n112\n28\n28\n28\nRoBBR Cochrane\n125\n125\n198\n133\n206\n119\n0\n0\n0\nRoBBR Non-Cochrane\n412\n474\n467\n472\n478\n186\n0\n0\n0\nTable 6: Datasets statistics per risk type.\ntively. For training, a temperature of 0.7 and 2,048\ntokens as maximum output length are used.\nD\nSilver Steps and Labels Manual\nVerification\nTo assess the quality of the automatically generated\nreasoning steps and silver labels used for VPRM\ntraining, we selected a random sample of 20 in-\nstances from the full dataset and manually eval-\nuated their correctness. The evaluation was con-\nducted by two master’s students in NLP familiar\nwith the task. For each instance, annotators in-\nspected the complete step-level reasoning trace and\nverified two properties: (i) whether the sequence\nof steps constituted a valid decision path for the\ntarget risk-of-bias domain, according to Cochrane’s\ndomain-specific guidelines; and (ii) whether each\nstep and label was valid given the underlying paper.\nFor each item, we recorded whether the overall\nreasoning trace was coherent, and for each individ-\nual step we recorded whether the step and its label\nwere valid.\nMetric\nFraction\nCoherent instances\n100.0 %\nCorrect steps\n100.0 %\nCorrect labels\n96.7%\nTable 7: Manual verification of 20 randomly sampled\nsilver-labelled reasoning traces.\nTable 7 summarises the proportion of coherent\ninstances, correct steps, and correct labels observed\nin this manual evaluation. Results of the manual\nverification show that the automatically generated\nPrompt for synthetic data annotation\nArticles: {articles}\nYour task is to produce a structured reasoning trace for the\nfollowing risk of bias domain to justify the ground truth value.\nComparison: {comparison}\nOutcome: {outcome}\nBias: {bias_id} – {bias_definition}\nGround_truth: {bias_value}\nYou must follow the structured reasoning procedure defined\nfor each risk-of-bias domain (A–I). For every domain, you must use\nthe exact step names and allowable categorical labels listed below:\n{steps_and_labels}\nFollow this exact output structure:\n{{\n\"step_name\": \"step_label\",\n\"step_name_rationale\": \"your detailed rationale\",\n...\n(repeat for all steps required by the bias domain)\n}}\nFigure 4: Prompt for synthetic data annotation.\nSteps and labels\nA — Random sequence generation\nIdentify_randomization_report →reported | not_reported\nClassify_randomization_method →random | non_random\nAssess_sequence_predictability →unpredictable | predictable\nBaseline_imbalance →likely | none\nB — Allocation concealment\nIdentify_concealment_report →reported | not_reported\nDetermine_concealment_method →adequate | inadequate\nAssess_possibility_of_foreknowledge →no | possible\nC — Blinding of participants and personnel\nIdentify_blinding_report →reported | not_reported\nAssess_blinding_status →participants | personnel | both | none\nEvaluate_blinding_effectiveness →effective | ineffective\nD — Blinding of outcome assessment\nIdentify_outcome_blinding_report →reported | not_reported\nAssess_assessor_blinding →yes | no\nEvaluate_blinding_effect_on_measurement →no | possible\nE — Incomplete outcome data\nQuantify_missing_data →none | low | high\nIdentify_missing_data_reason\n→\nadequate\n|\ninadequate\n|\nnot_reported\nAssess_handling_of_missing_data →appropriate | inappropriate\nEstimate_bias_due_to_missing_data →unlikely | likely\nF — Selective reporting\nIdentify_protocol_availability →available | not_available\nCompare_outcomes_reported →all | partial | none\nDetect_unexpected_outcomes →none | added\nEvaluate_reporting_selectivity →no | possible | yes\nG — Baseline outcomes similar\nIdentify_baseline_outcomes_report →reported | not_reported\nCompare_baseline_outcomes →similar | different\nEvaluate_impact_of_differences →likely_impact | unlikely_impact\nH — Baseline characteristics similar\nIdentify_baseline_characteristics_report →reported | not_reported\nCompare_baseline_characteristics →similar | different\nEvaluate_impact_of_differences →likely_impact | unlikely_impact\nI — Contamination\nIdentify_contamination_risk_report →reported | not_reported\nAssess_contamination_possibility →possible | unlikely\nAssess_contamination_impact →likely_impact | unlikely_impact\nFigure 5: Steps and labels.\n"}, {"page": 14, "text": "Prompt for training and inference\nArticles: {articles}\nQuestion:\nBased on the given article, what is the risk of\nbias for the following Comparison and Outcome?\nComparison: {comparison}\nOutcome: {outcome}\nThe bias you have to assess is defined as follows: {bias}\nYou must follow the structured reasoning procedure defined for each\nrisk-of-bias domain (A–I). For every domain, you must use the exact\nstep names and allowable categorical labels listed below:\n{steps_and_labels}\nFollow this exact structure for your reasoning:\n<think>\nStep 1: step_name\n...your thought process here...\nAnswer: step_label\nStep 2: step_name\n...your thought process here...\nAnswer: step_label\n(repeat for all steps required by the bias domain)\n</think>\n<answer>\nrisk: high | low | moderate\n</answer>\nFigure 6: Prompt for VPRM-training and -inference.\nsteps and labels used for VPRM training are of\nconsistently high quality. All inspected traces fol-\nlow the correct decision structure, and step-level\nlabels are almost always accurate, providing a solid\nground for model training.\nE\nRelated Work\nReinforcement Learning and Verifiable Re-\nwards\nSeveral extensions of reinforcement learn-\ning with verifiable rewards (RLVR) go beyond\noutcome-only supervision by enriching the reward\nsignal with structural information. Masked-and-\nreordered self-supervision provides auxiliary sig-\nnals encouraging coherent intermediate reason-\ning (Wang et al., 2025c), while self-verification\nmethods add progress-estimation or critique mod-\nules that guide models toward more reliable rea-\nsoning trajectories (Zeng et al., 2025). Theoretical\nanalyses further show that verifiable rewards influ-\nence trajectory selection in predictable ways, steer-\ning models toward high-success modes under veri-\nfiable criteria (Wen et al., 2025). These approaches\nstrengthen RLVR but remain fundamentally cen-\ntered on terminal-outcome verification.\nComplementary lines of research pursue pro-\ncess supervision, scoring CoT steps using neural\njudges (Lightman et al., 2024; Zelikman et al.,\n2022; Zhang et al., 2025; Zou et al., 2025). While\nsuch methods provide dense feedback unavail-\nable to outcome-only RL, they depend on model-\ngenerated evaluations and therefore inherit issues\nof opacity, bias, and reward hacking (Amodei et al.,\n2016; Skalse et al., 2022). Crucially, their interme-\ndiate rewards are not verifiable.\nTaken together, these works highlight two re-\nmaining gaps: existing approaches lack (i) verifia-\nbility of intermediate rewards and (ii) fine-grained\nstep-level supervision grounded in deterministic\nrules.\nTo date, no method provides reinforce-\nment learning over reasoning trajectories where\nevery step is evaluated by an externally check-\nable verifier. Verifiable Process Reward Models\n(VPRMs) address this gap by combining the ro-\nbustness of RLVR with step-wise, rule-based veri-\nfication, which not only enables transparent, struc-\nturally aligned reasoning but also removes the op-\nportunities for reward hacking inherent in neural\nprocess rewards.\nRoB Assessment and Automated Evidence Eval-\nuation.\nPrior work on automated RoB assess-\nment has largely relied on supervised modelling or\nprompted LLMs. Transformer-based systems such\nas RoBIn (Dias et al., 2025) frame RoB inference\nas a machine reading comprehension task and train\nclassifiers directly on annotated evidence. Other ap-\nproaches enhance pretrained LLMs with retrieval\nor auxiliary decision heads, as in RoBGuard (Ji\net al., 2025).\nSeveral studies investigate LLM\nprompting for RoB assessment, reporting limited\nreliability when models operate without explicit\nprocedural constraints (Huang et al., 2025; Šuster\net al., 2024). Likewise, analyses of LLM-based\ncritical appraisal highlight dependence on model\npretraining and prompt sensitivity rather than veri-\nfiable optimisation (Wang et al., 2025a; Lai et al.,\n2025). Across these methodologies, existing sys-\ntems employ prompting or supervised fine-tuning,\nbut none leverage reinforcement learning for RoB\nassessment. Our work is, to our knowledge, the\nfirst to introduce RL-based training in this domain.\nF\nHyperparameters and APIs\nWe executed all the experiments either via API or\non our own cluster. We used the paid-for OpenAI\nAPI to access GPT-4. On the other hand, we hosted\nand trained the open-source models used in this\npaper on a distributed cluster.\nSFT is performed for 5 epochs with a batch size\nof 1 (due to the large size of the input data) us-\ning a learning rate of 5 × 10−5 and the AdamW\n"}, {"page": 15, "text": "optimiser (Loshchilov and Hutter, 2017).\nFor the RL setups, we adopt the GRPO (Shao\net al., 2024) and DAPO (Yu et al., 2025) algo-\nrithms, training for 3 epochs with a learning rate\nof 1 × 10−6, per-device batch size 1, and 16 sam-\npled generations per batch. Both training protocols\nleverage gradient accumulation with 8 accumula-\ntion steps. All experiments are conducted using the\nOpen-R1 framework (Hugging Face, 2025) on 8\nNVIDIA A100 GPUs, each equipped with 80GB\nof memory. Models have been served for inference\nwith the vLLM framework (Kwon et al., 2023).\nG\nScientific Artefacts and Licensing\nIn this work, we used the following scientific arte-\nfacts. LLaMa 3.1 is licensed under a commercial\nlicense1. GPT-4 is licensed under a commercial li-\ncense2. Qwen2.5 is licensed under the Apache 2.0\nlicense3. Granite 3.1 is licensed under the Apache\n2.0 license4. DeepSeek models are licensed under\nthe MIT license5. Mining text and data from the\nCochrane library is permitted for non-commercial\nresearch through the Wiley API.6. The usage of the\nlisted artefacts is consistent with their licenses.\n1https://llama.meta.com/doc/overview\n2https://openai.com/policies/terms-of-use\n3https://qwenlm.github.io/blog/qwen3\n4https://www.ibm.com/architectures/\nproduct-guides/granite-31\n5https://api-docs.deepseek.com/news/\nnews250120\n6https://www.cochranelibrary.com/help/access\n"}, {"page": 16, "text": "Figure 7: Example of forest plot containing three studies and their corresponding risk of bias map.\nHawkey 2015\nText\nAutologous Hematopoetic Stem Cell Transplantation for Refractory Crohn Disease: A\nRandomised Clinical Trial\nImportance Case reports and series suggest hematopoietic stem cell transplantation\n(HSCT) may benefit some patients with Crohn disease.\n...\nOutcome\nClinical Remission\nComparison\nStem Cell vs Placebo or control\nBias type\nA\nBias name\nRandom sequence generation (selection bias)\nBias definition\nBias arising from the randomization process\nBias value\n+\nAnnotation\n\"Identify_randomization_report\": \"reported\",\n\"Identify_randomization_report_rationale\": \"The methods section of the trial explicitly\nstates that participants were allocated to groups using a computer-generated random\nsequence. The authors describe the randomisation process in sufficient detail, indicating\nthat a statistician who was not involved in participant recruitment generated the\nsequence.\",\n\"Classify_randomization_method\": \"random\",\n\"Classify_randomization_method_rationale\": \"A computer-generated sequence (e.g.,\nusing a random number generator or statistical software) is a truly random method. It does\nnot rely on any deterministic or quasi-random technique such as alternation, date of birth,\nor admission order, therefore it is classified as ’random’.\",\n\"Assess_sequence_predictability\": \"unpredictable\",\n\"Assess_sequence_predictability_rationale\": \"Because the sequence was generated by a\ncomputer algorithm and concealed from the investigators enrolling participants (allocation\nwas performed via sealed, opaque envelopes prepared after the sequence was generated),\nneither the investigators nor the participants could foresee the upcoming allocation. This\nmakes the sequence unpredictable.\",\n\"Baseline_Imbalance\": \"none\",\n\"Baseline_Imbalance_rationale\": \"Baseline tables show that demographic and clinical\ncharacteristics (age, sex, disease severity, etc.) are comparable across groups with no\nstatistically significant differences, indicating that the randomisation produced balanced\ngroups and is unlikely to have introduced bias.\"\nTable 8: The corresponding entry in COCHRANEFORESTEXT for Hawkey 2015 in Figure 7. Rationales are not used\nfor RL training.\n"}]}