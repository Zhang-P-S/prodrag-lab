{"doc_id": "arxiv:2512.08193", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.08193.pdf", "meta": {"doc_id": "arxiv:2512.08193", "source": "arxiv", "arxiv_id": "2512.08193", "title": "ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access", "authors": ["Jiwoo Park", "Ruoqi Liu", "Avani Jagdale", "Andrew Srisuwananukorn", "Jing Zhao", "Lang Li", "Ping Zhang", "Sachin Kumar"], "published": "2025-12-09T02:52:06Z", "updated": "2025-12-09T02:52:06Z", "summary": "We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.08193v1", "url_pdf": "https://arxiv.org/pdf/2512.08193.pdf", "meta_path": "data/raw/arxiv/meta/2512.08193.json", "sha256": "b123c377591d7ff2dd4c45d4148b8e4bbe8f2f03ecceeb648923adf7651065a7", "status": "ok", "fetched_at": "2026-02-18T02:24:37.821450+00:00"}, "pages": [{"page": 1, "text": "CLINICALTRIALSHUB: Bridging Registries and Literature for\nComprehensive Clinical Trial Access\nJiwoo Park\nRuoqi Liu\nAvani Jagdale\nAndrew Srisuwananukorn\nJing Zhao\nLang Li\nPing Zhang\nSachin Kumar\nThe Ohio State University, Columbus OH\npark.3620@osu.edu\nAbstract\nWe present CLINICALTRIALSHUB, an inter-\nactive search-focused platform that consoli-\ndates all data from ClinicalTrials.gov and aug-\nments it by automatically extracting and struc-\nturing trial-relevant information from PubMed\nresearch articles. Our system effectively in-\ncreases access to structured clinical trial data\nby 83.8% compared to relying on Clinical-\nTrials.gov alone, with potential to make ac-\ncess easier for patients, clinicians, researchers,\nand policymakers, advancing evidence-based\nmedicine. CLINICALTRIALSHUB uses large\nlanguage models such as GPT-5.1 and Gemini-\n3-Pro to enhance accessibility. The platform\nautomatically parses full-text research articles\nto extract structured trial information, translates\nuser queries into structured database searches,\nand provides an attributed question-answering\nsystem that generates evidence-grounded an-\nswers linked to specific source sentences. We\ndemonstrate its utility through (1) a user study\ninvolving clinicians, clinical researchers, and\nPhD students of pharmaceutical sciences and\nnursing, and (2) a systematic automatic evalua-\ntion of its information extraction and question\nanswering capabilities.1\n1\nIntroduction\nAccess to clinical trial information is essential for\npatients seeking new treatments and for clinicians,\nresearchers, and policymakers working to advance\nmedical care. Containing over 500K registrations,\nClinicalTrials.gov (CTG) is the primary resource\nmany of these groups use to search and identify\nongoing and completed trials. All trials in CTG\n1The\ndemonstration\nis\nhosted\nat:\nhttps:\n//cth-frontend-103266204202.us-central1.run.app/.\nA\ndemonstration\nvideo\nis\navailable\nat:\nhttps:\n//www.youtube.com/watch?v=uCPxyw7Abh0.\nThe\nsource code to run the demo locally is available at:\nhttps://github.com/jiwoo-jus/clinical-trials-hub.\nEvaluation\ncode\nfor\ncomparing\nmodel\nperformance\nis\navailable\nat:\nhttps://github.com/jiwoo-jus/\nclinical-trials-hub-evaluation.\nDatabase\nFilter/Status\nCount\nCTG\nAll Studies\n543,172\nCTG\nCompleted w/ Results\n60,449\nPubMed\nMax Sensitivitya\n962,774\nPubMed\nMax Specificitya\n138,279\nPubMed\nMax Sensitivity + CTG linkedb\n63,928\nPubMed\nMax Specificity + CTG linkedb\n35,671\nTable 1: Clinical trial data coverage comparison. aOpen-\naccess trials identified using PubMed Clinical Study\nCategories filters from Haynes et al. (Wilczynski et al.,\n2011), with PMC Open Access restriction. bSubset of\ntrials already registered on ClinicalTrials.gov.\nare available in a structured format allowing the\nusers to easily navigate, filter, or download them.\nHowever, many trials remain unregistered or are\nreported only in publications, particularly those\nconducted outside the US. PubMed, which indexes\nover 35 million biomedical papers and abstracts,\noften includes results from these unregistered trials.\nAs shown in Table 1, a substantial number of trials\naccessible from PubMed are not registered on CTG.\nThe difference in coverage reflects the lack of inte-\ngration between trial registries and published litera-\nture. Despite both being critical resources, PubMed\nand CTG exist as isolated silos with incompatible\nformats. PubMed, containing free-text articles, can\nbe especially difficult for non-researchers to parse\nor filter, requiring substantial manual effort that\ncould take weeks or months.\nTo address this gap, we present CLINICALTRI-\nALSHUB, a unified user-centered trial search plat-\nform that combines trial registry data from CTG\nand published literature from PubMed into a sin-\ngle interface (§2). Our system makes three main\ncontributions. First, our interface accepts natural\nlanguage queries and displays unified search re-\nsults combining trials from both sources in a single\nranked list, merging overlapping trials (§3). Sec-\n1\narXiv:2512.08193v1  [cs.CL]  9 Dec 2025\n"}, {"page": 2, "text": "ond, using large language models (LLMs), we\nextract structured information from free text\nPubMed articles making up to 899,846 previously\nunregistered clinical trials easily searchable and\nfilterable (§4). Lastly, we build an attributed ques-\ntion answering feature that allows the users to ask\nquestions over individual trials (§5); answers are\ngenerated with attribution provided in the trial text.\nOur system showcases practical use cases span-\nning diverse medical contexts, from evidence cu-\nration and systematic reviews to dosing protocols\nand treatment evaluation. We validate the system’s\nutility through comprehensive evaluations compar-\ning frontier LLMs (Gemini-3-Pro (Gemini Team,\nGoogle, 2025), GPT-5.1 (OpenAI, 2025), Claude-\n4.5-Sonnet (Anthropic, 2025)) to select optimal\nmodels for each feature.\nWe conduct quantita-\ntive benchmarking to assess information extraction\naccuracy using a curated benchmark and evalu-\nate question-answering quality using the FACTS\ngrounding benchmark (Jacovi et al., 2025). Addi-\ntionally, we perform a user study with seven med-\nical professionals. The evaluations confirm the\nplatform’s effectiveness across its core functionali-\nties and showcase its utility over using PubMed or\nCTG alone.\n2\nCLINICALTRIALSHUB UI Design\nThis section describes the user interface of our sys-\ntem. The platform consists of two primary pages: a\nmain search page and individual trial detail pages.\nMain Search Page.\nThe main search interface\n(an overview is shown in Figure 1; additional\nscreenshots are available in Appendix G) offers\na unified search experience. In search bar panel,\nusers can enter natural language queries (top) or use\nstructured input forms specifying condition, inter-\nvention, and other terms for PICO-based search\n(middle), or utilize expert-level PubMed/CTG\nquery forms (bottom). The search results display\na ranked list of trials from both CTG and PubMed\nsources, with merged entries when a trial appears\nin both databases. A filtering sidebar enables\nrefinement by data source, study type, phase, de-\nsign allocation, and other trial-specific categories.\nThe details sidebar provides a quick preview of\nabstracts and metadata when clicking trial titles.\nCLINICALTRIALSHUB also provides two experi-\nmental features to enhance the search experience.\nUsers can specify inclusion and exclusion criteria\nfor early-stage screening; when viewing a trial’s\npreview sidebar, the system provides a quick eligi-\nbility assessment based on the abstract and meta-\ndata. Additionally, an AI-insight feature generates\nfive context-aware insights based on the current\nsearch results, supporting iterative Q&A by main-\ntaining context from the user’s queries and recent\ninteractions.\nIndividual Trial Detail Page.\nUpon selecting a\ntrial, users access a comprehensive detail page (an\noverview is shown in Figure 2; additional screen-\nshots are provided in Appendix H) with three main\ncomponents evaluated in our studies: Structured\nInformation displays trial metadata in an easy-\nto-navigate format. This information is directly\navailable for registered trials on CTG, while for\nPubMed-only items it is extracted from free-text\narticles; Full Text shows the entire publication\ntext for PubMed entries. For CTG entries, refer-\nenced PMC articles are listed, and these can then\nbe clicked and expanded to full text as well. In-\nteractive QA allows users to ask questions about\nthe trial, with answers generated by an LLM fed\nPMC articles or CTG structured data depending on\nthe item, and generates evidence-based responses.\nWhen users click the evidence, it auto-scrolls and\nhighlights upon clicking a citation for verification.\n3\nCLINICALTRIALSHUB Search\nOur search system comprises three components:\n(1) query refinement that converts natural language\nqueries to platform-specific structured queries, (2)\nmulti-source search on the two platforms, (3) rele-\nvance reranking and deduplication. Figure 3 illus-\ntrates the overall pipeline.\n3.1\nQuery Refinement\nThe query refinement component translates user\nqueries into structured search parameters.\nOur\nimplementation handles both structured field in-\nputs and natural language queries. When users\nprovide individual field values (condition, interven-\ntion, other terms), we use them directly. For natural\nlanguage queries, we use an LLM (GPT-5.1) that\napplies clinical terminology normalization to de-\ncompose the required fields. We design prompt\ntemplates to extract condition terms, intervention\nspecifications, and auxiliary keywords. The prompt\nused to extract these meanings from user queries is\nshown in Appendix I.\nThe extracted output maps directly to database-\nspecific query construction,\nwhere PubMed\n2\n"}, {"page": 3, "text": "Figure 1: Main search interface with unified results, filters, and eligibility preview\nsearches combine the structured parameters with\nBoolean operators and predefined clinical trial de-\ntection patterns, while CTG requires the decom-\nposed fields as separate API parameters. This ap-\nproach maintains consistent query semantics across\nheterogeneous database interfaces while preserving\nthe original search intent.\n3.2\nMulti-Source Search\nBoth CTG and PubMed offer their own search APIs.\nGive the refined query, we execute both APIs asyn-\nchronously with the following platform specific\noptimization strategies.\nPubMed\nemploys a two-stage retrieval process\nusing NCBI’s E-utilities API.\n• E-SEARCH: executes optimized Boolean queries\nagainst PubMed’s index to extract relevant\nPMIDs filtering for journal restrictions, demo-\ngraphic constraints, and condition-specific query\nenhancement.\n• E-FETCH: performs batch metadata retrieval for\ncomprehensive bibliographic information, includ-\ning structured abstracts, MeSH terms, chemical\nsubstances, grant information, and clinical trial\ncross-references.\nCTG Integration\nutilizes a dual-source ap-\nproach for comprehensive trial data access:\n• Primary API: Retrieves basic trial information\nand NCT identifiers via the official ClinicalTri-\nals.gov REST API with support for complex fil-\ntering, including study type, phase, enrollment\ncriteria, and geographical constraints.\n• AACT Database: Leverages the daily-updated\nPostgreSQL mirror of ClinicalTrials.gov for en-\nhanced metadata extraction, detailed outcome\nmeasures, and complex relational queries not sup-\nported by the primary API.\n3.3\nReranking and Result Integration\nWe apply BM25 scoring (Robertson and Walker,\n1994) separately to each source, normalize scores,\nand add bonuses based on the similarity ranking\nwithin each source.\nWe bidirectionally merge\nPubMed and CTG entries that reference each other,\nproviding a relevance bonus to merged pairs. All\nentries are then sorted by final scores and displayed\nto users. Complete technical details including cor-\npus construction, scoring formulas, and deduplica-\ntion algorithms are provided in Appendix B.\n4\nInformation Extraction\nAll entries on CTG follow a structured format that\nallows users to navigate and filter across different\ntrials as well as download them for analyses. For\nexample, a patient looking for phase 3 trials for\n3\n"}, {"page": 4, "text": "Figure 2: Trial detail page with structured information and evidence-grounded QA\nUser Query\nPM Query\nCTG Query\nQuery \nGeneration\nBM25 Scoring\nMetadata \nExtraction\nBM25 Scoring\nMetadata \nPreparation\nNormalization & Merging\nClinical Trials Hub\nPubMed API\nClinicalTrials.gov \nAPI\nFigure 3: Search pipeline\ntheir gender and age range can easily apply a filter\nand find relevant trials. On the other hand, PubMed\narticles are research papers in free text form which\nmakes it harder to perform such tasks. Hence, we\nimplement an information extraction pipeline that\nextracts all relevant trial-related data from papers\nin the same format as CTG, making indexing, fil-\ntering, browsing, and analysis through them easier.\nWe employ an LLM for this purpose that takes as\ninput (parts of) the paper and generates the struc-\ntured output in a JSON format.\nWe perform information extraction for 211 fields\nin parallel by dividing the task into six modules\nfor the protocol section, four modules for the re-\nsults section, and one module for derived section,\neach with explicit field definitions and enumerated\nvalue constraints constructed based on CTG data\nschema2 fed to the LLM (see fields list at Table 14\nin Appendix C). This approach minimizes both\nuser wait times and hallucinations caused by the\nLLM processing overwhelming amounts of infor-\nmation. We describe them in detail in Appendix C.\nThe prompt templates are provided in Appendix J\n4.1\nEvaluation\nWe evaluated three frontier LLMs (Gemini-3-Pro,\nGPT-5.1, and Claude-4.5-Sonnet) to measure struc-\n2https://beta-ut.clinicaltrials.gov/api/v2/\nstudies/metadata\n4\n"}, {"page": 5, "text": "tural completeness (key-level: whether the field ex-\nists) and semantic accuracy (value-level: whether\nthe extracted value matches the ground truth).\nDataset construction\nWe create an evaluation\ndataset comprising 100 PMC-CTG trial pairs. To\ndo this, we queried PubMed for clinical trials (sub-\nsection C.3) published after 2021 (the current data\nschema’s modernization point) with PMC full-text\navailability and CTG status set to completed. We\nthen check for a one-to-one mapping verified bidi-\nrectionally through the PMC and CTG databases.\nWe focus on the 21 distinct field types in protocol\nsection covering study design, arm group interven-\ntions, outcome measures, and eligibility criteria.\nWe excluded CTG fields appearing in fewer than\n15% of records and fields primarily found in PMC\narticles but inconsistently updated on CTG, con-\nsidering them unsuitable as reliable ground truth.\nSince results reporting has been required, limited\nenforcement has led to results information being\nmore complete in PMC articles, while CTG entries\nmay be missing or outdated. Conversely, CTG of-\nten lists extensive secondary outcomes and adverse\nevents, whereas PMC articles emphasize primary\noutcomes. We plan to create a systematically vali-\ndated benchmark dataset covering this results sec-\ntion as our future work. Model predictions and\nground truth data are available here.\nFor key-level evaluation, we flatten all fields\nfrom both model prediction and reference, preserv-\ning hierarchical relationships. For each field in the\nschema, we classify its extraction status as: True\nPositive (field exists in both prediction and refer-\nence), False Positive (field appears in prediction\nbut not defined in the CTG data schema), False\nNegative (field exists in reference but missing from\nprediction), or Extra Valid (field appears in pre-\ndiction and is valid according to the schema, but\nabsent from this specific reference case, making it\nunevaluable for correctness).\nFor value-level evaluation, for fields with True\nPositive status, we assess extracted values using\na three-stage approach.\nFirst, we perform ex-\nact string matching with normalization. For mis-\nmatches, we calculate semantic similarity using\nGPT-5.1 as judge with a 0.70 threshold (prompt\nshown in subsection C.4). We choose this threshold\nqualitatively by manually analyzing model outputs.\nFor structured list fields, we create an m × n simi-\nlarity matrix where m is the number of reference\nelements and n is the number of predicted elements\nto score all reference-prediction element pairs, then\nuse the Hungarian algorithm (Kuhn, 1955) to opti-\nmize total similarity across matched pairs, allowing\naccurate per-element evaluation regardless of list\norder.\nResults\nTable Table 2 summarizes the overall\nperformance. GPT-5.1 achieved the highest per-\nformance across all metrics, recording a Key-\nlevel F1 of 0.980 and a Value-level F1 of 0.890.\nGemini-3-Pro exhibited competitive performance\n(Key F1 0.960, Value F1 0.870), whereas Claude-\n4.5-Sonnet showed a clear performance gap (Key\nF1 0.740, Value F1 0.660). However, we note that\nthe differences between GPT-5.1 and Gemini-3-Pro\nare small across both key- and value-level metrics.\nBecause we used GPT-5.1 as the semantic similar-\nity judgen, a small bias in its favor is possible. To\nmitigate this, we manually reviewed some of the\ndisagreement cases and confirmed that the judged\nmatches were generally valid. Given the overall\ncloseness of model performance, our results sug-\ngest that either GPT-5.1 or Gemini-3-Pro could\nserve as a reasonable backbone model in practice.\nWe select GPT-5.1 for our system primarily for cost-\nefficiency and practical deployment considerations\nrather than because of any substantial performance\ngap.\nA more fine-grained view is presented in Ta-\nble 13, which reports value-level F1 scores for each\nindividual field alongside the average number of\nExtra-Valid (EV) cases. Although EV cases cannot\nbe directly evaluated due to the lack of a refer-\nence value, they nevertheless reveal meaningful\npatterns about the underlying data sources. Sev-\neral descriptive and eligibility-related fields exhibit\nhigh EV frequencies, indicating that PMC articles\noften contain information that is absent in CTG.\nImportantly, these same fields also show high F1\nscores when CTG does provide a reference value,\nsuggesting that discrepancies arise primarily from\nCTG incompleteness rather than model hallucina-\ntion. This pattern reinforces the complementary\nvalue of LLM-based extraction when dealing with\nfields that are inconsistently maintained or only\npartially populated in CTG.\n5\nInteractive Grounded QA\nOn the trial details page, we provide an LLM-\npowered chat interface that allows the user to\nask question about individual trials with evidence-\ngrounded responses. To select the optimal model\n5\n"}, {"page": 6, "text": "Level\nModel\nPrecision\nRecall\nF1\nKey\nGemini-3-Pro\n1.000\n0.930\n0.960\nGPT-5.1\n1.000\n0.970\n0.980\nClaude-4.5-Sonnet\n0.900\n0.690\n0.740\nValue\nGemini-3-Pro\n0.830\n0.910\n0.870\nGPT-5.1\n0.840\n0.960\n0.890\nClaude-4.5-Sonnet\n0.740\n0.670\n0.660\nTable 2: Information extraction performance across key-\nlevel and value-level metrics.\nfor this component, we evaluated three fron-\ntier LLMs (Gemini-3-Pro, GPT-5.1, Claude-4.5-\nSonnet).\nBenchmark Selection\nFor this evaluation, we\nused the FACTS Grounding benchmark (Jacovi\net al., 2025) , which measures a model’s ability to\ngenerate responses factually grounded in provided\ncontext documents. The benchmark aligns directly\nwith our requirements by incorporating substantial\nmedical-domain coverage (29%), supporting long\ncontext documents up to 32K tokens with diverse\nquery patterns, and explicitly constraining models\nto rely solely on the provided context.\nEvaluation Methodology\nFACTS employs a\ntwo-phase evaluation conducted by three judge\nmodels(Gemini-1.5-Pro, GPT-4o, and Claude-3.5-\nSonnet). 1. Instruction Following (Eligibility):\nJudges assess whether responses adequately ad-\ndress the user request, assigning verdicts of No\nissues, Minor issue(s), or Major issue(s). A re-\nsponse is deemed ineligible (and assigned a score\nof 0) if all three judges classify it as having major\nissues, preventing it from advancing to the ground-\ning evaluation phase. 2. Grounding: Judges evalu-\nate whether each response is fully grounded in the\nsource document by classifying every sentence as\nsupported, unsupported, contradictory, or no_rad\n(requires no factual grounding). A response re-\nceives a positive grounding verdict from a judge\nonly if all sentences are either supported or no_rad.\nThe factuality score for each response is the aver-\nage of the three judges’ verdicts, and each model’s\nfinal score is the mean factuality score across all\nevaluated responses.\nEvaluation setup\nWe evaluated Gemini-3-Pro,\nGPT-5.1, and Claude-4.5-Sonnet on 236 medical-\ndomain samples from FACTS, generating all re-\nsponses but evaluating the top 100 shortest average\nresponse length to balance cost and significance\nfollowing FACTS guideline.\nModel\nJudge\nAverage\nGemini\nGPT\nClaude\nGemini-3-Pro\n0.897\n0.920\n0.830\n0.930\nGPT-5.1\n0.680\n0.670\n0.610\n0.760\nClaude-4.5-Sonnet\n0.767\n0.740\n0.690\n0.860\nTable 3: Evaluation scores by judge model. Each row\nshows a prediction model evaluated by three differ-\nent judge models (Gemini-3-Pro, GPT-5.1, Claude-4.5-\nSonnet) and their average score.\nFor grounding evaluation, We used the JSON\nprompt template from the available templates,\nwhich FACTS identified as optimal for Gemini-\n1.5-Pro and GPT-4o. This JSON template instructs\nthe model to output structured JSON objects for\neach sentence, with classification labels, explicit\nrationales explaining each decision, and supporting\nevidence excerpts from the context document.\nIn contrast, the implicit-span-level prompt tem-\nplate generates unstructured natural language out-\nput, listing each sentence with a simple binary ac-\ncurate/inaccurate label and concluding with a final\nverdict, without providing detailed rationales or\nsupporting evidence. In FACTS, this approach had\npreviously been preferred for Claude-3.5-Sonnet\ndue to its lower complexity in structured output\ngeneration. However, Claude-4.5-Sonnet exhibited\nsubstantially improved JSON generation reliability,\nallowing us to use the more informative template\nformat. (see Table 15)\nResults\nGemini-3-Pro achieved the highest\ngrounding score (0.897), notably outperforming\nGPT-5.1 (0.680) and Claude-4.5-Sonnet (0.767),\nwith consistently stronger performance across all\nthree evaluators (see Table 3). We additionally\nrecorded completion tokens, response time, and\nresponse length for all generated responses. As\nshown in Table 4, while Gemini-3-Pro required sig-\nnificantly higher completion tokens (1,318 avg. vs.\n∼220 for others) and generation time (14.33s vs.\n∼4s), it produced the most concise final answers\n(698 characters vs. ∼950). This suggests that Gem-\nini performs a deeper internal reasoning process to\nsynthesize grounded, compact responses; this be-\nhavior is critical for trustworthy clinical QA. Con-\nsequently, we selected Gemini-3-Pro as our QA\nbackbone.\n6\nUser Study\nTo assess the practical utility of CLINICALTRIAL-\nSHUB, we conducted an initial user study with\n6\n"}, {"page": 7, "text": "Model\nTokens\nTime\nLength\nGemini-3-Pro\n1318.44\n14.33\n698.01\nGPT-5.1\n227.71\n2.81\n1006.82\nClaude-4.5-Sonnet\n210.95\n5.34\n896.54\nTable 4: Performance comparison. Tokens: Avg. com-\npletion tokens; Time: Avg. latency (s); Length: Avg.\noutput characters.\nseven medical professionals: hematologists, pathol-\nogists, dentists, clinical statisticians, pharmacists,\npharmaceutical scientists, and nursing researchers.\nParticipants explored the system for tasks corre-\nsponding to their typical use of PM and CTG. Ta-\nble 5 summarizes key findings. Detailed study\nmaterials are provided in Appendix D.\nMetric\nScore\nSearch Feature Experience (0–5):\nQuery generation rating\n4.50\nFiltering rating\n4.40\nEligibility check rating\n4.50\nCombined search utility\n4.14\nInformation Extraction Accuracy (0–5):\nStudy Overview\n5.0\nStudy Plan\n5.0\nParticipation Requirements\n4.83\nBaseline Characteristics\n4.83\nOutcome Measures\n4.83\nParticipant Flow\n4.7\nAdverse Events\n4.6\nChatbot answer quality (0–5)\n4.86\nTable 5: User study results summary. Scores reflect\nparticipant ratings of ClinicalTrialsHub functionality.\nSearch Stage. Six of seven participants found\n6 or more relevant studies with CLINICALTRI-\nALSHUB compared to 5/7 for PubMed and 3/7\nfor CTG among the top 30 results, demonstrating\nimproved relevance through unified search with\nBM25 reranking. Query generation (4.50), filter-\ning (4.40), and eligibility checking (4.50) features\nreceived high ratings. The overall combined search\ncapability received strong approval (4.14), validat-\ning the value of eliminating cross-platform naviga-\ntion. Time-saving received moderate rating (3.71),\npotentially reflecting initial learning curve.\nReview Stage.\nSix of seven participants re-\nviewed the structured extraction from PubMed.\nPerceived accuracy was consistently high: Study\nOverview and Study Plan received perfect scores\n(5.0), while Participation Requirements, Baseline\nCharacteristics, and Outcome Measures scored\n4.83. Results-oriented sections (Participant Flow:\n4.7, Adverse Events: 4.6) also rated highly. All\nseven participants used the chatbot, rating answer\nquality (4.86) and overall detail page efficiency\n(4.86) very highly.\n7\nRelated Work\nPrior work has aimed to improve clinical trial in-\nformation access and evidence synthesis in various\nways. Trialstreamer (Marshall et al., 2020) struc-\ntures PubMed articles for rapid evidence brows-\ning. RobotReviewer (Marshall et al., 2016) auto-\nmates trial data extraction and risk-of-bias evalua-\ntion specifically for systematic reviews. LinkedCT\n(Hassanzadeh et al., 2009) transforms ClinicalTri-\nals.gov data into structured linked data though it\ndoes not integrate literature sources, unlike our\nwork. Prior work studied the use of LLMs this\nspace as well—for patient-trial matching, clini-\ncal trial design, and participant recruitment (Wang\net al., 2024). Most recently, TrialPanorama (Wang\net al., 2025a) established a large-scale database and\nbenchmark for these tasks, while LEADS (Wang\net al., 2025b) introduced a foundation model specif-\nically designed to enhance human-AI collaboration\nin medical literature mining. CLINICALTRIAL-\nSHUB extends these approaches into a unified plat-\nform, bridging registry and literature silos. Unlike\nTrialstreamer or LinkedCT, it integrates both data\nsources, and compared to RobotReviewer’s narrow\nfocus on systematic reviews, it supports broader\ninteractive exploration and structured retrieval for\ndiverse clinical and research tasks.\n8\nConclusion and Future Work\nWe presented CLINICALTRIALSHUB, a unified\nplatform that integrates structured trial registry data\nfrom ClinicalTrials.gov with structured informa-\ntion extracted from unstructured PubMed publica-\ntions using LLMs. Our system enhances access\nto comprehensive clinical trial information by en-\nabling unified search, structured information extrac-\ntion and attributed question answering, supporting\nthe diverse needs of patients, clinicians, and re-\nsearchers. In future work, we will improve our\nextraction models through domain-adapted LLM\nfine-tuning and integrate patient-centric features\nsuch as eligibility matching and personalized trial\nsummaries. While our initial user study with seven\nmedical professionals demonstrated the platform’s\nutility across diverse clinical roles, we plan to con-\nduct larger-scale usability studies to validate these\n7\n"}, {"page": 8, "text": "findings and guide further development.\nReferences\nAnthropic. 2025. Claude 4.5 sonnet system card. Tech-\nnical report, Anthropic.\nGemini Team, Google. 2025. Gemini 3: A multimodal\nfamily of capable models. Technical report, Google\nDeepMind.\nOktie\nHassanzadeh,\nAnastasios\nKementsietsidis,\nLipyeow Lim, Renée J Miller, and Min Wang. 2009.\nLinkedct: A linked data space for clinical trials.\nTechnical Report CSRG-596, University of Toronto.\nAlon Jacovi, Andrew Wang, Chris Alberti, Connie\nTao, Jon Lipovetz, Kate Olszewska, Lukas Haas,\nMichelle Liu, Nate Keating, Adam Bloniarz, Carl\nSaroufim, Corey Fry, Dror Marcus, Doron Kuklian-\nsky, Gaurav Singh Tomar, James Swirhun, Jinwei\nXing, Lily Wang, Madhu Gurumurthy, and 7 others.\n2025. The facts grounding leaderboard: Benchmark-\ning llms’ ability to ground responses to long-form\ninput. Preprint, arXiv:2501.03200.\nHarold W. Kuhn. 1955. The hungarian method for the\nassignment problem. Naval Research Logistics Quar-\nterly, 2(1–2):83–97.\nIain J Marshall, Joël Kuiper, and Byron C Wallace.\n2016. Robotreviewer: evaluation of a system for\nautomatically assessing bias in clinical trials. Jour-\nnal of the American Medical Informatics Association,\n23(1):193–201. Originally published online 2015.\nIain J Marshall, Benjamin Nye, Joël Kuiper, Anna\nNoel-Storr, Rachel Marshall, Rory Maclean, Frank\nSoboczenski, Ani Nenkova, James Thomas, and By-\nron C Wallace. 2020. Trialstreamer: A living, auto-\nmatically updated database of clinical trial reports.\nJournal of the American Medical Informatics Associ-\nation, 27(12):1903–1912.\nOpenAI. 2025. Gpt-5.1 system card. Technical report,\nOpenAI.\nStephen E Robertson and Steve Walker. 1994. Some\nsimple effective approximations to the 2-poisson\nmodel for probabilistic weighted retrieval. In SI-\nGIR’94: Proceedings of the Seventeenth Annual In-\nternational ACM-SIGIR Conference on Research and\nDevelopment in Information Retrieval, organised by\nDublin City University, pages 232–241. Springer.\nZifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin,\nZhiyong Lu, and Jimeng Sun. 2024. Accelerating\nclinical evidence synthesis with large language mod-\nels. Preprint, arXiv:2406.17755.\nZifeng Wang, Qiao Jin, Jiacheng Lin, Junyi Gao, Jathur-\nshan Pradeepkumar, Pengcheng Jiang, Benjamin\nDanek, Zhiyong Lu, and Jimeng Sun. 2025a. Tri-\nalpanorama: Database and benchmark for system-\natic review and design of clinical trials. Preprint,\narXiv:2505.16097.\nZifeng Wang, Qiao Jin, Weicheng Ma, and 1 others.\n2025b. Leads: a foundation model for medical litera-\nture mining. Nature Communications, 16(1):1–16.\nNancy L Wilczynski, Kathleen A McKibbon, and\nRobert B Haynes. 2011. Sensitive clinical queries\nretrieved relevant systematic reviews as well as pri-\nmary studies: an analytic survey. Journal of Clinical\nEpidemiology, 64(12):1341–1349.\nA\nAdditional Statistics Details\nOur data expansion is calculated as:\nExpansion(%) = PMC trials without CTG\nTotal PMC trials\n× 100\n(1)\nThis yields expansion rates of 74.2% (speci-\nficity: 102,608/138,279) to 93.4% (sensitivity:\n898,846/962,774), averaging 83.8%.\nB\nReranking and Result Integration\nDetails\nWe implement BM25-based reranking mechanisms\n(Robertson and Walker, 1994) to individual search\nresults, followed by deduplication to merge related\npublications and trials.\nCorpus Construction:\nFor PubMed documents,\nwe construct searchable text by concatenating: (1)\narticle titles, (2) abstracts (structured or unstruc-\ntured), (3) author-provided keywords, (4) Medical\nSubject Headings (MeSH) descriptors, and (5) jour-\nnal names. For CTG documents, we concatenate:\n(1) trial titles, (2) condition specifications, (3) brief\nsummaries, and (4) keywords.\nBM25 Scoring:\nDuring integration,\nBM25\nscores are computed for each source separately,\nthen min-max normalized to the [0, 1] range and\ncombined with a position-based bonus from the\noriginal API ranking. For each document di, the\nscore is computed as:\nBM25(di) −BM25min\nBM25max −BM25min\n+ 0.2 · N −i\nN\nwhere BM25(di) represents the raw BM25 score,\nBM25min and BM25max are the minimum and\nmaximum scores for normalization, N is the to-\ntal number of results, and i is the zero-indexed\nposition in the original API ranking. The con-\nstant 0.2 represents the maximum position bonus.\nWhen BM25max = BM25min (indicating identical\nscores), the BM25 component is set to 0.0 for all\ndocuments, and the final ranking relies entirely on\nthe original API position.\n8\n"}, {"page": 9, "text": "Bidirectional\nDeduplication:\nWe\nmerge\nPubMed and CTG entries that reference each\nother.3 Merged pairs receive a relevance bonus.4\nAfter rescoring and merging, all entries are sorted\nby their final scores and shown to the user.\nC\nInformation Extraction Details\nC.1\nModular Prompt System\nThe extraction framework is organized into 3 main\nsections corresponding to the CTG API v2.0 struc-\nture. We provide the exact prompts in our reposi-\ntory.\n• protocolSection: Contains basic study informa-\ntion including identification modules (NCT ID, ti-\ntle, sponsors), status modules (enrollment, dates,\nphases), design modules (study type, allocation,\nmasking), arms and interventions, eligibility cri-\nteria, contacts, and locations.\n• resultsSection: Encompasses study outcomes\nand results when available, including participant\nflow, baseline characteristics, outcome measures,\nadverse events, and statistical analyses.\n• derivedSection: Includes system-generated data\nelements such as condition browse modules, in-\ntervention browse modules, and MeSH term map-\npings.\nC.2\nData Validation and Schema\nConformance\nDuring extraction, we asynchronously operate a\nvalidation pipeline.\n• Schema-level validation enforces structural in-\ntegrity across all five data sections. Each ex-\ntracted field undergoes type checking against\nCTG’s built-in types, including character limits\nfor text fields (briefTitle: 300 chars, officialTi-\ntle: 600 chars, eligibilityCriteria: 20,000 chars),\nISO 8601 date formatting, and proper nesting for\ncomplex structures like baseline characteristics\nand outcome measures.\n• Enumerated value validation ensures all cat-\negorical fields contain only permitted values.\nThe system validates against ClinicalTrials.gov’s\ncomprehensive enum definitions for critical fields\n3This merging criterion is designed for high precision.\nWe acknowledge that might miss certain trials that may have\nunidirectional references but do not reference each other (for\ninstance, trials that were registered before the publication but\nnever updated)\n4We empirically determine this bonus to 0.3.\nincluding studyType, allocation, intervention-\nModel, phases, sex, and standardized age groups.\nNon-conforming values trigger immediate cor-\nrection through fuzzy matching against allowed\nvalues.\n• Clinical terminology verification integrates\nwith the NCBI MeSH API to validate medical\nsubject headings in real-time. The system per-\nforms fuzzy matching for condition and interven-\ntion terms, automatically suggesting and apply-\ning standardized MeSH descriptors. This pre-\nvents terminology drift and ensures compatibility\nwith biomedical databases.\n• Statistical consistency checking validates the\ncoherence of quantitative data, verifying group-\nmeasure associations in baseline characteris-\ntics, confirming unit consistency across mea-\nsurements, and validating statistical parameters\n(means, standard deviations, confidence inter-\nvals) for mathematical soundness.\nMissing information is handled through deliberate\nomission rather than placeholder generation, pre-\nserving data integrity for downstream analysis.\nC.3\nSample ID Filter Query - PubMed\n((\"randomized controlled trial\"[\nPublication Type]\nOR \"controlled clinical trial\"[\nPublication Type]\nOR \"randomized\"[Title/Abstract]\nOR \"placebo\"[Title/Abstract]\nOR \"clinical trials as topic\"[\nMeSH Terms:noexp]\nOR \"randomly\"[Title/Abstract]\nOR \"trial\"[Title])\nNOT (\"animals\"[MeSH Terms] NOT \"\nhumans\"[MeSH Terms]))\nAND (\"english\"[Language] OR \"\nEnglish\"[lang])\nAND \"pubmed pmc open access\"[\nFilter]\nAND \"clinicaltrials gov\"[\nSecondary Source ID]\nAND \"2021/01/01\"[Date -\nPublication] : \"3000\"[Date -\nPublication]\nC.4\nSemantic Similarity Prompt\nCompare these clinical trial\nfield values for '{\n9\n"}, {"page": 10, "text": "field_explanation}'. Return\nonly a number 0-1 for semantic\nsimilarity.\nText1: {reference_value}\nText2: {predicted_value}\nD\nUser Study Details\nTo assess the practical utility of ClinicalTrialsHub\nin realistic workflows, we conducted an initial user\nstudy with seven medical professionals. Partici-\npants were recruited across diverse roles: hematol-\nogy, pathology, dentistry, clinical statistics, phar-\nmacy, pharmaceutical science (PhD student), and\nnursing (PhD candidate). The participant pool com-\nprised a balanced distribution of clinicians (3/7),\nclinical researchers (2/7), and students or trainees\n(2/7), thereby representing key stakeholder groups\nwho utilize clinical trial data. Each participant\nreceived an explanation of the system’s features\nand was asked to use CLINICALTRIALSHUB for\ntasks that correspond to their typical use of PM\nand CTG. The contexts in which participants ex-\nplored baseline systems (PM/CTG) and CLINICAL-\nTRIALSHUB, along with their stated purposes, are\ndescribed in Table 7, while the actual queries they\ninput for each platform are shown in Table 9.\nD.1\nSearch Stage Evaluation\nQuery Generation\nAlthough only two partici-\npants explicitly used the natural language query\ngeneration feature, both rated it highly (4.50). This\nlow usage rate is unsurprising given that most par-\nticipants were already proficient in constructing\nBoolean queries for PubMed and CTG. Table 8\nshows their natural language inputs and the system-\ngenerated structured queries. These results demon-\nstrate that the system accurately decomposes com-\nplex natural language requests into structured pa-\nrameters, particularly benefiting less experienced\nusers or those working outside their primary do-\nmain.\nFiltering\nFive participants used the filtering capa-\nbilities and rated this feature highly (4.40). The fil-\nters applied spanned temporal constraints (publica-\ntion date ranges from 5-10 years), study design pa-\nrameters (study type—interventional/observational,\nRCT status, data availability requirements (PMC\nOpen Access, CTG with results posted), and pop-\nulation specifications (humans, age restrictions,\ncompletion status). Table 10 documents the spe-\ncific filter combinations each participant employed.\nThe CLINICALTRIALSHUB’s filtering mechanism\nenabled users to impose identical selection crite-\nria on heterogeneous data sources, reducing the\nneed to mentally translate filter semantics between\nplatform-specific interfaces.\nEligibility Check\nFour participants utilized the\neligibility criteria specification feature, which also\nreceived a high average rating (4.50). This fea-\nture allowed users to define detailed inclusion and\nexclusion criteria that went beyond simple filter\ncombinations. For instance, one participant speci-\nfied inclusion criteria requiring \"randomized con-\ntrolled or single-arm registrational trials with ≥\n50 patients\" that \"reported at least one primary\noutcome,\" while explicitly excluding \"studies in-\ncluding other myeloproliferative neoplasms with-\nout fibrosis.\" A diabetes researcher defined even\nmore granular criteria spanning intervention type\n(\"dyadic or family-based behavioral, psychoedu-\ncational, or self-management intervention\"), pop-\nulation characteristics (\"adults ≥18 years with\ntype 2 diabetes\"), outcome requirements (diabetes\nself-efficacy, self-management behaviors, dyadic\nprocesses, or HbA1c), and temporal constraints\n(\"follow-up of at least 3 months\"). The complete\nset of criteria specifications is provided in Table 11.\nThe high rating reflects users’ need for nuanced eli-\ngibility assessment that cannot be captured through\nsimple keyword filtering alone—particularly for\nsystematic review preparation and evidence syn-\nthesis where precise population and study design\nspecifications are critical.\nOverall System Utility\nTable 6 shows the dis-\ntribution of relevant studies participants identified\namong the top 30 results. 6 of 7 participants found\n6 or more relevant studies with CLINICALTRIAL-\nSHUB, compared to 5 of 7 for PubMed and only\n3 of 7 for CTG. Although CTH demonstrated im-\nproved relevance, the maximum response option\nof ’11+’ prevents precise quantification of the mag-\nnitude of improvement.\nNevertheless, our uni-\nfied search with BM25 reranking successfully in-\ntegrated heterogeneous data sources into a single\nranking system without degrading user satisfac-\ntion. This suggests that reducing manual screening\neffort through cross-source integration is achiev-\nable even when reconciling disparate ranking al-\ngorithms. The overall combined search capabil-\nity received strong approval (4.14), validating the\nvalue proposition of eliminating cross-platform\n10\n"}, {"page": 11, "text": "Platform\n0\n1–5\n6–10\n11+\nPubMed\n0\n2\n2\n3\nClinicalTrials.gov\n0\n4\n2\n1\nClinicalTrialsHub\n0\n1\n3\n3\nTable 6: Distribution of relevant studies identified\namong top 30 results.\nnavigation and manual deduplication. However,\nthe time-saving metric received a more moderate\nrating (3.71). This may reflect the initial learning\ncurve associated with a new interface or, alterna-\ntively, that experienced users already possess ef-\nficient workflows for rapidly applying trial filters\non baseline systems. We view these responses as\nan opportunity to identify superior interaction pat-\nterns from existing systems and either integrate\nthem into ClinicalTrialsHub or better expose our\nsystem’s capabilities to accelerate clinical trial re-\nsearch activities.\nD.2\nReview Stage Evaluation\nIn the review stage, we focused on how well the\ndetail page supported close reading of individ-\nual trials.\nParticipants were first asked to rate\nthe accuracy of seven structured sections distilled\nfrom PMC (Study Overview, Participation Require-\nments, Study Plan, Participant Flow, Baseline Char-\nacteristics, Outcome Measures, Adverse Events) on\na 0–5 scale, then to judge whether this representa-\ntion and the integrated chatbot detail page helped\nthem interpret studies more efficiently and save\ntime.\nInformation Extraction\n6 of 7 participants re-\nported reviewing the structured extraction from\nPubMed within CLINICALTRIALSHUB. Across\nthose respondents, perceived accuracy was consis-\ntently high: both the Study Overview and Study\nPlan modules received perfect mean scores of\n5.0/5.0, while Participation Requirements, Base-\nline Characteristics, and Outcome Measures clus-\ntered tightly of 4.83. Even the more detailed results-\noriented sections, Participant Flow (4.7) and Ad-\nverse Events (4.6, with one non-response), were\nrated near the top of the scale. These scores suggest\nthat, for the protocol-level fields we benchmark in\n§4, clinicians also subjectively experience the struc-\ntured representations as faithful to the source.\nQA\nAll seven participants used the chatbot on\nthe detail page, and they were asked to provide up\nto three concrete examples (paper ID, question, and\nanswer) from their own sessions. Their questions\nillustrate how the assistant is used as an interpretive\nlayer rather than a generic Q&A tool. Several par-\nticipants asked design and endpoint focused ques-\ntions, such as summarizing how the sample size\nwas determined, clarifying what the primary end-\npoint was and whether it was met, or checking\nwhether specific biomarkers were collected. Others\nqueried safety and practical implications, includ-\ning requests for the most common adverse events,\nrecommended safest order for clinical steps, or\nwhether particular clinical outcomes showed im-\nprovement. Some participants targeted fields in\nprotocol section, such as total enrollment numbers,\ncompletion dates, or the geographic distribution\nof trial sites. Across these diverse uses, partici-\npants rated the chatbot’s answers as highly accurate\nand relevant (4.86) and also agreed that the com-\nbined detail page—full text, structured view, and\nchatbot—helped them interpret study information\nefficiently (4.86).\n11\n"}, {"page": 12, "text": "Table 7: Participant occupations and task contexts\nID\nOccupation\nTask for This Evaluation\n1\nMD, Specializing in Hematol-\nogy\nI am the page editor for the Myelofibrosis evidence page on a hematology platform\nthat aggregates and curates high-quality data from recent hematologic trials. My re-\nsponsibility is to regularly review newly published clinical trials and update the page\nwith evidence that meets my inclusion criteria. For this evaluation, I need to identify\nand organize the conclusions of recent myelofibrosis trials that should be added to the\nMyelofibrosis evidence page.\n2\nPharmacist\nI am looking for dosing decision evidence for a specific patient who is receiving cefepime\nand continuous renal replacement therapy (CRRT) in the setting of acute kidney failure,\nto ensure that the cefepime dose remains below the toxic range while still being effective.\n3\nPhD Student – pharmaceutical\nscience\nI am preparing my candidacy proposal and reviewing preliminary studies exploring\naspirin for the prevention of pre-eclampsia in high-risk pregnancies. I would like to\nexamine how prior studies have evaluated placental biomarkers in this context.\n4\nStatistician\nI am designing a clinical trial for a new EGFR-targeted therapy in non-small cell lung\ncancer. For this evaluation, I need to review existing evidence on EGFR-targeted\ntherapies, including their efficacy and safety profiles, to support the study design.\n5\nNurse (PhD Candidate)\nI am conducting a systematic review of dyadic and family-based interventions for adults\nwith type 2 diabetes, focusing on diabetes self-management, self-efficacy, and related psy-\nchosocial outcomes. My responsibility is to identify recent randomized and registrational\ntrials, screen them against predefined inclusion criteria, and organize the conclusions\nwith respect to dyadic or family-based diabetes self-management interventions.\n6\nDentist; General Dentistry\nI am preparing a paper that discusses the need to standardize IV moderate sedation\ntraining and competency assessment in dental residency programs. For this evaluation,\nI want to identify clinical trials and observational studies on IV moderate sedation in\ndentistry, including evidence on patient selection, resident training, and the appropriate\norder and quantity of medications for successful and safe IV moderate sedation.\n7\nPathologist (Dermatopathology\n/ Gynecologic Pathology)\nI recently reviewed a case involving a metastatic cutaneous melanoma with a\nBRAFV600E mutation, confirmed through our molecular service. The patient has\ncompleted wide excision and sentinel lymph node evaluation, and the oncology team\nis now considering adjuvant systemic therapy options. For this case, I would like to\nreview the clinical evidence supporting commonly used adjuvant therapies for metastatic\nmelanoma so I can provide informed context when discussing the pathology findings\nwith the treating oncologists. In particular, I want to find studies evaluating adjuvant\nsystemic treatments used after complete resection of metastatic melanoma, including\noutcomes such as recurrence risk and treatment-related toxicity.\nTable 8: Participant Natural Language Queries and Generated Structured Queries\nID\nNatural Language Query\nStructured Query (CTH Output)\n2\nRandomized controlled trials involving continu-\nous renal replacement therapy and cefepime in\npatients with acute kidney injury\nCondition: acute kidney injury\nIntervention: continuous renal replacement therapy OR cefepime\nOther terms: randomized controlled trials\n7\nI want clinical trials evaluating chemotherapy in\nhigh-stage melanoma, particularly those report-\ning BRAFV600E mutation status.\nCondition: high-stage melanoma OR BRAFV600E mutation\nIntervention: chemotherapy\n12\n"}, {"page": 13, "text": "Table 9: Participant Search Queries Across Systems\nID\nPubMed Query\nClinicalTrials.gov Query\nCTH Query\n1\nmyelofibrosis\nOR\npoly-\ncythemia vera OR essential\nthrombocythemia\nOR\nmyeloproliferative\nCondition:\nmyelofibrosis OR poly-\ncythemia vera OR essential thrombo-\ncythemia OR myeloproliferative\nCondition:\nmyelofibrosis OR poly-\ncythemia vera OR essential thrombo-\ncythemia OR myeloproliferative\n2\ncefepime and crrt\nCondition: Acute Kidney Injury (AKI)\nIntervention:\nContinuous Renal Re-\nplacement Therapy (CRRT)\nOther terms: cefepime\n–\n3\nAspirin AND Pre-eclampsia\nAND biomarkers\nCondition: Pre-eclampsia\nIntervention: aspirin\nOther terms: biomarkers\nCommon: pregnancy\nCondition: pre-eclampsia\nIntervention: aspirin\nOther terms: biomarkers,\nplacental biomarkers\n4\nEGFR-targeted\ntherapies\nnon small cell lung cancer\nCondition: non small cell lung cancer\nIntervention: EGFR-targeted therapies\nCommon:\n“EGFR-targeted therapies\nnon small cell lung cancer”\nIntervention: EGFR-targeted therapies\n5\n“type 2 diabetes” OR “type\n2 diabetes mellitus” OR\nT2DM AND (dyadic OR\nfamily-based OR spouse\nOR\npartner\nOR\ndyadic\nOR\ncaregiver\nOR\nself-\nmanagement OR education)\nCondition: Type 2 Diabetes Mellitus\n(T2DM)\nOther terms: dyadic / family-based in-\nterventions, self-management outcomes\nCondition: type 2 diabetes mellitus\nOther terms:\ndyadic, family-based,\nspouse,\npartner,\ncaregiver,\nself-\nmanagement, education\n6\nIV moderate sedation AND\nmidazolam AND fentanyl\nOther terms: IV moderate sedation AND\nmidazolam AND fentanyl\nOther terms: Dental Anxiety AND Den-\ntal procedures AND IV moderate seda-\ntion guidelines\n7\nmetastatic melanoma and\nBRAFV600E mutation and\nchemotherapy\nCondition: metastatic melanoma\nIntervention: chemotherapy\nOther terms: BRAF V600E mutation\npositive\nCommon:\ncutaneous melanoma OR\nadjuvant\nmelanoma\nOR\nresected\nmelanoma OR \"stage III melanoma\"\nOR \"BRAF V600E melanoma\"\nTable 10: Participant Filters Used Across Systems\nID\nPubMed Filters\nCTG Filters\nCTH Filters\n1\nPublication Date (Last 5 years)\nPhase (III/IV)\nArticle Type (Clinical Trial, RCT)\nAccess (PMC OA)\nSpecies (Humans)\nCompletion Date (Last 5 years)\nPhase (III/IV)\nStudy Type (Interventional)\nHas Results (True)\nStatus (Completed)\nDate (Last 5 years)\nPhase (III/IV)\nStudy Type (Clinical Trial, RCT)\nPubMed (PMC OA, Humans)\nCTG (With Results, Completed)\n2\nAge\nPublication Date\nAge\nCompletion Date\nAge\nDate\n3\nPublication Date\nHas Results (True)\nPhase\nStudy Type\n4\nPublication Date (Last 10 years)\nArticle Type (Clinical Trial)\nCompletion Date (From:11/17/2015)\nDate (Last 10 years)\nStudy Type (Clinical Trial)\n5\nPublication Date (Last 10 years)\nArticle Type (Clinical Trial, RCT)\nSpecies (Humans)\nAge (Adults)\nLanguage (English)\nCompletion Date (Last 10 years)\nStudy Type (Interventional, Randomized)\nStatus (Completed)\nHas Results (True)\nAge (Adults)\nDate (Last 10 years)\nStudy Type (Clinical Trial, RCT)\nAge (Adults)\nPubMed (PMC OA, Humans)\nCTG (With Results, Completed)\n13\n"}, {"page": 14, "text": "Table 11: Participant Inclusion and Exclusion Criteria\nID\nInclusion Criteria\nExclusion Criteria\n1\nRandomized controlled or single-arm registrational trials\nwith ≥50 patients\nReported at least one primary outcome\nOther myeloproliferative neoplasms without fibrosis\n(e.g., PV or ET without MF)\nPrefibrotic MF or early myeloproliferative disease with-\nout confirmed fibrosis\n2\nAKI\nCRRT\n5\nRandomized controlled trials or single-arm registrational\ntrials with approximately ≥50 participants\nAdults (≥18 years) with type 2 diabetes mellitus\nDyadic or family-based behavioral,\npsychoeduca-\ntional, or self-management intervention involving part-\nner/spouse/family\nReported at least one primary or key outcome (self-\nefficacy, self-management behaviors, dyadic/family pro-\ncesses, HbA1c, etc.)\nFollow-up ≥3 months after baseline\nStudies focusing only on type 1 diabetes, gestational\ndiabetes, or prediabetes\nPurely pharmacologic comparisons without behavioral/e-\nducational/family components\nPediatric/adolescent-only populations\nCase reports, very small pilot studies, qualitative-only\nstudies without quantitative outcomes\n6\nDental\nGeneral Anesthesia\n14\n"}, {"page": 15, "text": "Table 12: Participant Chatbot Interactions\nID\nChatbot Interaction\n1\nPMC12092244 | What was the exact pelabresib dose and schedule?\nPelabresib was administered orally at a starting dose of 125 mg once daily for 14 consecutive days followed by a 7-day\nbreak, forming a 21-day cycle. Dose modifications between 50–175 mg once daily were permitted based on adverse\nevents and treatment response.\nPMC12092244 | What was the primary endpoint, and was it met?\nThe primary endpoint was achieving a ≥35% reduction in spleen volume from baseline at week 24. This endpoint\nwas met, with a significantly greater proportion of patients in the pelabresib–ruxolitinib arm achieving this reduction\ncompared to the placebo–ruxolitinib arm.\nPMC12092244 | What were the most common adverse events?\nThe most common treatment-emergent adverse events were hematological—primarily thrombocytopenia and anemia.\nAmong non-hematological events, diarrhea was most frequent in the pelabresib–ruxolitinib arm, whereas constipation\nwas most frequent in the placebo–ruxolitinib arm.\n2\nPMC10576861 | Since ACORN showed higher neurotoxicity with Cefepime, should we reconsider de-escalating\nfrom Piperacillin-Tazobactam to Cefepime solely based on renal concerns?\nBased on the ACORN trial findings, reconsidering the practice of de-escalating from Piperacillin-Tazobactam to\nCefepime solely for renal protection is warranted. The study demonstrated that there was no significant difference in the\nhighest stage of acute kidney injury or death between the two treatment groups. Furthermore, the trial concluded that\nPiperacillin-Tazobactam did not increase the incidence of acute kidney injury, whereas Cefepime was associated with a\nhigher incidence of neurological dysfunction.\nPMC9463324 | The study showed creatinine elevation but no difference in dialysis or mortality. Does this mean\nthe current KDIGO criteria are too sensitive for antibiotic-induced AKI?\nThe study suggests that the creatinine-based KDIGO criteria may identify \"pseudotoxicity\" rather than true renal injury\nin the context of Vancomycin and Piperacillin-Tazobactam therapy. The authors propose that the observed creatinine\nelevation is likely due to the inhibition of tubular secretion rather than damage to the kidney parenchyma, a conclusion\nsupported by the lack of corresponding changes in alternative biomarkers (cystatin C and BUN) and the absence of\nincreased risk for dialysis or mortality.\n3\nNCT03893630 | Did the study report any differences in placental biomarker levels between the 81mg and 162mg\naspirin groups?\nYes, the study reported specific mean changes in placental biomarker levels for the 81mg and 162mg aspirin groups,\nalthough statistical significance of the differences was not explicitly detailed in the provided text. For Placental Growth\nFactor (PIGF), the mean change from baseline to Event 2 was 568.3 pg/ml for the 81mg group and 509.7 pg/ml for the\n162mg group. For soluble Intercellular Adhesion Molecule (s-ICAM), the mean change from baseline to Event 2 was\n1.6 ng/ml for the 81mg group and 7.9 ng/ml for the 162mg group. Data for IL-6 and TNF were not reported due to\ninsufficient serum samples.\n4\nPMC12139608 | Can you summarize how the sample size is determined in this study?\nThe study powered for ≥85% to detect a 25% difference in ORR using one-sided α=0.025; planned enrollment was\n126 patients.\nPMC10318754 | Can you summarize the statistical analysis methods used?\nAnalyses included chi-square/Fisher tests and QoL correlations with PFS/OS and sponsorship.\nPMC9509093 | Summarize tabulated results for all endpoints.\nAumolertinib improved PFS (19.3 vs 9.9 mo); ORR 73.8% vs 72.1%; DCR 93.0% vs 96.7%; DoR 18.1 vs 8.3 mo; fewer\nrash/diarrhea events.\n5\nPMC8266295 | Were any clinical outcomes such as HbA1c reported, and if so, what were the main results?\nThe paper does not report clinical outcomes such as HbA1c changes as part of the results. The study focused on family\nsupporter involvement, support techniques, and caregiving experience rather than direct clinical outcomes like HbA1c.\n6\nPMC5858011 | What do they recommend as the safest order?\nMidazolam-first caused fewer desaturation events (90–92%).\n7\nNCT01682083 | If OS wasn’t clearly significant early on, is the decision to use dabrafenib/trametinib driven\nmostly by the strong RFS benefit?\nYes, the provided results indicate that the primary outcome, Relapse-free Survival (RFS), demonstrated a statistically\nsignificant benefit with a hazard ratio of 0.47 and a p-value of < 0.0001. In contrast, the Overall Survival (OS) analysis\nresulted in a p-value of 0.006, which did not meet the pre-specified significance threshold of 0.000019 for the first interim\nanalysis.\n15\n"}, {"page": 16, "text": "E\nExtraction Schema Fields\nE.1\nExtraction Schema Evaluation Fields\nPiece\n# Ref\n# EV\nGemini\nGPT\nClaude\nDesignObservationalModel\n1\n7.67\n1.00\n1.00\n1.00\nDetailedDescription\n64\n31.67\n1.00\n1.00\n0.86\nStudyType\n100\n0.00\n1.00\n1.00\n0.85\nDesignAllocation\n99\n0.67\n0.99\n1.00\n0.85\nBriefSummary\n100\n0.00\n1.00\n1.00\n0.83\nCondition\n100\n0.00\n1.00\n1.00\n0.83\nDesignInterventionModel\n99\n0.33\n0.98\n1.00\n0.85\nPhase\n99\n0.67\n0.98\n1.00\n0.85\nEnrollmentCount\n100\n0.00\n0.99\n0.99\n0.84\nDesignMasking\n99\n0.67\n0.98\n0.99\n0.85\nNCTId\n100\n0.00\n0.99\n1.00\n0.82\nKeyword\n52\n44.00\n1.00\n1.00\n0.80\nMinimumAge\n99\n0.67\n0.99\n0.99\n0.77\nEligibilityCriteria\n100\n0.00\n0.99\n0.99\n0.77\nHealthyVolunteers\n99\n1.00\n0.99\n0.99\n0.76\nSex\n100\n0.00\n0.99\n0.99\n0.77\nMaximumAge\n54\n39.00\n0.99\n1.00\n0.74\nStdAge\n100\n0.00\n0.98\n0.95\n0.76\nDesignWhoMasked\n59\n3.67\n0.88\n0.94\n0.77\nBriefTitle\n100\n0.00\n0.71\n1.00\n0.84\nOfficialTitle\n100\n0.00\n0.71\n0.81\n0.82\nTable 13: Average Extra-Valid (EV) case analysis and information extraction performance for each field. # Ref:\nnumber of evaluation cases (out of 100) where the field appears in CTG ground-truth. # EV: average cases across\nmodels where the field was Extra-Valid (present in PMC but missing from CTG). Values ≥10 are bolded. Highest\nF1 per row is bolded. Rows ordered by average F1.\nE.2\nExtraction Schema All Fields\nTable 14: Comprehensive list of all fields included in our information-extraction schema.\nIndex\nPiece\nField Index\nprotocolSection.identificationModule\n1\nNCTId\nnctId\n2\nOrgStudyId\norgStudyIdInfo.id\n3\nOrgStudyIdType\norgStudyIdInfo.type\n4\nOrgStudyIdLink\norgStudyIdInfo.link\n5\nSecondaryId\nsecondaryIdInfos.id\n6\nSecondaryIdType\nsecondaryIdInfos.type\n7\nSecondaryIdLink\nsecondaryIdInfos.link\n8\nBriefTitle\nbriefTitle\n9\nOfficialTitle\nofficialTitle\n10\nAcronym\nacronym\n11\nOrgFullName\norganization.fullName\nContinued on next page\n16\n"}, {"page": 17, "text": "Table 14 – Continued from previous page\nIndex\nPiece\nField Index\n12\nOrgClass\norganization.class\nprotocolSection.descriptionModule\n13\nBriefSummary\nbriefSummary\n14\nDetailedDescription\ndetailedDescription\nprotocolSection.conditionsModule\n15\nCondition\nconditions\n16\nKeyword\nkeywords\nprotocolSection.designModule\n17\nStudyType\nstudyType\n18\nPatientRegistry\npatientRegistry\n19\nTargetDuration\ntargetDuration\n20\nPhase\nphases\n21\nDesignAllocation\ndesignInfo.allocation\n22\nDesignInterventionModel\ndesignInfo.interventionModel\n23\nDesignInterventionModelDescription\ndesignInfo.interventionModelDescription\n24\nDesignPrimaryPurpose\ndesignInfo.primaryPurpose\n25\nDesignObservationalModel\ndesignInfo.observationalModel\n26\nDesignTimePerspective\ndesignInfo.timePerspective\n27\nDesignMasking\ndesignInfo.maskingInfo.masking\n28\nDesignMaskingDescription\ndesignInfo.maskingInfo.maskingDescription\n29\nDesignWhoMasked\ndesignInfo.maskingInfo.whoMasked\n30\nEnrollmentCount\nenrollmentInfo.count\n31\nEnrollmentType\nenrollmentInfo.type\nprotocolSection.armsInterventionsModule\n32\nArmGroupLabel\narmGroups.label\n33\nArmGroupType\narmGroups.type\n34\nArmGroupDescription\narmGroups.description\n35\nArmGroupInterventionName\narmGroups.interventionNames\n36\nInterventionType\ninterventions.type\n37\nInterventionName\ninterventions.name\n38\nInterventionDescription\ninterventions.description\n39\nInterventionArmGroupLabel\ninterventions.armGroupLabels\nprotocolSection.outcomesModule\n40\nPrimaryOutcomeMeasure\nprimaryOutcomes.measure\n41\nPrimaryOutcomeDescription\nprimaryOutcomes.description\n42\nPrimaryOutcomeTimeFrame\nprimaryOutcomes.timeFrame\n43\nSecondaryOutcomeMeasure\nsecondaryOutcomes.measure\n44\nSecondaryOutcomeDescription\nsecondaryOutcomes.description\n45\nSecondaryOutcomeTimeFrame\nsecondaryOutcomes.timeFrame\n46\nOtherOutcomeMeasure\notherOutcomes.measure\n47\nOtherOutcomeDescription\notherOutcomes.description\n48\nOtherOutcomeTimeFrame\notherOutcomes.timeFrame\nprotocolSection.eligibilityModule\nContinued on next page\n17\n"}, {"page": 18, "text": "Table 14 – Continued from previous page\nIndex\nPiece\nField Index\n49\nEligibilityCriteria\neligibilityCriteria\n50\nHealthyVolunteers\nhealthyVolunteers\n51\nSex\nsex\n52\nMinimumAge\nminimumAge\n53\nMaximumAge\nmaximumAge\n54\nStdAge\nstdAges\n55\nStudyPopulation\nstudyPopulation\n56\nSamplingMethod\nsamplingMethod\nresultsSection.participantFlowModule\n57\nFlowPreAssignmentDetails\npreAssignmentDetails\n58\nFlowRecruitmentDetails\nrecruitmentDetails\n59\nFlowTypeUnitsAnalyzed\ntypeUnitsAnalyzed\n60\nFlowGroupId\ngroups.id\n61\nFlowGroupTitle\ngroups.title\n62\nFlowGroupDescription\ngroups.description\n63\nFlowPeriodTitle\nperiods.title\n64\nFlowMilestoneType\nperiods.milestones.type\n65\nFlowMilestoneComment\nperiods.milestones.comment\n66\nFlowAchievementGroupId\nperiods.milestones.achievements.groupId\n67\nFlowAchievementComment\nperiods.milestones.achievements.comment\n68\nFlowAchievementNumSubjects\nperiods.milestones.achievements.numSubjects\n69\nFlowAchievementNumUnits\nperiods.milestones.achievements.numUnits\n70\nFlowDropWithdrawType\nperiods.dropWithdraws.type\n71\nFlowDropWithdrawComment\nperiods.dropWithdraws.comment\n72\nFlowReasonGroupId\nperiods.dropWithdraws.reasons.groupId\n73\nFlowReasonComment\nperiods.dropWithdraws.reasons.comment\n74\nFlowReasonNumSubjects\nperiods.dropWithdraws.reasons.numSubjects\nresultsSection.baselineCharacteristicsModule\n75\nBaselinePopulationDescription\npopulationDescription\n76\nBaselineTypeUnitsAnalyzed\ntypeUnitsAnalyzed\n77\nBaselineGroupId\ngroups.id\n78\nBaselineGroupTitle\ngroups.title\n79\nBaselineGroupDescription\ngroups.description\n80\nBaselineDenomUnits\ndenoms.units\n81\nBaselineDenomCountGroupId\ndenoms.counts.groupId\n82\nBaselineDenomCountValue\ndenoms.counts.value\n83\nBaselineMeasureTitle\nmeasures.title\n84\nBaselineMeasureDescription\nmeasures.description\n85\nBaselineMeasurePopulationDescription\nmeasures.populationDescription\n86\nBaselineMeasureParamType\nmeasures.paramType\n87\nBaselineMeasureDispersionType\nmeasures.dispersionType\n88\nBaselineMeasureUnitOfMeasure\nmeasures.unitOfMeasure\n89\nBaselineMeasureDenomUnits\nmeasures.denoms.units\n90\nBaselineMeasureDenomCountGroupId\nmeasures.denoms.counts.groupId\n91\nBaselineMeasureDenomCountValue\nmeasures.denoms.counts.value\nContinued on next page\n18\n"}, {"page": 19, "text": "Table 14 – Continued from previous page\nIndex\nPiece\nField Index\n92\nBaselineClassTitle\nmeasures.classes.title\n93\nBaselineClassDenomUnits\nmeasures.classes.denoms.units\n94\nBaselineClassDenomCountGroupId\nmeasures.classes.denoms.counts.groupId\n95\nBaselineClassDenomCountValue\nmeasures.classes.denoms.counts.value\n96\nBaselineCategoryTitle\nmeasures.classes.categories.title\n97\nBaselineMeasurementGroupId\nmeasures.classes.categories.measurements.groupId\n98\nBaselineMeasurementValue\nmeasures.classes.categories.measurements.value\n99\nBaselineMeasurementSpread\nmeasures.classes.categories.measurements.spread\n100\nBaselineMeasurementLowerLimit\nmeasures.classes.categories.measurements.lowerLimit\n101\nBaselineMeasurementUpperLimit\nmeasures.classes.categories.measurements.upperLimit\nresultsSection.outcomeMeasuresModule\n102\nOutcomeMeasureType\noutcomeMeasures.type\n103\nOutcomeMeasureTitle\noutcomeMeasures.title\n104\nOutcomeMeasureDescription\noutcomeMeasures.description\n105\nOutcomeMeasurePopulationDescription\noutcomeMeasures.populationDescription\n106\nOutcomeMeasureReportingStatus\noutcomeMeasures.reportingStatus\n107\nOutcomeMeasureAnticipatedPostingDate\noutcomeMeasures.anticipatedPostingDate\n108\nOutcomeMeasureParamType\noutcomeMeasures.paramType\n109\nOutcomeMeasureDispersionType\noutcomeMeasures.dispersionType\n110\nOutcomeMeasureUnitOfMeasure\noutcomeMeasures.unitOfMeasure\n111\nOutcomeMeasureCalculatePct\noutcomeMeasures.calculatePct\n112\nOutcomeMeasureTimeFrame\noutcomeMeasures.timeFrame\n113\nOutcomeMeasureTypeUnitsAnalyzed\noutcomeMeasures.typeUnitsAnalyzed\n114\nOutcomeMeasureDenomUnitsSelected\noutcomeMeasures.denomUnitsSelected\n115\nOutcomeGroupId\noutcomeMeasures.groups.id\n116\nOutcomeGroupTitle\noutcomeMeasures.groups.title\n117\nOutcomeGroupDescription\noutcomeMeasures.groups.description\n118\nOutcomeDenomUnits\noutcomeMeasures.denoms.units\n119\nOutcomeDenomCountGroupId\noutcomeMeasures.denoms.counts.groupId\n120\nOutcomeDenomCountValue\noutcomeMeasures.denoms.counts.value\n121\nOutcomeClassTitle\noutcomeMeasures.classes.title\n122\nOutcomeClassDenomUnits\noutcomeMeasures.classes.denoms.units\n123\nOutcomeClassDenomCountGroupId\noutcomeMeasures.classes.denoms.counts.groupId\n124\nOutcomeClassDenomCountValue\noutcomeMeasures.classes.denoms.counts.value\n125\nOutcomeCategoryTitle\noutcomeMeasures.classes.categories.title\n126\nOutcomeMeasurementGroupId\noutcomeMeasures.classes.categories.measurements.groupId\n127\nOutcomeMeasurementValue\noutcomeMeasures.classes.categories.measurements.value\n128\nOutcomeMeasurementSpread\noutcomeMeasures.classes.categories.measurements.spread\n129\nOutcomeMeasurementLowerLimit\noutcomeMeasures.classes.categories.measurements.lowerLimit\n130\nOutcomeMeasurementUpperLimit\noutcomeMeasures.classes.categories.measurements.upperLimit\n131\nOutcomeMeasurementComment\noutcomeMeasures.classes.categories.measurements.comment\n132\nOutcomeAnalysisParamType\noutcomeMeasures.analyses.paramType\n133\nOutcomeAnalysisParamValue\noutcomeMeasures.analyses.paramValue\n134\nOutcomeAnalysisDispersionType\noutcomeMeasures.analyses.dispersionType\n135\nOutcomeAnalysisDispersionValue\noutcomeMeasures.analyses.dispersionValue\n136\nOutcomeAnalysisStatisticalMethod\noutcomeMeasures.analyses.statisticalMethod\nContinued on next page\n19\n"}, {"page": 20, "text": "Table 14 – Continued from previous page\nIndex\nPiece\nField Index\n137\nOutcomeAnalysisStatisticalComment\noutcomeMeasures.analyses.statisticalComment\n138\nOutcomeAnalysisPValue\noutcomeMeasures.analyses.pValue\n139\nOutcomeAnalysisPValueComment\noutcomeMeasures.analyses.pValueComment\n140\nOutcomeAnalysisCINumSides\noutcomeMeasures.analyses.ciNumSides\n141\nOutcomeAnalysisCIPctValue\noutcomeMeasures.analyses.ciPctValue\n142\nOutcomeAnalysisCILowerLimit\noutcomeMeasures.analyses.ciLowerLimit\n143\nOutcomeAnalysisCIUpperLimit\noutcomeMeasures.analyses.ciUpperLimit\n144\nOutcomeAnalysisCILowerLimitComment outcomeMeasures.analyses.ciLowerLimitComment\n145\nOutcomeAnalysisCIUpperLimitComment outcomeMeasures.analyses.ciUpperLimitComment\n146\nOutcomeAnalysisEstimateComment\noutcomeMeasures.analyses.estimateComment\n147\nOutcomeAnalysisTestedNonInferiority\noutcomeMeasures.analyses.testedNonInferiority\n148\nOutcomeAnalysisNonInferiorityType\noutcomeMeasures.analyses.nonInferiorityType\n149\nOutcomeAnalysisNonInferiorityComment outcomeMeasures.analyses.nonInferiorityComment\n150\nOutcomeAnalysisOtherAnalysisDescriptionoutcomeMeasures.analyses.otherAnalysisDescription\n151\nOutcomeAnalysisGroupDescription\noutcomeMeasures.analyses.groupDescription\n152\nOutcomeAnalysisGroupId\noutcomeMeasures.analyses.groupIds\nresultsSection.adverseEventsModule\n153\nEventsFrequencyThreshold\nfrequencyThreshold\n154\nEventsTimeFrame\ntimeFrame\n155\nEventsDescription\ndescription\n156\nEventsAllCauseMortalityComment\nallCauseMortalityComment\n157\nEventGroupId\neventGroups.id\n158\nEventGroupTitle\neventGroups.title\n159\nEventGroupDescription\neventGroups.description\n160\nEventGroupDeathsNumAffected\neventGroups.deathsNumAffected\n161\nEventGroupDeathsNumAtRisk\neventGroups.deathsNumAtRisk\n162\nEventGroupSeriousNumAffected\neventGroups.seriousNumAffected\n163\nEventGroupSeriousNumAtRisk\neventGroups.seriousNumAtRisk\n164\nEventGroupOtherNumAffected\neventGroups.otherNumAffected\n165\nEventGroupOtherNumAtRisk\neventGroups.otherNumAtRisk\n166\nSeriousEventTerm\nseriousEvents.term\n167\nSeriousEventOrganSystem\nseriousEvents.organSystem\n168\nSeriousEventSourceVocabulary\nseriousEvents.sourceVocabulary\n169\nSeriousEventAssessmentType\nseriousEvents.assessmentType\n170\nSeriousEventNotes\nseriousEvents.notes\n171\nSeriousEventStatsGroupId\nseriousEvents.stats.groupId\n172\nSeriousEventStatsNumEvents\nseriousEvents.stats.numEvents\n173\nSeriousEventStatsNumAffected\nseriousEvents.stats.numAffected\n174\nSeriousEventStatsNumAtRisk\nseriousEvents.stats.numAtRisk\n175\nOtherEventTerm\notherEvents.term\n176\nOtherEventOrganSystem\notherEvents.organSystem\n177\nOtherEventSourceVocabulary\notherEvents.sourceVocabulary\n178\nOtherEventAssessmentType\notherEvents.assessmentType\n179\nOtherEventNotes\notherEvents.notes\n180\nOtherEventStatsGroupId\notherEvents.stats.groupId\n181\nOtherEventStatsNumEvents\notherEvents.stats.numEvents\nContinued on next page\n20\n"}, {"page": 21, "text": "Table 14 – Continued from previous page\nIndex\nPiece\nField Index\n182\nOtherEventStatsNumAffected\notherEvents.stats.numAffected\n183\nOtherEventStatsNumAtRisk\notherEvents.stats.numAtRisk\nresultsSection.moreInfoModule\n184\nLimitationsAndCaveatsDescription\nlimitationsAndCaveats.description\n185\nAgreementPISponsorEmployee\ncertainAgreement.piSponsorEmployee\n186\nAgreementRestrictionType\ncertainAgreement.restrictionType\n187\nAgreementRestrictiveAgreement\ncertainAgreement.restrictiveAgreement\n188\nAgreementOtherDetails\ncertainAgreement.otherDetails\n189\nPointOfContactTitle\npointOfContact.title\n190\nPointOfContactOrganization\npointOfContact.organization\n191\nPointOfContactEMail\npointOfContact.email\n192\nPointOfContactPhone\npointOfContact.phone\n193\nPointOfContactPhoneExt\npointOfContact.phoneExt\nderivedSection.conditionBrowseModule\n194\nConditionMeshId\nmeshes.id\n195\nConditionMeshTerm\nmeshes.term\n196\nConditionAncestorId\nancestors.id\n197\nConditionAncestorTerm\nancestors.term\n198\nConditionBrowseLeafId\nbrowseLeaves.id\n199\nConditionBrowseLeafName\nbrowseLeaves.name\n200\nConditionBrowseLeafRelevance\nbrowseLeaves.relevance\n201\nConditionBrowseBranchAbbrev\nbrowseBranches.abbrev\n202\nConditionBrowseBranchName\nbrowseBranches.name\nderivedSection.interventionBrowseModule\n203\nInterventionMeshId\nmeshes.id\n204\nInterventionMeshTerm\nmeshes.term\n205\nInterventionAncestorId\nancestors.id\n206\nInterventionAncestorTerm\nancestors.term\n207\nInterventionBrowseLeafId\nbrowseLeaves.id\n208\nInterventionBrowseLeafName\nbrowseLeaves.name\n209\nInterventionBrowseLeafRelevance\nbrowseLeaves.relevance\n210\nInterventionBrowseBranchAbbrev\nbrowseBranches.abbrev\n211\nInterventionBrowseBranchName\nbrowseBranches.name\n21\n"}, {"page": 22, "text": "F\nQA Evaluation Details\nJudge Model\nPrompt Template\nMacro-F1\nAcc.\nFPR\nFNR\nF1 (+)\nF1 (−)\nClaude 3.5 Sonnet\nSpan-level\n68.85\n77.83\n20.97\n22.38\n85.58\n52.13\nImplicit span-level\n70.24\n83.50\n45.16\n11.34\n90.10\n50.37\nResponse-level\n61.88\n83.25\n72.58\n6.69\n90.42\n33.33\nJSON\n56.04\n64.78\n33.87\n35.47\n75.64\n36.44\nJSON (alt)\n55.37\n66.75\n46.77\n30.81\n77.91\n32.84\nJSON w. double-check\n49.50\n54.68\n25.81\n48.84\n65.67\n33.33\nSimpleQA template\n55.39\n85.22\n88.71\n1.45\n91.87\n18.92\nGemini 1.5 Pro\nSpan-level\n55.84\n79.31\n79.03\n10.17\n88.03\n23.64\nImplicit span-level\n56.66\n85.47\n87.10\n1.45\n91.99\n21.33\nResponse-level\n48.82\n82.02\n95.16\n4.07\n90.04\n7.59\nJSON\n71.47\n86.95\n56.45\n5.23\n92.48\n50.47\nJSON (alt)\n66.03\n85.96\n69.35\n4.07\n92.05\n40.00\nJSON w. double-check\n64.89\n76.35\n37.10\n21.22\n84.95\n44.83\nSimpleQA template\n51.54\n84.73\n93.55\n1.16\n91.64\n11.43\nGPT-4o\nSpan-level\n63.08\n81.53\n64.52\n10.17\n89.18\n36.97\nImplicit span-level\n55.43\n83.99\n87.10\n3.20\n91.11\n19.75\nResponse-level\n51.54\n84.73\n93.55\n1.16\n91.64\n11.43\nJSON\n69.68\n80.54\n32.26\n17.15\n87.83\n51.53\nJSON (alt)\n66.78\n82.02\n53.23\n11.63\n89.28\n44.27\nJSON w. double-check\n57.62\n64.04\n17.74\n39.24\n74.11\n41.13\nSimpleQA template\n47.04\n83.74\n98.39\n1.45\n91.13\n2.94\nTable 15: Evaluation of judge models and prompt templates, reproduced from the FACTS Grounding Benchmark\npaper (Table 2).\n22\n"}, {"page": 23, "text": "G\nUser Interface - Search Page\nFigure 4: Search panel\nFigure 5: Advanced search panel\n23\n"}, {"page": 24, "text": "Figure 6: Search results\nFigure 7: Filtering sidebar\n24\n"}, {"page": 25, "text": "Figure 8: Preview sidebar\nFigure 9: Eligibility check results\n25\n"}, {"page": 26, "text": "Figure 10: AI Insights\nH\nUser Interface - Detail Page\nFigure 11: Detail page\n26\n"}, {"page": 27, "text": "Figure 12: QA evidence highlight\n27\n"}, {"page": 28, "text": "I\nSearch Query Generation Prompt Templates\nFigure 13: Search Query Generation - System Prompt\n28\n"}, {"page": 29, "text": "Figure 14: Search Query Generation - User Prompt\nJ\nInformation Extraction Prompt Templates\nFigure 15: Protocol Section - Identification Module\n29\n"}, {"page": 30, "text": "Figure 16: Protocol Section - Description Module, Conditions Module\nFigure 17: Protocol Section - Design Module\n30\n"}, {"page": 31, "text": "Figure 18: Protocol Section - Arms Interventions Module\nFigure 19: Protocol Section - Outcomes Module\n31\n"}, {"page": 32, "text": "Figure 20: Protocol Section - Eligibility Module\nFigure 21: Results Section - Participant Flow Module\n32\n"}, {"page": 33, "text": "Figure 22: Results Section - Baseline Characteristics Module\n33\n"}, {"page": 34, "text": "Figure 23: Results Section - Outcomes Measures Module 1\n34\n"}, {"page": 35, "text": "Figure 24: Results Section - Outcomes Measures Module 2\n35\n"}, {"page": 36, "text": "Figure 25: Results Section - Adverse Events Module\n36\n"}, {"page": 37, "text": "Figure 26: Results Section - More Info Module\n37\n"}, {"page": 38, "text": "Figure 27: Results Section - Condition Browse Module, Intervention Browse Module\n38\n"}]}