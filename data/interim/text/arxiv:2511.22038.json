{"doc_id": "arxiv:2511.22038", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.22038.pdf", "meta": {"doc_id": "arxiv:2511.22038", "source": "arxiv", "arxiv_id": "2511.22038", "title": "Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing", "authors": ["Rochana Chaturvedi", "Yue Zhou", "Andrew Boyd", "Brian T. Layden", "Mudassir Rashid", "Lu Cheng", "Ali Cinar", "Barbara Di Eugenio"], "published": "2025-11-27T02:47:30Z", "updated": "2025-11-27T02:47:30Z", "summary": "Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.22038v1", "url_pdf": "https://arxiv.org/pdf/2511.22038.pdf", "meta_path": "data/raw/arxiv/meta/2511.22038.json", "sha256": "c73f4890c994aa9f4cc2d43889ba56317efc8ece2ddfa0b68db8c14b300dd70c", "status": "ok", "fetched_at": "2026-02-18T02:26:02.125373+00:00"}, "pages": [{"page": 1, "text": "Early Risk Prediction with Temporally and Contextually Grounded\nClinical Language Processing\nRochana Chaturvedi*1, Yue Zhou2, Andrew Boyd2, Brian T. Layden2, Mudassir Rashid3,\nLu Cheng3, Ali Cinar3, Barbara Di Eugenio2\n1Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA\n2University of Illinois Chicago, Chicago, IL, USA\n3Illinois Institute of Technology, Chicago, IL, USA\nCorrespondence: rchaturvedi@anl.gov\nAbstract\nClinical notes in Electronic Health Records\n(EHRs) capture rich temporal information on\nevents, clinician reasoning, and lifestyle factors\noften missing from structured data. Leverag-\ning them for predictive modeling can be im-\npactful for timely identification of chronic dis-\neases. However, they present core natural lan-\nguage processing (NLP) challenges: long text,\nirregular event distribution, complex tempo-\nral dependencies, privacy constraints, and re-\nsource limitations. We present two comple-\nmentary methods for temporally and contextu-\nally grounded risk prediction from longitudinal\nnotes. First, we introduce HIT-GNN, a hier-\narchical temporal graph neural network that\nintegrates intra-note temporal event structures,\ninter-visit dynamics, and medical knowledge\nto model patient trajectories with fine-grained\ntemporal granularity. Second, we propose RE-\nVEAL, a lightweight, test-time framework that\ndistills the reasoning of large language models\ninto smaller verifier models. Applied to oppor-\ntunistic screening for Type 2 Diabetes (T2D)\nusing temporally realistic cohorts curated from\nprivate and public hospital corpora, HIT-GNN\nachieves the highest predictive accuracy— es-\npecially for near-term risk—while preserving\nprivacy and limiting reliance on large propri-\netary models. REVEAL enhances sensitivity\nto true T2D cases and retains explanatory rea-\nsoning. Our ablations confirm the value of\ntemporal structure and knowledge augmenta-\ntion, and fairness analysis shows HIT-GNN per-\nforms more equitably across subgroups.\n1\nIntroduction\nModeling disease progression represents one of\nthe most critical challenges in modern healthcare,\nwith particularly high stakes for chronic conditions\nlike type 2 diabetes (T2D), which affects half a bil-\nlion people worldwide and continues rising (IDF,\n*This work was conducted while the author was affiliated\nwith the University of Illinois Chicago.\n2025). The rich information in free-text clinical\nnotes available in EHRs provides an opportunity\nfor NLP predictive modelling. These notes present\ntwo natural perspectives for understanding disease\nprogression and suggest distinct yet complemen-\ntary computational approaches. First, the temporal\nprogression of symptoms, diagnoses, treatments,\nand interventions that are natural for understanding\nthe underlying disease pathology requires struc-\ntured modeling. Second, capturing the semantic\nrichness of clinical notes calls for contextualized\nlanguage understanding. These two perspectives\nare particularly relevant for opportunistic screening,\nwhere patients with low socioeconomic status often\nmiss routine care and timely screening (Danielson\net al., 2023). In such settings, clinicians must act\non all historical information available at a given\nvisit to estimate the disease risk. Formally, given\npatient documents d1, d2, . . . , dn observed at non-\ndecreasing sequence of patient visits 1, 2, . . . , n,\nwe want to estimate the risk at next visit (n + 1).\nTo address these complementary modeling\nneeds, this work proposes two novel methods that\nintegrate domain-specific modeling with targeted\nrepresentation learning techniques from a sequence\nof clinical notes, optimized for low-resource set-\ntings. To address structured temporal modeling,\nwe propose Hierarchical Temporal Graph Neural\nNetwork (HIT-GNN), a dynamic model that cap-\ntures patient state evolution with fine-grained tem-\nporal granularity and knowledge-enhanced struc-\nture. This fine-grained temporal knowledge is cru-\ncial for understanding disease pathways. For exam-\nple, elevated glucose after corticosteroid use likely\nindicates drug-induced hyperglycemia, not T2D,\nwhile the same glucose levels before any steroid\ntherapy may signal early T2D onset. This distinc-\ntion motivates our HIT-GNN framework, which\nextracts event-temporal graphs from individual clin-\nical notes using state-of-the-art methods, and aug-\nments them with semantic information from a clin-\n1\narXiv:2511.22038v1  [cs.CL]  27 Nov 2025\n"}, {"page": 2, "text": "ical knowledge base. These multi-layered event-\ntemporal graphs are modeled with dynamic graph\nneural networks (GNNs) to capture patient timeline\nwithin and across visits. While GNNs are effective\nfor relational reasoning, their performance depends\non rich contextual embeddings; thus, we incorpo-\nrate clinically pretrained language model (CPLM)\nembeddings and knowledge-graph embeddings to\nenhance input representations with rich semantics.\nThis addresses the long-recognized need to incor-\nporate causal and temporal patterns into diagnostic\nreasoning (Patil et al., 1981), moving beyond mod-\neling patient trajectories as sequences of visits.\nComplementing this structured approach, cur-\nrent large language models (LLMs) offer strong\nlocalized and context-aware reasoning capabilities\nthat can effectively model the underlying text se-\nmantics. However, they often struggle with long-\nrange context integration and structured reasoning—\nlimitations particularly pronounced in the clinical\ndomain, where patient histories span multiple long\nnotes. Additionally, real-world healthcare deploy-\nments face privacy and resource constraints, lim-\niting the use of proprietary or large-scale models.\nOur second method, Reasoning with Verifier-Aided\nLabeling (REVEAL), is a lightweight, scalable\ninference-time architecture that preserves LLM-\nstyle reasoning within resource-constrained clini-\ncal settings. It distills reasoning from a large LLM\ninto a smaller model, scaling performance with in-\nterpretability without the full computational cost of\ntraining or deploying massive models.\nTogether,\nthese\ncomplementary\nmodeling\nparadigms—graph-based temporal reasoning and\ninterpretable LLM inference—provide a holistic\nrisk prediction framework grounded in structure\nand semantics. We evaluate them in the context of\nopportunistic screening of T2D as a representative\nuse case, using a real private hospital (PH) corpus\nand a corpus curated from MIMIC-IV (Johnson\net al., 2023). Additionally, due to the prevalent\ndemographic biases (Meng et al., 2022; Zhou et al.,\n2025a; Hall et al., 2015), we conduct a fairness\nanalysis to better understand model behavior\nacross demographic groups.\nContributions. (1) We present two complemen-\ntary representation learning frameworks from lon-\ngitudinal clinical notes, tailored for low-resource,\nprivacy-sensitive settings: (i) HIT-GNN: the first\napplication of temporal relation extraction systems\nfor clinical risk prediction that integrates intra-\ndocument temporal relations, inter-visit dynamics,\nand medical knowledge, enabling reasoning across\nboth local event structures and longitudinal patient\ntrajectories. (ii) REVEAL: an inference-time scal-\ning framework where a smaller LLM validates pre-\ndictions from a larger frozen LLM, inheriting in-\nterpretability and improving accuracy without full\nretraining.\n(2) We demonstrate the translational value of tem-\nporally enriched representations in a real clinical\napplication—opportunistic screening for T2D, es-\npecially in immediate-risk horizons, where inter-\nvention is most impactful.\n(3) We curate rigorous datasets from public\n(MIMIC-IV) and private (PH corpus) sources, ex-\nclude post-diagnosis inputs to prevent label leakage\nand ensure balanced cohorts for fair evaluation.\n(4) Extensive ablations of graph architectures and\nembedding choices confirm value of fine-grained\ntemporal structure and knowledge augmentation.\n(5) Our fairness analysis highlights how clinical\ndata can encode bias even without explicitly model-\ning sensitive attributes, and discusses implications\nfor equitable model design.\nThese contributions enable a clinically grounded\nframework that integrates events, timelines, and\nsemantic context to enable low-cost, privacy-\nconscious decision support.\n2\nRelated Literature\nComputational Approaches for Disease Progres-\nsion Prediction\nDespite significant advances in\npredictive modeling using electronic health records\n(EHR), many studies continue to rely predomi-\nnantly on structured EHR data (Lipton et al., 2016;\nSinghal et al., 2023b; Jiang et al., 2024),which is\noften noisy (Hersh et al., 2013) and incomplete\n(Capurro et al., 2014). Among efforts that leverage\nfree-text, recent works either use only the last clini-\ncal note (Xu et al., 2023; Nguyen et al., 2024), re-\nlying on structured data to capture temporal trends;\nor use a coarser form of temporal modeling where\neach note is embedded using a language model\n(Huang et al., 2020), a bag-of-concepts (Chaturvedi\net al., 2023), or topics (Ghassemi et al., 2015),\nand a BiLSTM models the representations from\nmultiple notes across patient visits. While these\nstudies explore various fusion strategies, a key gap\nremains: no prior work models temporal relations\nboth within a single clinical note and across multi-\nple notes/visits to forecast long-term disease risk.\nOur work directly addresses this gap by: (1) lever-\n2\n"}, {"page": 3, "text": "aging the sequence of clinical notes for each pa-\ntient, and (2) introducing multi-layered modeling\nto leverage event-temporal information extracted\nfrom each note and across the notes from multiple\nvisits to track evolving health trajectories.\nLLM in Healthcare\nThe integration of Large\nLanguage Models (LLMs) into healthcare is a\nrapidly evolving field (Zhou et al., 2024a; He et al.,\n2024). Key developments include impressive per-\nformance on medical question answering (QA)\nbenchmarks (Singhal et al., 2023a). However, out-\nside of controlled QA settings, LLMs still struggle\non tasks requiring complex reasoning for disease di-\nagnosis in real clinical settings (Hager et al., 2024;\nWang et al., 2024b), and often exhibit performance\ndisparities across demographic groups (Zhou et al.,\n2025b). Recent research has demonstrated that\nscaling test-time compute through verifier-guided\nsearch can significantly improve LLMs’ reasoning\ncapabilities and often outperforms simply scaling\nmodel size (Snell et al., 2024). However, scal-\ning test-time compute of LLMs in low-resource\nreal-world healthcare tasks is still underexplored.\nOur work contributes to this literature by explor-\ning verifier-guided reasoning approaches tailored\nto low-resource clinical prediction task.\n3\nData Curation\nOur first dataset, the PH corpus, is curated from\nprivate data comprising adults (age ≥18 years) col-\nlected from the University of Illinois Hospital and\nHealth Sciences System (UI Health) between Jan-\nuary 2010 and July 2021. We also curate a second\ncorpus from Mimic-IV (Medical Information Mart\nfor Intensive Care, version 4) to study the general-\nizability of our methods. MIMIC-IV is a large, pub-\nlicly available database comprising de-identified\nclinical notes from ICU encounters at Beth Is-\nrael Deaconess Medical Center in Boston from\n2008–2019. We exclude diagnosis of other types\nof diabetes (e.g., Type-1 diabetes, gestational dia-\nbetes), and construct the T2D and the non-diabetic\n(Nod) cohorts using ICD codes—standardized di-\nagnostic codes used to classify medical conditions.\nCohort refinement steps ensure that post-\ndiagnosis data is excluded to prevent label leakage.\nWhile we use the date of the first recorded ICD\ncode as a proxy for the onset of type 2 diabetes\n(T2D), this signal is known to be noisy (Hersh\net al., 2013) and often delayed relative to when the\ndiagnosis is first documented in clinical notes. To\nmitigate this, we additionally process the T2D co-\nhort’s notes using a large language model (LLM) to\nidentify the earliest explicit mention of a T2D diag-\nnosis in unstructured text. This type of additional\nfiltering is often overlooked in existing works, yet\nit is crucial for ensuring realistic model evalua-\ntion. For example, Zhang et al. (2024) show that\na widely used sepsis prediction model frequently\nissues risk alerts only after clinicians have docu-\nmented suspicion, undermining its practical utility.\nFinally, we construct a demographically matched\ntest set for fair evaluation. This is achieved by es-\ntimating T2D propensity scores (Rosenbaum and\nRubin, 1985) from demographic variables, includ-\ning Age, Gender, and Race (and Ethnicity in the\ncase of the PH corpus, where this attribute is com-\nbined with Race in MIMIC-IV). This is followed\nby 1:1 greedy nearest-neighbor matching without\nreplacement to select NoD controls.1 The final\nPH corpus comprises 3332 patients (712 in the\ntest set), and the final MIMIC-IV subset contains\n5802 patients (291 in the test set). Both datasets\npose unique strengths and limitations—PH corpus\ncontains richer longitudinal histories with diverse\nnote types ideal for chronic disease modeling but\nit is not public. In contrast, while MIMIC-IV is\none of the most widely used, large public datasets,\nit only focuses on ICU notes, omitting much of\nthe hospitalization timeline, and has narrower tem-\nporal windows (84.5% patient records comprise\nsingle visits (Cui et al., 2025)). Further, MIMIC-IV\nnotes are de-identified, lacking an essential tempo-\nral anchor—DATE—considered protected health\ninformation (PHI).2 Figure 1 shows demographic\ndifferences: females are the majority in PH but a\nminority in MIMIC-IV; Black is the majority in PH,\nWhite in MIMIC-IV, with Asian as the minority. PH\ncorpus records Hispanics under a separate ethnicity\nattribute. Therefore, evaluating methods on both\ndatasets is important. However, given MIMIC-IV’s\nlimited temporal continuity, its results would likely\nrepresent a lower bound on model performance\nachievable in richer, real-world settings.\n4\nHIT-GNN: Disease Prediction using\nHierarchical Temporal Graphs\nThis section presents our Hierarchical Temporal\nGraph Neural Network (HIT-GNN) for modeling\n1The data filtering details are provided in Appendix A.\n2Health Insurance Portability and Accountability Act\n(HIPAA), a U.S. law, considers dates as PHI.\n3\n"}, {"page": 4, "text": "Figure 1: Demographic distribution in PH and MIMIC-IV test sets.\nfine-grained temporal information to predict T2D\nrisk. We extract and refine event-temporal graphs\nfrom clinical notes (sections 4.1–4.4) and detail our\nmodeling approach in section 4.5.\n4.1\nExtracting and Aligning Event-Temporal\nGraphs from Notes\nWe use state-of-the-art temporal relation extraction\nmodels from Chaturvedi et al. (2025) to identify\nclinical entities (Problem, Test, Treatment, Clin-\nical Department, Clinical Occurrences, Eviden-\ntial), time expressions (Date, Time, Duration, Fre-\nquency), and their temporal relations (before, after,\noverlap) from each clinical note, forming a tempo-\nral graph. We then perform entity normalization\nto cluster synonymous terms as a single entity and\nalso to link entities to an external knowledge graph.\n4.2\nEntity Linking/Normalization\nClinical Entities\nIf any event is of type Prob-\nlem/Treatment/Test/Clinical Department, we use\nMetamap (Aronson and Lang, 2010) to map the\nevent text to a unique concept in UMLS Metathe-\nsaurus (Bodenreider, 2004).3 UMLS comprises\na vast repository integrating synonymous medi-\ncal terms to unique concept identifiers (CUI) and\nalso defines various relationships among concepts,\nincluding hierarchical (e.g., is-a) and associative\n(e.g., part-of, related-to) links. Besides the metathe-\nsaurus, UMLS also consists of a Semantic Network.\nThis higher-level abstraction organizes UMLS con-\ncepts into broader semantic types, such as Disease\nor Syndrome, Pharmacologic Substance, Gene, etc.\nIt also defines permissible semantic relationships\nbetween semantic types, for example, Pharmaco-\nlogic Substance treats Disease or Syndrome, Find-\ning associated_with Disease or Syndrome. We\nrepresent these semantic types as additional nodes\n3We do not normalize other entity types, as it leads to noise,\nas per a manual inspection of 150 clinical notes.\nin the temporal graph and connect them to their cor-\nresponding entity nodes to preserve type hierarchy\nwith is-a relations. This results in a heterogeneous\ngraph that combines semantic relations based on\nmedical knowledge with the temporal sequence\ninformation determined from the note text.\nDates\nWe use Microsoft Recognizers Text\n(Huang et al., 2017) to normalize dates, and link\nmultiple mentions of same date together.\n4.3\nNode Representation\nTo represent a node, we compute its correspond-\ning textual span’s embedding by concatenating the\nin-context BioMedBERT (Gu et al., 2021) embed-\ndings of the first and last tokens with the span-\nwidth embeddings.4 We represent multiple linked\nmentions of an entity as a single node in each tem-\nporal graph, and initialize it with the mean of the\nembeddings of all linked mentions. To represent\nsemantic type nodes, we average BioMedBERT\nembeddings over all tokens in their label (preferred\nname). We also use knowledge graph embeddings\n(KG-embeddings) for Concept Unique Identifiers\n(CUIs) derived from the UMLS knowledge graph\nfor each linked event, as introduced by Maldonado\net al. (2019). The final node representations are\nconcatenated KG embeddings and text embeddings\n(BioMedBERT) over all mentions.\n4.4\nReduced Temporal Graph\nWe apply the Timegraph algorithm (Miller and\nSchubert, 1990) to obtain a reduced temporal graph.\nIt builds the graph one edge at a time, starting with\nthe most confident edges based on prediction prob-\nabilities and dropping inconsistent edges (that lead\nto a cycle). The final graph contains before, after,\nand overlap edges. We flip the after edges to be-\nfore for simplicity. The descriptive statistics of the\n4We use the fine-tuned model version and width embed-\ndings from Chaturvedi et al. (2025).\n4\n"}, {"page": 5, "text": "Note1\nNote2\nNote3\nBiLSTM\nBiLSTM\nBiLSTM\nEntity \nNormalization\nProcessing Notei\nGNN\nAggregation, \nNode Pooling\nGraph Embedding\nT2D/NoD\nFC \n+\nSigmoid\nDocument \nLevel\nTREx\nTimegraph\n(Reduced, \nconsistent graph)\nAugment\nSemantic \nNetwork\nBioMedBERT\nKG Embedding (pretrained)\nFigure 2: HIT-GNN Architecture: Hierarchical Temporal GNN that models intra- and inter-document temporal\ndependencies between clinical entities and integrates UMLS knowledge for type 2 diabetes (T2D) risk prediction.\nextracted graphs are provided in Table 1.5\nParameter\nPH-Data\nMIMIC-IV\n#Patients\n3332\n5802\nAvg. #tokens/doc\n1209.2 (637.6)\n454.5 (208.3)\nAvg. #nodes/doc\n114.7 (46.1)\n74.8 (27.9)\nAvg. #Edges/doc\n261.1 (133.4)\n158.2 (76.8)\n#doc or visits\n3.2(1.7)\n2.3 (1.4)\nTable 1: Data statistics: mean number of tokens per\ndocument, nodes, and edges in the extracted temporal\ngraph per document (doc), per patient, and the number\nof documents per patient. All statistics are averaged over\npatient IDs, with standard deviations in parentheses.\n4.5\nHierarchical Temporal Modeling\nWe model irregular time-series of temporal graphs\nacross patient visits using HIT-GNN for Type 2\nDiabetes prediction. Each visit is a graph Gt =\n(Vt, Et) with temporal (Before/After/Overlap) and\nsemantic (UMLS) edges. HIT-GNN captures intra-\ngraph and inter-graph dependencies via a GNN and\na recurrent neural network (RNN).\nIntra-visit Modeling. Each graph is encoded\nwith a multi-layer GraphSAGE encoder (with mean\naggregation, residual connections, and layer nor-\nmalization). Given initial features h(0)\nv\nof node v,\nN(v) the neighbors of v, and learnable parameters\nW(k)\n1 , W(k)\n2 , node updates at layer k are:\nh(k)\nv\n= LayerNorm\n\u0010\nW(k)\n1 h(k−1)\nv\n+ W(k)\n2\n1\n|N(v)|\nX\nu∈N(v)\nh(k−1)\nu\n\u0011\n+ h(k−1)\nv\n5The mean number of tokens to get estimated document\nlength is counted based on LLaMA3.1-8b tokenizer, as we\nalso include comparisons with this model.\nwhere h(k)\nv\nis the representation of node v at layer\nk. Updated node embeddings are mean-pooled to\nobtain the graph representation for a visit gt.\nInter-visit Modeling: temporal dependencies\nacross visits are modeled as:\nzt = BiLSTMϕ(g1, . . . , gt),\nzT = [−→\nh T ; ←−\nh 1]\nWhere g1, . . . , gt is the sequence of graph en-\ncodings over visits. The final forward and back-\nward hidden states from bidirectional LSTM\n(BiLSTM) are concatenated to represent a pa-\ntient’s trajectory. Finally, a fully connected net-\nwork with sigmoid (σ(·)) gives binary outcome\n(ˆy)—T2D or NoD (No Diabetes):\nˆy = σ(W · zT + b)\nFigure 2 shows the model architecture, integrat-\ning UMLS knowledge graph, document-level tem-\nporal information, and cross-visit progression.\n5\nReasoning with Verifier-Aided Labeling\n(REVEAL)\nHealthcare applications face significant resource\nconstraints, which limit the deployment of LLMs;\nyet, maintaining reasoning capabilities is crucial\nfor effective clinical decision-making. Recent ad-\nvances in test-time compute scaling offer a promis-\ning solution: using smaller models enhanced by\nverification mechanisms, rather than relying solely\non larger, computationally expensive ones. To this\nend, the REVEAL framework combines a reasoner\nLLM with a smaller verifier LLM, in three steps:\n(1) a reasoner LLM generates N reasoning paths\nwith predictions; (2) a fine-tuned verifier evaluates\nthe credibility of these paths and assigns a score\nfrom 0–1, and (3) predictions across different rea-\nsoning paths are aggregated using verifier scores.\n5\n"}, {"page": 6, "text": "Concretely, at training time, the reasoner model\ngenerates five stochastic outputs per example, each\nincluding a prediction (‘true’/‘false’) and an ex-\nplanation. A small LLM verifier, fine-tuned with\nLoRA (Hu et al., 2022), classifies each output as\n‘correct’ or ‘incorrect’ using the prompt in Figure\n8. The normalized probabilities of the output to-\nkens serve as the verifier’s confidence score. At\ninference time, the reasoner generates N (N = 10)\ndiverse predictions. Each prediction contains a risk\noutcome (‘true’/‘false’) and a corresponding ex-\nplanation. The verifier scores them, and the final\noutcome is selected by majority vote among the\ntop-k highest-confidence predictions.\n6\nExperiments\nThis section presents a comprehensive empirical\nevaluation of HIT-GNN and REVEAL. 6\n6.1\nBaseline Methods\nZero-shot Prompting: The model directly predicts\nT2D risk from clinical notes using deterministic de-\ncoding (temperature=0.0). We extract normalized\ntoken probabilities for ‘true’/‘false’ predictions to\ncompute AUC scores (see prompt in Figure 6. We\nuse LLaMA3.1-8b and LLaMA3.2-1, and also com-\npare the performance on MIMI:C-IV with prevail-\ning large-scale models, DeepSeek-V3 (671B pa-\nrameter Mixture-of-Experts model) and GPT-4o.7\nSelf-Consistency (LLaMA3.1-8B-SC): We gen-\nerate N (N = 10) diverse predictions per patient\nusing stochastic sampling (temperature=1.0) and\ntake the majority vote for the final classification.\nSupervised Fine-tuning (SFT): We fine-tune a\n1B LLM (given the low-resource setting) using\nLoRA for direct binary classification. The model\n(LLaMA3.2-1B-ft) is fine-tuned to output only\n‘true’ or ‘false’ labels without explanations due\nto lack of reasoning data (see prompt in Figure 7).\nCLSTM We also use concept-based CNN-LSTM\nbaseline (Chaturvedi et al., 2023). Here we model\nextracted entities (nodes of temporal graphs) with\na CNN and max pooling, then process each docu-\nment representation with an LSTM.\n6See Appendix B for prompts and experimental details.\nWe will publicly release our code and data curation scripts\nupon publication.\n7Not evaluated on PH corpus due to privacy constraints.\n6.2\nEvaluation Metrics\nWe report precision and recall for both groups, the\nmacro-average F1, and ROC-AUC scores.8 For\nfairness evaluation, we use the following metrics:\nDemographic Parity Difference (DPD)\n: differ-\nence in positive prediction rates between a target\ngroup (Z = z) and others (Z ̸= z).\nDPD = P( ˆY = 1 | Z = z) −P(ˆY = 1 | Z ̸= z)\nWhere ˆY is the predicted label (T2D = 1, NoD=\n0), and Z is a sensitive attribute (e.g., gender, race).\nEqual Opportunity Difference (EOD)\n: differ-\nence in true positive rates (TPR) between the tar-\nget group and others: TPRZ=z −TPRZ̸=z\nWhere TPRZ=z = P( ˆY = 1 | Y = 1, Z = z).\n7\nResults and Discussion\nTable 2 presents the main experimental results.9\nThe first panel comprises LLM-based models. Re-\ngarding zero-shot performance, the LLaMA3.2-1B\nmodel fails to identify patients at risk of diabetes,\nclassifying almost all patients into the ‘no-risk’\n(NoD) cohort. The performance improves as the\nmodel capacity increases to 8B variant. While\nself-consistency (SC) has shown promising per-\nformance in other tasks and domains, LLaMA3.1-\n8B-SC performs poorly on both datasets.\nThe\nLLaMA3.2-1B-ft (fine-tuned) model demonstrates\nstrong performance, achieving a ROC-AUC of\n65.06% on the PH corpus subset and 64.41%—the\nhighest among all models—on MIMIC-IV. While\nthis fine-tuned LLM excels as a classifier, it cannot\nprovide reasoning or explanations for its outputs.10\nOur REVEAL model does not significantly out-\nperform the LLaMA3.2-1B-ft model. However, it\npreserves the reasoning output and improves upon\nthe zero-shot 8B variant in terms of F1 and T2D\ngroup recall, highlighting its sensitivity in captur-\ning true positives for this subgroup. For larger mod-\nels, including DeepSeek-V3 with impressive clini-\ncal decision-making capabilities (Sandmann et al.,\n8ROC-AUC scores are computed from prediction proba-\nbilities. For LLMs, this is the probability of generating ‘true’\nat the predicted token position. For LLM-SC, it is the ‘true’\ntoken probability, averaged across all runs.\n9We also perform a pairwise comparison of the top four\nmodels using bootstrap sampling to estimate whether the dif-\nferences are statistically significant, based on non-parametric\nbootstrap resampling test (see Appendix Table C).\n10Obtaining gold-standard reasoning data at scale for fine-\ntuning is costly and labor-intensive, especially from already\noverburdened clinical experts, limiting the feasibility of fine-\ntuning with reasoning.\n6\n"}, {"page": 7, "text": "Model\nPH-Data\nMimic-IV\nAUC\nF1\nT2D\nNoD\nAUC\nF1\nT2D\nNoD\nP\nR\nP\nR\nP\nR\nP\nR\nLLaMA3.2-1B (0-shot)\n46.80\n35.46\n39.29\n3.09\n49.56\n95.22\n40.00\n33.10\n0\n0\n49.48\n1.00\nLLaMA3.1-8B (0-shot)\n57.73\n56.98\n57.49\n53.93\n56.61\n60.11\n53.71\n51.03\n61.54\n27.21\n52.65\n82.64\nDeepSeek-V3 (0-shot)\n-\n-\n-\n-\n-\n-\n55.02\n47.85\n69.23\n18.37\n52.38\n91.67\nGPT-4o (0-shot)\n-\n-\n-\n-\n-\n-\n57.75\n51.81\n75.56\n23.13\n54.07\n92.36\nLLaMA3.1-8B-SC\n55.62\n55.49\n56.29\n50.28\n55.08\n60.96\n52.12\n43.43\n66.67\n12.24\n51.14\n93.75\nLLaMA3.2-1B-ft\n65.06\n61.62\n59.53\n78.93\n68.75\n46.35\n64.41\n59.17\n63.89\n46.94\n57.38\n72.92\nREVEAL\n57.98\n65.24\n66.77\n60.96\n64.08\n69.66\n53.13\n51.60\n55.21\n36.05\n51.79\n70.14\nCLSTM\n68.24\n65.62\n63.19\n76.69\n70.36\n55.34\n63.08\n59.62\n63.96\n48.30\n57.78\n72.22\nHIT-GNN\n72.24\n67.28\n71.48\n58.43\n64.85\n76.69\n62.97\n62.88\n63.27\n63.27\n62.50\n62.50\n#patients\n712\n356\n356\n291\n147\n144\nTable 2: Performance of HIT-GNN and REVEAL against different baselines.\n2025), the gains are less pronounced: it achieves a\nsimilar AUC to the much smaller LLaMA3.1-8B,\nbut with lower F1 and T2D recall. GPT-4o attains\na higher AUC but exhibits worse T2D recall than\nboth LLaMA3.1-8B and REVEAL, and an overall\nweaker performance than LLaMA3.2-1B-ft.\nThe second panel summarizes results with struc-\ntured approaches. The CLSTM model provides a\nstrong baseline, significantly outperforming LLMs\nin AUC and T2D recall on the PH corpus and per-\nforming comparably on MIMIC-IV. HIT-GNN per-\nforms even better, attaining the highest AUC on the\nPH corpus and the highest T2D recall on MIMIC-\nIV, showing superior effectiveness in identifying\nat-risk patient. These improvements over other top-\nperforming models (REVEAL, LLaMA3.2-1B-ft,\nand CLSTM) are statistically significant. In con-\ntrast, no other model shows significant improve-\nments over HIT-GNN across key metrics (AUC,\nF1, and T2D recall) on either corpus. For example,\nwhile LLaMA3.2-1B-ft attains the highest AUC on\nMIMIC-IV, its advantage over CLSTM and HIT-\nGNN is not statistically significant. HIT-GNN also\noutperforms both the larger models (DeepSeek-V3\nand GPT-4o) across key metrics. We next present\nadditional analyses and ablation studies.\n7.1\nPrediction Horizons\nWe compare performance across prediction hori-\nzons by dividing the time from last pre-diagnosis\nvisit to diagnosis into 3-month windows, combin-\ning each T2D subgroup with all NoD controls to\ncompute AUC.11 The final window aggregates val-\nues above the 95th percentile to avoid sparsity. Re-\nsults (Figure 3) for the three best-performing mod-\nels (HIT-GNN, REVEAL, and LLaMA3.2-1B-ft)\nshow HIT-GNN performs more robustly across win-\n11Appendix D Figure 9 shows trends for T2D group recall.\ndows and outperforms LLMs in near-term predic-\ntion (≤1.5 years). This performance aligns with\nEHR realities where longitudinal completeness is\nrare and supports timely preventive care.\n7.2\nHIT-GNN Ablations\nNode Embeddings:\nFigure 4a shows HIT-GNN\nAUC with different node embeddings.12 Initializ-\ning the nodes with in-context BioMedBERT em-\nbeddings (‘Text-only’) outperforms CUI embed-\ndings (‘KG-only’), combining both further im-\nproves performance across datasets and metrics.\nSubgraph Variations:\nFigure 4b shows HIT-\nGNN AUC across input subgraphs.13\nThe\nfirst (‘Temporal’) captures the extracted temporal\ngraphs, the second (‘KG’) adds semantic nodes\nand edges from UMLS and removes the tempo-\nral (before, overlap) edges, and the last combines\nboth. While KG-graph is more important for the\nPH corpus; for MIMIC-IV, both are significant.\n7.3\nFairness Analysis\nTable 4 presents results on fairness metrics. In\nthe PH corpus, all models exhibit negative demo-\ngraphic parity (DPD) for Hispanics; this is more\npronounced with HIT-GNN, which also yields a\nslight negative equal opportunity difference (EOD)\n(−0.08). Across gender, HIT-GNN shows a low\nDPD bias against Females and no opportunity bias,\nwhile LLM models show moderate-to-high biases\nagainst Males. Across racial groups, all models are\nnegatively biased against the minorities (Unknown\n(NI) in the PH corpus and Asians in MIMIC-IV)and\nslightly favor the majority Black subgroup in the\nPH corpus. LLaMA3.2-1B-ft strongly favors this\ngroup in MIMIC-IV also. LLMs are somewhat\n12Appendix D, Figure 10 shows T2D recall.\n13Appendix D, Figure 11 shows T2D group recall trends.\n7\n"}, {"page": 8, "text": "Figure 3: AUC as a function of the prediction horizon, evaluated over consecutive 3-month windows.\n(a) Embedding ablations.\n(b) Subgraphs ablations.\nFigure 4: HIT-GNN performance ablations.\nnegative towards the White minority in the PH cor-\npus, while all models are slightly negative against\nthis group in MIMIC-IV, where it has a majority.\nHIT-GNN shows a high positive bias for Hispanics\nin MIMIC-IV, and LLMs show low-to-moderate\nbias. Overall, HIT-GNN is relatively fairer.\n7.4\nComputational Efficiency\nTable 3 summarizes the resource usage across mod-\nels (excluding GPT-4o and DeepSeek-V3, which\nare accessed via API: GPT-4o via the OpenAI\nAPI, and DeepSeek-V3 via the TogetherAI API).\nA single A100 GPU with 80 Gb RAM is used for\nall experiments (accelerated inference is used for\nLLaMA experiments).14 Training the HIT-GNN\n14Because HiTGNN relies on MetaMap’s single-threaded\nAPI for concept linking, preprocessing is not parallelized. This\none-time step—temporal graph extraction, UMLS linking,\nTimeGraph, and embeddings lookup—takes 42 seconds per\nPH note and 15.6 seconds per MIMIC-IV note.\nmodel takes approximately 1.5 minutes, and infer-\nence per patient takes .007 seconds. For CLSTM,\ntraining the model takes 12 minutes, and infer-\nence on one patient takes .02 seconds. For in-\nference with the LLaMA model using accelerated\ninference on a single A100 GPU, the average time\nrequired with explanations is approximately 6 sec-\nonds. Fine-tuning the model takes approximately\n18 hours, and inference with this version takes\n0.2 seconds. REVEAL verifier training takes 30\nhours (excluding training data preparation, which\ntakes approximately 30 seconds per patient for ob-\ntaining explanations from LLaMA3.1-8B along 5\nreasoning paths). Inference along 10 reasoning\npaths from this model takes 62 seconds (obtaining\nand verifying reasoning along 10 paths). The full\nLLaMA3.2-1B model requires 2.4 GB of memory,\nwhile LLaMA3.1-8B requires 15 GB. The LORA\nadaptors require an additional 20 MB. In contrast,\nCLSTM takes 54 MB and HIT-GNN is even lighter\nat 44 Mb, demonstrating impressive scalability in\nterms of both speed and memory usage.15\n8\nConclusion\nThis paper presents two complementary ap-\nproaches for clinical risk prediction from longi-\ntudinal notes: HIT-GNN, a temporally grounded\ndynamic GNN augmented with clinical knowledge\ngraphs, and REVEAL, a test-time LLM scaling\nframework for interpretable predictions. HIT-GNN\nachieves superior predictive performance and ro-\nbustness, especially on the immediate-risk hori-\n15Both the CLSTM and HIT-GNN models utilize the state-\nof-the-art, open-source end-to-end temporal relation extrac-\ntion models SPANTREX and GRAPHTREX from Chaturvedi\net al. (2025). Our reported results utilize SPANTREX, which\nrequires 0.44 GB of disk space. We also obtain similar perfor-\nmance using GRAPHTREX, which outperforms SPANTREX\non the temporal relation extraction benchmarks but not on our\ntask, and is larger at 4.35 GB.\n8\n"}, {"page": 9, "text": "Model\nTraining Time\nInference Time (per patient)\nMemory Usage\nInput\n(minutes)\n(seconds)\nHIT-GNN\n1.5\n0.007\n44 MB\n5 notes\nCLSTM\n12\n0.02\n54 MB\n5 notes\nLLaMA3.1-1B (with explanations)\npre-trained\n6\n2.4 GB\n2 notes\nLLaMA3.1-8B (with explanations)\npre-trained\n6\n15 GB\n2 notes\nLLaMA3.1-1B-ft (no explanations)\n18 hours\n0.2\n2.42 GB\n2 notes\nREVEAL (with explanations)\n30 hours\n62\n17.42 GB\n2 notes\nTable 3: Comparison of computational efficiency across models for PH–MIMIC-IV corpora. The temporal graph\nextraction and UMLS linking are one-time preprocessing costs incurred for HIT-GNN and CLSTM models. It takes\napproximately 48 seconds per note for the PH corpus and 15.6 seconds per note for MIMIC-IV. The table excludes\nthe LLM pretraining time and one-time data preparation costs for fine-tuning REVEAL model; the time shown\nis using 5 reasoning paths for training, and 10 for inference. LLMs perform best with the last two notes, while\nHIT-GNN and CLSTM perform best with the last five.\nPH corpus\nMIMIC-IV\nG\nGNN\nRVL\nft\nG\nGNN\nRVL\nft\nEthnicity\nDPD\nH\n-0.12\n-0.05\n-0.02\nN\n0.12\n0.05\n0.02\nEOD\nH\n-0.08\n0\n0.01\nN\n0.08\n0\n-0.01\nGender\nDPD\nF\n-0.04\n0.03\n0.03\nF\n-0.02\n0.04\n0.02\nM\n0.04\n-0.03\n-0.03\nM\n0.02\n-0.04\n-0.02\nEOD\nF\n0\n0.06\n0.06\nF\n0\n-0.10\n0.19\nM\n0\n-0.06\n-0.06\nM\n0\n0.10\n-0.19\nRace\nDPD\nB\n0.09\n0.10\n0.04\nB\n0.03\n0.03\n0.12\nW\n0\n-0.09\n-0.03\nW\n-0.05\n-0.03\n-0.04\nNI\n-0.11\n-0.06\n-0.03\nH\n0.21\n0.07\n0.02\nA\n-0.21\n-0.10\n-0.31\nEOD\nB\n0.08\n0.08\n0.06\nB\n-0.05\n-0.06\n0.16\nW\n0\n-0.09\n-0.03\nW\n-0.03\n-0.01\n-0.05\nNI\n-0.11\n-0.02\n-0.03\nH\n0.25\n0.15\n-0.05\nA\n-0.14\n-0.03\n-0.32\nTable 4: DPD/EOD for subgroups (G): H (Hispanic),\nN (non-Hispanic), F (Female), M (Male), B (Black),\nW (White), NI (Unknown), A (Asian). Models: GNN\n(HIT-GNN), RVL(REVEAL), ft (LLaMA3.2-1B-ft).\nzon, while remaining computationally efficient,\nwith ablations confirming the importance of both\nfine-grained temporal structure and external knowl-\nedge enrichment. REVEAL provides a promising\nbalance between performance and explainability\nthrough verified rationales. Notably, even large\nmodels like GPT-4o show limited zero-shot perfor-\nmance on this complex clinical task. Demographic\nfairness analysis indicates how clinical data can en-\ncode bias without explicit sensitive attribute mod-\neling. Our temporally realistic cohorts, created by\nfiltering post-diagnosis notes, avoid data leakage\noften overlooked in prior work.\nBy advancing beyond static health snapshots to\ndynamic patient representations, this work captures\nthe complexity of real-world clinical trajectories\nthrough event-centric modeling that offers low-\nresource, privacy-preserving alternatives adaptable\nto cross-institutional settings. Finally, our frame-\nwork could also be potentially extended to other\nchronic diseases, as well as to non-clinical domains\nthat involve longitudinal reasoning over rich tex-\ntual data, such as financial forecasting or conflict\nprediction using news reports.\n9\nEthics Statement\nUse of the PH corpus was approved by the insti-\ntutional review board (IRB) at the University of\nIllinois, and MIMIC-IV was reviewed by the IRB\nat Beth Israel Deaconess Medical Center. Both\ndatasets were obtained after completing the re-\nquired training and accessed through secure proto-\ncols in accordance with institutional and data use\npolicies (see https://physionet.org/content/\nmimiciv/1.0/ for MIMIC-IV).\nWhile our models achieve strong results, they\nare not intended to be used as standalone diagnostic\ntools and must be deployed with clinicians at the\ncenter. The necessary next steps are ongoing vali-\ndation with clinical collaborators, user studies, and\nexplainability audits. Future systems should incor-\nporate active learning loops that capture clinician\nfeedback and investigate bias mitigation to ensure\nequitable early detection across all subgroups.\n10\nLimitations\nWhile the PH corpus offers a more holistic and\nlongitudinal view of a patient’s clinical journey,\nit cannot be publicly shared due to privacy con-\nstraints. To promote reproducibility, we will make\nour data curation scripts and code publicly avail-\nable. Future work could address several key areas.\nFirst, mitigating demographic and institutional bi-\nases remains an important direction to ensure equi-\n9\n"}, {"page": 10, "text": "table model performance. Second, aligning graph-\nderived insights with LLM-generated rationales\nmay enhance interpretability and foster clinician\ntrust. Additionally, incorporating multimodal data\nby integrating text-extracted temporal graphs with\nstructured EHR and medical imaging could further\nenrich the representations. Cross-institutional adap-\ntation to handle diverse entity types and formats is\nanother promising direction.\nReferences\n2025. IDF Diabetes Atlas, 11 edition. International\nDiabetes Federation, Brussels, Belgium.\nAlan R Aronson and François-Michel Lang. 2010. An\noverview of metamap: historical perspective and re-\ncent advances. Journal of the American Medical\nInformatics Association, 17(3):229–236.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nDaniel Capurro, Erik van Eaton, Robert Black, and Peter\nTarczy-Hornoch. 2014. Availability of structured and\nunstructured clinical data for comparative effective-\nness research and quality improvement: a multisite\nassessment. EGEMS, 2(1).\nRochana Chaturvedi, Peyman Baghershahi, Sourav Me-\ndya, and Barbara Di Eugenio. 2025. Temporal rela-\ntion extraction in clinical texts: A span-based graph\ntransformer approach. In Proceedings of the 63rd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 25765–\n25788, Vienna, Austria. Association for Computa-\ntional Linguistics.\nRochana Chaturvedi, Mudassir Rashid, Brian T Layden,\nAndrew Boyd, Ali Cinar, and Barbara Di Eugenio.\n2023. Sequential representation of sparse hetero-\ngeneous data for diabetes risk prediction. In 2023\nIEEE International Conference on Bioinformatics\nand Biomedicine (BIBM), pages 831–834. IEEE.\nHejie Cui, Alyssa Unell, Bowen Chen, Jason Alan Fries,\nEmily Alsentzer, Sanmi Koyejo, and Nigam Shah.\n2025. Timer: Temporal instruction modeling and\nevaluation for longitudinal clinical records.\nKirstie K Danielson, Brett Rydzon, Milena Nicosia,\nAnjana Maheswaren, Yuval Eisenberg, Janet Lin,\nand Brian T Layden. 2023. Prevalence of undiag-\nnosed diabetes identified by a novel electronic med-\nical record diabetes screening program in an urban\nemergency department in the us. JAMA Network\nOpen, 6(1):e2253275–e2253275.\nMarzyeh Ghassemi, Marco A. F. Pimentel, Tristan Nau-\nmann, Thomas Brennan, David A. Clifton, Peter\nSzolovits, and Mengling Feng. 2015. A multivari-\nate timeseries modeling approach to severity of ill-\nness assessment and forecasting in icu with sparse,\nheterogeneous clinical data. In Proceedings of the\nTwenty-Ninth AAAI Conference on Artificial Intelli-\ngence, AAAI’15, page 446–453. AAAI Press.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nPaul Hager, Friederike Jungmann, Robbie Holland, Ku-\nnal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob\nVielhauer, Marcus Makowski, Rickmer Braren, Geor-\ngios Kaissis, and 1 others. 2024. Evaluation and\nmitigation of the limitations of large language mod-\nels in clinical decision-making. Nature medicine,\n30(9):2613–2622.\nWilliam J Hall, Mimi V Chapman, Kent M Lee, Yese-\nnia M Merino, Tainayah W Thomas, B Keith Payne,\nEugenia Eng, Steven H Day, and Tamera Coyne-\nBeasley. 2015.\nImplicit racial/ethnic bias among\nhealth care professionals and its influence on health\ncare outcomes: a systematic review. American jour-\nnal of public health, 105(12):e60–e76.\nKai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan,\nMengling Feng, and Erik Cambria. 2024. A survey\nof large language models for healthcare: from data,\ntechnology, and applications to accountability and\nethics. Preprint.\nWilliam R Hersh, Mark G Weiner, Peter J Embi, Ju-\ndith R Logan, Philip RO Payne, Elmer V Bern-\nstam, Harold P Lehmann, George Hripcsak, Tim-\nothy H Hartzog, James J Cimino, and 1 others. 2013.\nCaveats for the use of operational electronic health\nrecord data in comparative effectiveness research.\nMedical care, 51(8 0 3):S30–S37.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, and 1 others. 2022. Lora: Low-rank\nadaptation of large language models. ICLR, 1(2):3.\nKexin Huang, Abhishek Singh, Sitong Chen, Edward\nMoseley, Chih-Ying Deng, Naomi George, and\nCharolotta Lindvall. 2020. Clinical xlnet: Modeling\nsequential clinical notes and predicting prolonged me-\nchanical ventilation. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n94–100.\nWenhao Huang, Zijia Lin, Chris McConnell, and\nBörje F. Karlsson. 2017. Recognizers-Text: Recogni-\ntion and resolution of numbers, units, and date/time\nentities expressed across multiple languages.\nPengcheng Jiang, Cao Xiao, Adam Richard Cross, and\nJimeng Sun. 2024. Graphcare: Enhancing healthcare\npredictions with personalized knowledge graphs. In\nThe Twelfth International Conference on Learning\nRepresentations (ICLR).\n10\n"}, {"page": 11, "text": "Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin\nGayles, Ayad Shammout, Steven Horng, Tom J Pol-\nlard, Sicheng Hao, Benjamin Moody, Brian Gow, and\n1 others. 2023. Mimic-iv, a freely accessible elec-\ntronic health record dataset. Scientific data, 10(1):1.\nZachary Chase Lipton, David C. Kale, Charles Elkan,\nand Randall C. Wetzel. 2016. Learning to diagnose\nwith LSTM recurrent neural networks. In 4th Inter-\nnational Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings.\nRamon Maldonado, Meliha Yetisgen, and Sanda M\nHarabagiu. 2019. Adversarial learning of knowledge\nembeddings for the unified medical language system.\nAMIA Summits on Translational Science Proceedings,\n2019:543.\nChuizheng Meng, Loc Trinh, Nan Xu, James Enouen,\nand Yan Liu. 2022. Interpretability and fairness eval-\nuation of deep learning models on mimic-iv dataset.\nScientific Reports, 12(1):7166.\nStephanie A Miller and Lenhart K Schubert. 1990. Time\nrevisited 1. Computational Intelligence, 6(2):108–\n118.\nTuan Dung Nguyen, Thanh Trung Huynh, Minh Hieu\nPhan, Quoc Viet Hung Nguyen, and Phi Le Nguyen.\n2024. Carer-clinical reasoning-enhanced representa-\ntion for temporal health risk prediction. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 10392–10407.\nRamesh S Patil, Peter Szolovits, and William B\nSchwartz. 1981. Causal understanding of patient\nillness in medical diagnosis. In Computer-Assisted\nMedical Decision Making, pages 272–292. Springer.\nPaul R Rosenbaum and Donald B Rubin. 1985. Con-\nstructing a control group using multivariate matched\nsampling methods that incorporate the propensity\nscore. The American Statistician, 39(1):33–38.\nSarah Sandmann, Stefan Hegselmann, Michael Fujarski,\nLucas Bickmann, Benjamin Wild, Roland Eils, and\nJulian Varghese. 2025.\nBenchmark evaluation of\ndeepseek large language models in clinical decision-\nmaking. Nature Medicine, pages 1–1.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, and 1 others.\n2023a. Towards expert-level medical question an-\nswering with large language models. Preprint.\nPankhuri Singhal, Lindsay Guare, Colleen Morse, Anas-\ntasia Lucas, Marta Byrska-Bishop, Marie A Guer-\nraty, Dokyoon Kim, Marylyn D Ritchie, and Anurag\nVerma. 2023b. Detect: Feature extraction method\nfor disease trajectory modeling in electronic health\nrecords. AMIA Summits on Translational Science\nProceedings, 2023:487.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-\nmar. 2024. Scaling llm test-time compute optimally\ncan be more effective than scaling model parameters.\nPreprint, arXiv:2408.03314.\nGuanchu Wang, Junhao Ran, Ruixiang Tang, Chia-Yuan\nChang, Chia-Yuan Chang, Yu-Neng Chuang, Zirui\nLiu, Vladimir Braverman, Zhandong Liu, and Xia\nHu. 2024b. Assessing and enhancing large language\nmodels in rare disease question-answering. Preprint.\nYongxin Xu, Kai Yang, Chaohe Zhang, Peinie Zou,\nZhiyuan Wang, Hongxin Ding, Junfeng Zhao, Yasha\nWang, and Bing Xie. 2023.\nVecocare:\nVisit\nsequences-clinical notes joint learning for diagnosis\nprediction in healthcare data. In IJCAI, volume 23,\npages 4921–4929.\nShao Zhang, Jianing Yu, Xuhai Xu, Changchang Yin,\nYuxuan Lu, Bingsheng Yao, Melanie Tory, Lace M\nPadilla, Jeffrey Caterino, Ping Zhang, and 1 others.\n2024. Rethinking human-ai collaboration in complex\nmedical decision making: A case study in sepsis\ndiagnosis. In Proceedings of the CHI Conference on\nHuman Factors in Computing Systems, pages 1–18.\nHongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou,\nJinfa Huang, Jinge Wu, Yiru Li, Sam S Chen, Peilin\nZhou, Junling Liu, and 1 others. 2024a. A survey of\nlarge language models in medicine: Progress, appli-\ncation, and challenge. Preprint.\nYue Zhou, Barbara Di Eugenio, and Lu Cheng. 2025a.\nUnveiling performance challenges of large language\nmodels in low-resource healthcare: A demographic\nfairness perspective. In Proceedings of the 31st Inter-\nnational Conference on Computational Linguistics,\npages 7266–7278, Abu Dhabi, UAE. Association for\nComputational Linguistics.\nYue Zhou, Barbara Di Eugenio, and Lu Cheng. 2025b.\nUnveiling performance challenges of large language\nmodels in low-resource healthcare: A demographic\nfairness perspective. In Proceedings of the 31st Inter-\nnational Conference on Computational Linguistics,\npages 7266–7278, Abu Dhabi, UAE. Association for\nComputational Linguistics.\nA\nData Curation Details\nA.1\nPH Data\nWe show the data-preparation steps in Table 5. The\ninitial data is 164,910 patients, 15,140 of whom\nwere diagnosed with type 2 diabetes, and 3,708,168\nclinical notes. We exclude all post-diagnosis notes\n(these include notes after 3 days before the day\nof diagnosis). The 3-day pre-diagnosis window\nis considered to account for possible lag in trans-\nforming information from notes to ICD codes in\nstructured tables. This filter ensures we only con-\nsider pre-diagnosis information, leaving us with\naround 3 million notes. We then exclude the rare\n11\n"}, {"page": 12, "text": "demographic groups. These include race groups\nNative Hawaiian/Other Pacific Islander/American\nIndian/Alaska Natives/Asian, and unknown Gen-\nder/Ethnicity. We filter the data by note type and\nauthor, as it includes all providers and note types,\nsome of which are uninformative. This subset of\nnotes is determined with the help of a senior expert\nendocrinologist and is provided below:\nImportant Note Types\nInpatient Progress Note,\nProgress Notes, Family Medicine Note, History\nand Physical Note, Emergency Department Note,\nDischarge Summary, Nutrition Note, Endocrinol-\nogy Note, Specialty Pharmacy Services, General\nEye Note, Diet Instructions, ED Notes, Outpatient\nPharmacy Note, Discharge Note, Dialysis Round-\ning, Diabetes Education.\nImportant Author Types\nUnspecified, Anesthe-\nsiologist, Care Coordinator, Dentist, Dietician Stu-\ndent, Fellow, Licensed Clinical Social Worker, Li-\ncensed Practical Nurse, Medical Assistant, Medical\nStudent, Mental Health Counselor, Midwife, Nurse\nPractitioner, Nursing Student, Occupational Thera-\npist, Occupational Therapy Assistant, Occupational\nTherapy Student, Optometrist, Oral Surgeon, Phar-\nmacist, Physical Therapist, Physical Therapy Assis-\ntant, Physical Therapy Student, Physician, Physi-\ncian Assistant, Registered Dietitian.\nWe then remove notes shorter than 100 words,\nfollowing previous work in health-risk prediction\n(Ghassemi et al., 2015). Upon manual inspection\nof some of the notes in this filtered subset from the\npre-diagnosis date, we find that a diagnosis of T2D\nis already present but not entered in the structured\nrecords, or it is entered much later. As discussed\npreviously, such ICD-coding errors are common\n(Hersh et al., 2013). This motivated us to apply\nan additional filter using a popular large language\nmodel (LLM), LLaMA3.1-8B. LLMs are excep-\ntionally good at extracting explicit information al-\nready present in the text. We apply two different\nprompts (these are variations of the prompt in Fig-\nure 5), asking the model to identify if an explicit\ndiagnosis is already present in the note of the T2D\ncohort (due to scalability challenges, we do not\napply the LLM filter to the NoD cohort). Manual\ninspection of 50 samples reveals higher accuracy\nif the model answers yes to both prompts. We\nreadjust the earliest diagnosis date accordingly and\ndrop all notes on or after that date. This further\nreduces our T2D subset to 1717 patients. We split\nthis data into a training-testing split in an 80:20\nratio, stratified by the diagnosis group.\nFiltering Step\n#NoD\n#T2D\n#Notes\nInitial\n164,910\n15,140\n3,708,168\nPre-diagnosis notes\n164,910\n4,138\n3,146,140\nNon-rare demographics\n149,673\n3,878\n2,909,417\nImportant Note and Author-types\n108,635\n2,369\n1,451,626\nNote length\n106,222\n2,302\n1,050,839\nLLM for diagnosis date adjustment\n106,222\n1,717\n1,039,043\nTable 5: Data filtering steps with the number of non-\nDiabetic (NoD) and type 2 diabetes (T2D) patients, and\nthe number of notes after each step for the PH corpus.\nPrompt\nYou are a helpful assistant who can read and\nidentify required information from given\nclinical notes, either mentioned explicitly\nor in ICD code format.\nRequired information: a diagnosis of Dia-\nbetes for the given patient.\nPlease reply True if an explicit diagnosis is\nmentioned in the note.\nPlease answer False if:\n• there is no diagnosis,\n• the note negates the diagnosis (e.g.,\n“no diagnosis of diabetes” or “diabetes:\nnegative”),\n• if the diagnosis is associated with\nsomeone other than the patient (e.g.,\nfamily).\nDo\nnot\noutput\nanything\nother\nthan\nTrue/False.\nNOTE: [NOTE_TEXT]\nOutput:\nFigure 5: Prompt for identifying mention of type 2 dia-\nbetes in the initial set of pre-diagnosis notes identified\nbased on ICD codes.\nWe then apply propensity-weighted matching\nto construct a treatment (T2D) and control group\n(NoD) matched test sample (Rosenbaum and Ru-\nbin, 1985). We use the demographic attributes to\nmatch the covariates. For this, we first train a logis-\ntic regression model to estimate propensity scores\n(likelihood of being in the T2D group, given the\ndemographic attributes) (the model achieves an\nAUC score of 73.40%). We use a greedy nearest-\nneighbor 1:1 matching without replacement to con-\n12\n"}, {"page": 13, "text": "struct the final control group (NoD). For each\ntreated sample (T2D), we find an unmatched con-\ntrol (NoD) with the closest propensity score, ensur-\ning each control is used only once. Table 6 presents\nthe covariate balance before and after the matching.\nCovariate\nBefore\nAfter\nAGE\n0.683\n0.004\nBinarized GENDER (M=1)\n0.028\n0\nBinary RACE NI\n-0.057\n0\nBinarized RACE W\n-0.270\n0.005\nBinarized ETHNICITY (Y=1)\n0.002\n0\nTable 6: Covariate balance regarding standardized mean\ndifferences (SMD) before and after propensity score\nmatching for PH corpus test set.\nA.2\nMIMIC-IV T2D Data\nWe select the treatment group as the subset of\npatients with ICD code E11 (Type 2 diabetes)\namong the diagnoses. We exclude patients with\nICD codes from related diseases (E08-E13, O24)\nto exclude other types of diabetes.\nWe coar-\nsify race as Hispanic if Race is Hispanic/Latino,\nBLACK/AFRICAN AMERICAN if race is either\nBlack/African, and drop patients with race group\nlisted as ‘other’. Thereafter, we only retain pre-\ndiagnosis notes.\nMIMIC-IV notes contain dis-\ncharge summaries from which we retain “Chief\nComplaint”, “History of Present Illness”, and “Dis-\ncharge Instructions” sections and drop all patients\nwith more than 20 admissions following earlier\nworks. We then drop duplicated notes for a patient,\nand notes that are too short (<.05 quantile) and too\nlong (>0.95 quantile). Finally, we apply an LLM\nfilter on the T2D subgroup to correct the diagno-\nsis date. We use both LLaMA3.1-8B and GPT-4o\nand manual cleaning to construct a robust subset.\nFinally, we apply propensity-weighted matched\nsampling to get a balanced control group. The\ndata filtering steps are detailed in Table 7. The\nAUC score of the logistic regression model to esti-\nmate the propensity score is 63.64%. The covariate\nbalance before and after matching is presented in\nTable 8. The data is split into training and test in a\n90:10 ratio.\nFiltering Step\n#Notes\n#NoD\n#T2D\nInitial Data\n4,756,326\n180,640\nNo other types of diabetes\n4,578,799\n178,524\nrace not ‘other’\n4,033,399\n156,513\nAssign cohort\n4,033,399\n142,412\n14,101\npre-diagnosis notes\n3,220,793\n142,412\n5,952\nDrop if more than 20 admissions\n235,727\n59,566\n5,847\nNo Duplicates\n184,421\n54,803\n5,272\nNotes length in .05-.95 quantile\n175,201\n54,170\n5,207\nNo duplicates across sections\n174,995\n54,170\n5,207\nLLM + matched controls Filter\n14,957\n2,909\n2,909\nTable 7: Data filtering steps with the number of non-\nDiabetic (NoD) and type 2 diabetes (T2D) patients, and\nnumber of notes after each step for MIMIC-IV subset.\nCovariate\nBefore\nAfter\nAGE\n0.29\n0.001\nBinarized GENDER (M)\n0.12\n-0.001\nBinary RACE (B)\n0.316\n0\nBinarized RACE (H)\n0.110\n0.001\nBinarized RACE (W)\n-0.334\n0\nTable 8: Covariate balance in terms of standardized\nmean differences (SMD) before and after propensity\nscore matching on MIMIC-IV test set.\nB\nImplementation Details\nB.1\nPrompts\nFigures 6–8 present the prompts used for various\nLLM-based experiments.\nB.2\nExperimental Settings\nGNN\nFor GNN experiments, we use graphs from\nthe last five notes (we experimented with 1,2,5,10).\nWe train the models with 3-fold stratified cross-\nvalidation and save the best model over epochs on\neach fold. For inference on the held-out test set, we\ntake the average of prediction probabilities from all\nthree models and make the final prediction based\non that.16 SAGEConv operator from PyTorch Geo-\nmetric with default settings. We also experiment\nwith other GNN variants such as graph attention\nnetwork (GAT), relational graph convolutional op-\nerator (R-GCN), and heterogeneous GNN using\nheterogeneous graphs. None of these variants per-\nforms comparably. We experiment with 1−5 GNN\nlayers and find the best performance with k = 2\nlayers. For aggregating the nodes for graph repre-\nsentation, we also experiment with max pooling\nand pooling only the nodes corresponding to doc-\nument creation time (DCT/anchor nodes); neither\n16We also tried a usual training-validation split in an 80:20\nratio and get almost similar results.\n13\n"}, {"page": 14, "text": "System Prompt: Reasoner\nYou are a helpful medical assistant. Based on the\npatient’s information and the history of medical notes\nfrom one or more visits, assess if this patient has a\nlikelihood of being diagnosed with Type 2 diabetes\nin the near future.\nMake the assessment based on concrete medical evi-\ndence and how the patient’s condition has progressed.\nDo not solely rely on age as a risk factor.\n# Prediction Format:\n## Risk of Type 2 Diabetes: **[True/False]\n## Explanation: [Provide a brief explanation of the\nreasoning for the prediction.]\nUser Prompt\nCase History:\nPatient is {age} y.o.\n{race} {ethnicity}\n{gender}.\nNote Date: {date1}\n{note-type1} note\n{note-text1}\nNote Date: {date2}\n{note-type2} note\n{note-text2}\nNote: {ethnicity} and {note-type} are excluded\nfor the MIMIC-IV. {ethnicity} is also excluded for\nthe PH corpus if the value is ‘NI’.\nFigure 6: Prompts for inference from a reasoning model\nfor type 2 diabetes prediction.\nSystem Prompt: Fine-tuned Reasoner\nYou are a helpful medical assistant. Based on the\npatient’s information and the history of medical notes\nfrom one or more visits, assess if this patient has a\nlikelihood of being diagnosed with Type 2 diabetes\nin the near future.\nYou must respond with either:\n• true\n• false\nNo explanation is needed. Only output one of the two\nlabels above.\nFigure 7: System prompt used for fine-tuning and infer-\nence from a reasoning model for type 2 diabetes Risk\nPrediction. The User prompt is the same as in Figure 6.\nSystem Prompt: Fine-tuned Verifier\nYou are a medical reasoning assistant. Given a se-\nquence of clinical notes from a patient and an analy-\nsis predicting the risk of an imminent type 2 diabetes\n(T2D) diagnosis, judge if the analysis is correct or\nincorrect.\nYou must respond with either:\n• correct\n• incorrect\nNo explanation is needed. Only output one of the two\nlabels above.\nUser Prompt\nCase History:\nPatient is {age} y.o.\n{race} {ethnicity}\n{gender}.\nNote Date: {date1}\n{note-type1} note\n{note-text1}\nNote Date: {date2}\n{note-type2} note\n{note-text2}\nAnalysis\n{Reasoner Output}\nFigure 8: Prompts used for fine-tuning the verifier model\nin the REVEAL framework that tests if a type 2 diabetes\nrisk prediction model has made a correct prediction\nbased on given input and model reasoning.\n14\n"}, {"page": 15, "text": "works well. In place of an RNN to model multi-\ndocument graphs, we also perform simple attention-\nbased aggregation of cross-visit graph representa-\ntions and transformer-based aggregation, but both\nunderperform compared to BiLSTM.\nLLM\nDue to computational constraints, we only\nfine-tune a smaller LLaMA3.2-1B model instead of\nthe larger LLaMA3.1-8B variant, which exhausts\nmemory even with a batch size of 1 on an A100\n80GB GPU. For REVEAL, we use LLaMA3.1-8B\nas the zero-shot reasoner and LLaMA3.2-1B as\nthe fine-tuned verifier. We experimented with 1,\n2, 3, 5, 8, and 10 notes and found that including\nthe last two notes yields the best predictive perfor-\nmance. Since the LLM looks at detailed full-text\nnotes, longer context with additional notes causes\nit to underperform. On the other hand, since GNN\nlooks at a structured summary of the notes in the\nform of temporal graphs, it can incorporate addi-\ntional information effectively and gives the best\nperformance with five notes. We also evaluated the\neffect of demographic verbalization on LLM. We\nobserved a slight improvement in accuracy when\ndemographic details were included, while demo-\ngraphic integration reduces the performance of our\nGNN-based models.\nAll fine-tuning experiments are conducted on\nLLaMA3.2-1B using a batch size of 1 due to\nthe length of input sequences and memory limi-\ntations. We train the model for 30 epochs using the\nAdamW optimizer with a learning rate of 6e−06\nand a weight decay of 0.01.17 A linear learning\nrate schedule without warm-up is applied. Gradi-\nent accumulation is used with a factor of 2 to stabi-\nlize updates, given the small batch size. The loss\nfunction is standard cross-entropy with mean re-\nduction. In our setup, LoRA is applied to the query\nand value projection layers (q_proj and v_proj)\nwithin the transformer blocks. We use a rank of 8,\na LoRA scaling factor (alpha) of 16, and a dropout\nrate of 0.1 during training. This configuration re-\nsults in only a small fraction of the model parame-\nters being updated—approximately 0.07% (approx-\nimately 850K out of 1.24B)—making the training\nprocess efficient (31 minutes per epoch for PH cor-\npus and 32 minutes for MIMIC-IV) and tractable\non a single A100 80GB GPU while still allowing\nthe model to adapt to the binary classification task.\nIn the fine-tuning experiments, the pretrained\n17We experimented with learning rates of 5e−05 and 1e−06\nthat had lower performance.\nLLaMA3.2-1B model initially performs near\nchance level, with a balanced accuracy of 54.50%\non the validation set and 0.8% of outputs falling\noutside the valid label set. After fine-tuning, the\nbest model achieves an accuracy of 70.50% on\nthe validation set with no invalid predictions. On\nMIMIC-IV, the initial validation set accuracy is\nmuch lower at 44.00%, with 18.1% of predictions\nbeing invalid. This improves to 62.50% and zero\ninvalid predictions after fine-tuning.\nFor the REVEAL model, to ensure proper su-\npervision, we restrict training to examples where\nthe 8B reasoner produced both ‘true’ and ‘false’\npredictions across the five runs. This guarantees\nthat each case includes both valid and invalid rea-\nsoning paths, enabling the verifier to learn discrim-\ninative patterns. We have 5550 training examples\nfor the PH corpus and 9180 for MIMIC-IV. A 5%\nrandom held-out validation set stratified by the ‘cor-\nrect/incorrect’ label is reserved from each. After\neach epoch, the model is evaluated on the vali-\ndation set, and the best-performing model check-\npoint is saved. The verifier 1B model starts with\na base accuracy of 43.2% and 1% invalid predic-\ntions on the PH corpus, and the accuracy improves\nto 84.9% and no invalid predictions in 30 epochs.\nFor MIMIC-IV, the accuracy improves from 58.8%\ninitially to 67.5%.\nC\nStatistical Significance\nTo assess whether the observed differences in per-\nformance metrics presented in Table 2 are statisti-\ncally significant, we perform non-parametric boot-\nstrap resampling and compare each pair of the\ntop models—CLSTM, REVEAL, HIT-GNN, and\nLLaMA3.2-1B-ft. Specifically, for each metric\n(e.g., macro-F1, AUC, precision, recall), we repeat-\nedly sample subsets of the test set with replacement\nand compute the performance difference between\nmodel pairs over 10,000 bootstrap replicates. This\ngenerates an empirical distribution of differences\nfor each metric and model pair. From this distribu-\ntion, we computed the mean difference, the sample\nstandard deviation, and a 95% confidence interval\n(CI) using the 2.5th and 97.5th percentiles. Statis-\ntical significance is determined based on the pro-\nportion of bootstrapped differences falling below\nor above zero, yielding a two-tailed p-value. Sig-\nnificance levels are reported using the convention:\np<0.05 (*), p<0.01 (**), and p<0.001 (***). This\napproach makes no parametric assumptions and ro-\n15\n"}, {"page": 16, "text": "Model Pair\nPH Corpus\nMIMIC-IV\nAUC\nF1\nT2D\nNoD\nAUC\nF1\nT2D\nNoD\nP\nR\nP\nR\nP\nR\nP\nR\nReVeAL vs. LLaMA3.2-1B-ft\n-0.07*\n0.04\n0.07***\n-0.18***\n-0.05*\n0.23***\n-0.11*\n-0.08*\n-0.09\n-0.11*\n-0.06*\n-0.03\n(0.03)\n(0.02)\n(0.02)\n(0.03)\n(0.03)\n(0.03)\n(0.05)\n(0.04)\n(0.06)\n(0.06)\n(0.03)\n(0.05)\nCLSTM vs LLaMA3.2-1B-ft\n0.03\n0.04\n0.04**\n-0.02\n0.02\n0.09**\n-0.01\n0\n0\n0.01\n0\n-0.01\n(0.02)\n(0.02)\n(0.02)\n(0.03)\n(0.03)\n(0.03)\n(0.03)\n(0.01)\n(0.02)\n(0.01)\n(0.01)\n(0.02)\nCLSTM vs ReVeAL\n0.10***\n0\n-0.04\n0.16***\n0.06**\n-0.14***\n0.10\n0.08\n0.09\n0.12*\n0.06\n0.02\n(0.03)\n(0.02)\n(0.02)\n(0.03)\n(0.03)\n(0.03)\n(0.05)\n(0.04)\n(0.06)\n(0.06)\n(0.03)\n(0.05)\nReVeAL vs. HIT-GNN\n-0.14***\n-0.02\n0.02\n-0.16***\n-0.07**\n0.11***\n-0.10*\n-0.11**\n-0.08\n-0.27***\n-0.11***\n0.08\n(0.03)\n(0.02)\n(0.02)\n(0.03)\n(0.03)\n(0.03)\n(0.05)\n(0.04)\n(0.05)\n(0.05)\n(0.03)\n(0.05)\nLLaMA3.2-1B-ft vs. HIT-GNN\n-0.07***\n-0.06**\n-0.05**\n0.02\n-0.03\n-0.12***\n0.01\n-0.04\n0.01\n-0.16***\n-0.05 *\n0.10**\n(0.02)\n(0.02)\n(0.02)\n(0.03)\n(0.03)\n(0.03)\n(0.03)\n(0.03)\n(0.04)\n(0.04)\n(0.03)\n(0.04)\nCLSTM vs HIT-GNN\n-0.04*\n-0.02\n-0.02\n0.0\n-0.01\n-0.03\n0\n-0.03\n0.01\n-0.15***\n-0.05*\n0.10*\n(0.02)\n(0.02)\n(0.02)\n(0.02)\n(0.02)\n(0.03)\n(0.03)\n(0.03)\n(0.03)\n(0.04)\n(0.03)\n(0.04)\nTable 9: Pairwise model performance differences for top three models overall (first model minus second), based on\nbootstrap resampling test. Statistical significance: * p < 0.05, ** p < 0.01, *** p < 0.001.\nbustly quantifies uncertainty in model comparisons.\nThe results are summarized in Table 9.\nD\nAblation Results across T2D Group\nRecall\nFigure 9 presents T2D group recall trends across\nthe three best models (LLaMA3.2-1B-ft, REVEAL\nand HIT-GNN) with varying prediction windows.\nAgain, HIT-GNN performs either comparably (for\nPH data) or better (for MIMIC-IV) in the immedi-\nate prediction horizons.\nFigures 10 and 11 show consistent improvements\nin T2D group recall from both embeddings and\nboth subgraphs, respectively, across the two cor-\npora.\n16\n"}, {"page": 17, "text": "Figure 9: T2D recall as a function of the prediction horizon, evaluated over consecutive 3-month windows.\nFigure 10: T2D recall performance variation of HIT-\nGNN with different node embedding approaches.\nFigure 11: T2D recall performance variation by restrict-\ning to specific subgraphs. The KG model includes all\nnodes extracted from text, all KG nodes and associ-\nated edges, but no temporal edges. Temporal model\nincludes only the extracted temporal graphs (excluding\nKG-nodes and edges). The third contains both.\n17\n"}]}