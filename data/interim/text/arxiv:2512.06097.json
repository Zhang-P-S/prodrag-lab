{"doc_id": "arxiv:2512.06097", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.06097.pdf", "meta": {"doc_id": "arxiv:2512.06097", "source": "arxiv", "arxiv_id": "2512.06097", "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue", "authors": ["Emre Umucu", "Guillermina Solis", "Leon Garza", "Emilia Rivas", "Beatrice Lee", "Anantaa Kotal", "Aritran Piplai"], "published": "2025-12-05T19:04:28Z", "updated": "2025-12-05T19:04:28Z", "summary": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.06097v1", "url_pdf": "https://arxiv.org/pdf/2512.06097.pdf", "meta_path": "data/raw/arxiv/meta/2512.06097.json", "sha256": "609fda4424f5bc88060fad73cc526a92e848fa679002b398d3bebccd9d2ea996", "status": "ok", "fetched_at": "2026-02-18T02:25:10.771469+00:00"}, "pages": [{"page": 1, "text": "Empathy by Design: Aligning Large Language\nModels for Healthcare Dialogue\nEmre Umucu∗*, Guillermina Solis‡, Leon Garza†, Emilia Rivas†, Beatrice Lee§, Anantaa Kotal†, Aritran Piplai†\n∗Department of Public Health Sciences, The University of Texas at El Paso, USA\nEmail: eumucu@utep.edu\n‡Department of Nursing, The University of Texas at El Paso, USA\nEmail: gsolis2@utep.edu\n†Department of Computer Science, The University of Texas at El Paso, USA\nEmails: {lgarza3, erivas6}@miners.utep.edu, {akotal, apiplai}@utep.edu\n§Department of Rehabilitation Sciences, The University of Texas at El Paso, USA\nEmail: ylee6@utep.edu\nAbstract—General-purpose large language models (LLMs)\nhave demonstrated remarkable generative and reasoning capabil-\nities but remain limited in healthcare and caregiving applications\ndue to two key deficiencies: factual unreliability and a lack\nof empathetic communication. These shortcomings pose signif-\nicant risks in sensitive contexts where users, particularly non-\nprofessionals and caregivers, seek medically relevant guidance\nor emotional reassurance. To address these challenges, we intro-\nduce a Direct Preference Optimization (DPO)-based alignment\nframework designed to improve factual correctness, semantic co-\nherence, and human-centric qualities such as empathy, politeness,\nand simplicity in caregiver–patient dialogues. Our approach fine-\ntunes domain-adapted\nLarge Language Models (LLMs) using\npairwise preference data, where preferred responses reflect sup-\nportive and accessible communication styles while rejected ones\nrepresent prescriptive or overly technical tones. This direct opti-\nmization method aligns model outputs with human preferences\nmore efficiently than traditional reinforcement-learning-based\nalignment. Empirical evaluations across multiple open and pro-\nprietary LLMs show that our DPO-tuned models achieve higher\nsemantic alignment, improved factual accuracy, and stronger\nhuman-centric evaluation scores compared to baseline and com-\nmercial alternatives such as Google’s medical dialogue systems.\nThese improvements demonstrate that preference-based align-\nment offers a scalable and transparent pathway toward develop-\ning trustworthy, empathetic, and clinically informed AI assistants\nfor caregiver and healthcare communication. Our open-source\ncode is accessible at: https://github.com/LeonG19/Empathy-by-\nDesign\nIndex Terms—healthcare communication, large language mod-\nels, empathy in AI, caregiver support, human-centered artificial\nintelligence\nI. INTRODUCTION\nCaring for individuals with chronic or neuro-degenerative\nconditions such as Alzheimer’s disease and dementia requires\nnot only clinical coordination but also constant emotional\nresilience. Family caregivers and care partners often become\nthe primary interpreters of medical information, navigating\ncomplex treatment decisions, behavioral changes, and com-\nmunication challenges on a daily basis. While they cultivate\n* Corresponding authors\ndeep experiential knowledge of the individuals in their care,\nthey face uncertainty when medical guidance is inaccessible\nor overly technical. In such moments, caregivers increasingly\nturn to online resources and, notably, to LLMs for immediate,\nconversational support. LLMs have rapidly become integrated\ninto everyday life. They can explain complex ideas in plain\nlanguage, adjust to a user’s tone, and offer a sense of under-\nstanding that static websites cannot. For caregivers seeking\nclear, kind, and quick answers, these systems can feel like an\nalways-available companion in moments of doubt or stress.\nHowever, the growing reliance on generative models raises\nimportant concerns and potential risks [1]; particularly in\nhealthcare. Artificial Intelligence (AI) is increasingly embed-\nded in clinical practice, as seen in the number of FDA-\napproved medical AI devices rising from just 2 per year in\n2016, to 69 in 2022 [2], [3]. These tools undergo rigorous\nvalidation procedures to ensure accuracy, are deployed by\nhealthcare professionals, and are continually monitored and\nreported back to the FDA. In professional settings, such risks\nare carefully managed. Outside clinical settings, however, a\ngrowing number of individuals, such as caregivers, patients,\nand family members, are turning to online resources pow-\nered by LLMs to simplify complex terms, explore treatment\noptions, and, in some cases, even attempt self-diagnosis [4].\nThe challenge arises when these unregulated models are used\nfor medical guidance: without clinical oversight, they can\nproduce inaccurate, incomplete, or misleading information that\ninfluences real-world health decisions [5]. Thus the question\nremains, what happens when non-professionals, such as care-\ngivers, family members, or patients with chronic conditions\nlike dementia or Alzheimer’s, begin relying on tools that have\nnot been FDA-approved for their healthcare questions?\nLLMs are trained on vast amounts of data, much of which\nhas not been clinically validated [6]. While this may seem\nminor, the risk is significant: an Large Language Model (LLM)\ncould generate inaccurate, incomplete, or confusing responses,\nwhich in turn may lead to misdiagnosis, oversimplification of\ncomplex issues, or the spread of misinformation [2]. In a study\narXiv:2512.06097v1  [cs.CL]  5 Dec 2025\n"}, {"page": 2, "text": "Fig. 1: Overview of the proposed DPO fine-tuning workflow for healthcare language models. Web-scraped Alzheimer’s\nresources are transformed into paired question–answer examples, where preferred (A+) responses emphasize empathy and\nfactual accuracy, and rejected (A−) responses contain undesirable traits. Direct Preference Optimization (DPO) aligns the base\nmodel toward the preferred behavior, producing a fine-tuned LLM capable of compassionate and reliable caregiving dialogue.\nconducted by Nielsen et al. [7], a group of doctors evaluated\nGPT-4’s responses to medical questions using the Likert scale,\nassessing the accuracy, relevance, and depth. They concluded\nthat GPT-4’s answers lacked depth and were concerned that\nbecause it was trained on publicly available text, its answers\nwould potentially disseminate biases. In another study by Zada\net al. [8], LLMs were found to be correct in only 31% of\nscenarios when asked open-ended questions, mimicking real-\nworld self-diagnosis use cases. The authors concluded that\nthere is a critical need for a comprehensive self-diagnosis\ndataset to improve the performance of current LLMs, with\nthe goal of enhancing their reliability and potentially enabling\ntheir integration into future healthcare systems.\nBeyond factual accuracy, empathy plays a vital role in effec-\ntive healthcare communication. Research consistently shows\nthat empathetic communication improves patient trust, adher-\nence, and emotional outcomes, particularly in long-term or\nmemory-related conditions such as dementia [9]. Caregivers\noften seek not only correct information but also reassurance\nand emotional validation, elements that strongly influence how\nthey interpret and act on medical guidance [10]. When lan-\nguage models respond in a cold or overly clinical tone, users\nmay perceive them as dismissive or judgmental, which can in-\ncrease anxiety and reduce engagement [11], [12]. Conversely,\nresponses that acknowledge emotional context and convey\nunderstanding foster a sense of support and reliability, even\nwhen the advice remains non-prescriptive [13]. Therefore,\nan effective healthcare-oriented LLM must balance factual\nprecision with empathetic communication, ensuring that its\nguidance is both trustworthy and emotionally attuned to the\nneeds of caregivers and patients.\nTaken together, these concerns point to a broader chal-\nlenge: how can large language models communicate health-\nrelated information in ways that are both factually reliable\nand emotionally supportive? Our goal is to design a model that\ndelivers clear, unbiased, and easy-to-understand responses in a\nsupportive and friendly tone, supplying patients and caregivers\naccess to trustworthy guidance without over-complication or\nnegativity, while avoiding prescriptive medical advice.\nThis paper explores whether preference-based fine-tuning\ncan bridge this gap. Specifically, we investigate whether align-\ning models using Direct Preference Optimization (DPO) can\nenhance both the factual reliability and empathetic tone of\nresponses in healthcare-oriented dialogue. By focusing on how\nmodels express information, not just what they say, we aim\nto build systems that deliver accurate, clear, and emotionally\nattuned communication for caregivers and patients alike.\nOur contributions are threefold:\n• We develop and fine-tune three open-source LLMs that\nproduces responses emphasizing both factual accuracy\nand empathetic communication.\n• We conduct a comprehensive evaluation across custom-\n"}, {"page": 3, "text": "made semantic, factual, and human-centric metrics to\nmeasure the model’s improvement in reliability, readabil-\nity, and emotional alignment.\n• We release our trained models and implementation code\npublicly to support transparency, replication, and fur-\nther research (https://github.com/LeonG19/Empathy-by-\nDesign ).\nThrough these contributions, we aim to support the Geri-\natric Workforce Enhancement Program (GWEP) mission by\nadvancing culturally sensitive and age-friendly healthcare ed-\nucation. Aligned with the 4Ms framework, i.e., What Matters,\nMedications, Mentation, and Mobility, this model integrates\nAI-driven dialogue to reflect patient priorities and promote\nsafe, evidence-based care. By embedding AI in community\nhealth programs, GWEP sites can extend their reach, offering\nconsistent, compassionate, and accurate guidance to older\nadults and caregivers across underserved Hispanic/Latinx,\nAIAN, and veteran communities. Ultimately, this collabo-\nration merges human empathy with technological precision\nto strengthen caregiver communication, support workforce\ndevelopment, and enhance outcomes for aging populations\nnationwide.\nII. RELATED WORK\nLarge Language Models (LLMs) are transformer-based\nneural networks trained to predict the next token given a con-\ntext [14], [15]. These models are trained on trillions of tokens\nto more accurately resemble how a human would respond\ngiven a query [16]. LLMs now serve not just as predictive sys-\ntems but as conversational partners, knowledge assistants, and\nquestion-answering tools embedded in workflows. In Human\nComputer Interaction (HCI) contexts, LLMs support natural-\nlanguage interfaces by interpreting user input, maintaining\nconversational context, and generating coherent responses,\nreducing friction between human users and complex data sys-\ntems. Their ability to generate fluent, context-sensitive replies\nallows a wider range of users, including family caregivers\nand patients, to engage with machine-mediated information\nin a natural conversational format. A study reviewed human-\nevaluation methodologies for healthcare LLMs and found that\nconversational QA, patient education, and clinical decision\nsupport are among the most common interactive applications,\nunderlining the role of LLMs as dialogue-centric interfaces in\nclinical workflows [17].\nFine-Tuning a large language model involves adapting a\npre-trained model to a specific task or domain by updating a\nportion of its parameters using labeled examples. This process\nallows the model to retain the general linguistic and reason-\ning knowledge from pre-training while specializing in new\ndomains. Fine-tuning typically relies on supervised learning,\nwhere each prompt is paired with an ideal response that\nreflects the desired factuality, tone, and structure. In healthcare\nor caregiving contexts, fine-tuning helps align the model’s\nresponses with domain-specific communication norms—such\nas providing clear, empathetic, and supportive answers—while\navoiding irrelevant or overly technical details. The result is\na model that more accurately mirrors the communication\nexpectations of practitioners and caregivers within the domain.\n(citation required).\nDirect Preference Optimization (DPO) is a preference-\nbased fine-tuning approach designed to align language mod-\nels with human values without requiring a separate reward\nmodel or reinforcement learning loop. Instead, DPO directly\noptimizes the model’s parameters using pairwise preference\ndata—pairs of responses to the same prompt, where one is\nlabeled as preferred and the other as rejected. The DPO loss\nadjusts the model’s conditional probability distribution to in-\ncrease the likelihood of preferred responses relative to rejected\nones, while maintaining regularization through a reference\nmodel to prevent overfitting or distributional drift. This method\nsimplifies the traditional reinforcement-learning-from-human-\nfeedback (RLHF) pipeline while achieving similar alignment\nbenefits. In this study, DPO is used to refine stylistic and\nethical dimensions of the model’s output—promoting brevity,\nempathy, and factual accuracy in responses relevant to health-\ncare and caregiver support [18].\nMedical Knowledge + LLM Frameworks : Several recent\nsystems have sought to ground LLMs for caregiving, medical\nquestion answering (QA), or patient-facing support. Below we\nsummarize the most relevant prior work and highlight how\nour approach differs. Parmanto et al. [19] introduced CaLM,\na retrieval-augmented caregiving model built on LLaMA-\n2 7B fine-tuned for caregivers of Alzheimer’s and dementia\npatients. Their system combined a domain-specific knowledge\nbase with a Dense Passage Retriever (DPR) for context\nretrieval, followed by fine-tuning, showing consistent gains\nacross BLEU, ROUGE, and BERT-Score metrics. However,\nCaLM relies on a relatively small fine-tuning dataset and a\nstandard DPR retriever that struggles to capture fine-grained\nsemantic nuances such as negation or word order. Moreover,\nthe system does not explicitly optimize for readability, tone,\nor the avoidance of prescriptive medical advice. In contrast,\nour model leverages DPO on a proprietary caregiver-focused\nQA dataset constructed from dual-answer pairs (preferred\nversus rejected), enforcing friendliness, clarity, and accuracy in\nresponses. Other researchers have explored integrating LLMs\nwith medical knowledge graphs for clinical reasoning and\ndecision support [20], [21]. For example, the DR.KNOWS\nframework links UMLS-based (Unified Medical Language\nSystem) medical entities to LLM prompts for diagnostic\nreasoning, while SAGE uses KG integration to facilitate\nhealth information exploration and chronic condition manage-\nment. Although these approaches improve factual grounding\nand interpretability, they prioritize structured reasoning over\nconversational accessibility. Our system instead focuses on\nhuman-centered generation, emphasizing empathetic tone and\nsimplicity to better serve caregivers and non-experts. System-\natic reviews [22], [23] have highlighted the proliferation of\nhealthcare-specific RAG pipelines but also noted persistent\ngaps in evaluation frameworks addressing user accessibility,\ntone, and harm avoidance. Most prior systems emphasize\nfactual correctness and retrieval efficiency, whereas our work\n"}, {"page": 4, "text": "explicitly measures semantic similarity and readability to\nevaluate response quality for non-technical audiences.\nIn contrast to prior models that rely on standard RAG\nor KG-enhanced retrieval, our pipeline integrates retrieval,\nreranking, and fine-tuning through DPO with dual-answer\nhuman preference pairs. This allows the model to balance\nfactual reliability with empathetic communication, ensuring\naccessible, safe, and contextually grounded caregiver guid-\nance.\nIII. METHODOLOGY\nA. Task Definition\nModern large language models (LLMs) demonstrate re-\nmarkable generative capabilities for open-ended question an-\nswering. However, when applied directly in healthcare con-\ntexts, their responses may exhibit issues such as factual inac-\ncuracies, excessive technicality, or unsafe recommendations.\nTo address these challenges, we design a modular pipeline\nthat combines web-scraped healthcare data, LLM-based data\ngeneration, and preference-aligned fine-tuning using Direct\nPreference Optimization (DPO). Our goal is to balance factual\nreliability, accessibility, and empathetic communication in\nresponses tailored for caregivers and patients.\nB. Framework Overview\nOur\nframework\nintegrates\nstructured\ndataset\ncreation,\npreference-based optimization, and targeted evaluation to en-\nsure both reliability and human alignment. The pipeline com-\nprises four key stages (illustrated in Figure 1):\n1) Question–Answer Dataset Construction: We curate a\ndomain-specific QA dataset emphasizing clarity, empathy,\nand factual grounding across diverse caregiving scenarios.\nQuestions are collected from web-scraped caregiver fo-\nrums, educational resources, and verified medical sources.\n2) Preferred vs. Rejected Response Generation: For each\nquestion, multiple responses are generated using Llama\n3.1–8B–Lexi–Uncensored–V2. These are automatically\nclassified into preferred (accurate, empathetic, and sim-\nple) and rejected (overly technical, dismissive, or pre-\nscriptive) examples. This pairwise structure forms the\nfoundation for preference optimization. The template for\nthe dataset creation is in the text box titled Generated\nAnswers Prompt Example.\n3) Direct Preference Optimization (DPO) Fine-Tuning:\nThe model is fine-tuned on these paired examples using\nDPO, which directly optimizes the conditional likelihood\nof producing preferred answers relative to rejected ones.\nThis step teaches the model to internalize both factual\naccuracy and supportive tone without needing explicit\nreward modeling.\n4) Evaluation with Custom Metrics: The fine-tuned model\nis later assessed along semantic, factual, and human-\ncentric dimensions. While these metrics are detailed in the\nevaluation section, the fine-tuning stage itself is designed\nto embed these qualities within the training objective.\nGenerated Answers Prompt Example\nInstruction:\nAnswer the following question in ONE single response that meets\nthese requirements:\n1) Use complex but accurate terminology.\n2) DO prescribe supplements, medication.\n3) Deliver the answer in a rude, cold tone.\n4) Keep the answer only 200 tokens long.\nThen answer the following question in ONE single response that\nmeets these requirements:\n1) Use simple but accurate terminology.\n2) Do NOT prescribe supplements, medication.\n3) Deliver the answer in a friendly, sympathetic tone.\n4) Keep the answer only 100 tokens long.\nQuestion (sample):\nHow can Alzheimer’s diagnosis disclosure be handled compas-\nsionately?\nC. Fine-Tuning with Direct Preference Optimization (DPO)\nTo further align the model with domain-specific require-\nments, we employ Direct Preference Optimization (DPO) as a\nfine-tuning strategy. This approach leverages paired examples\nof good and bad answers to explicitly encode user preferences\ninto the model. (Include figure for DPO tuning example)\nDPO is a lightweight, reward-free alternative to reinforce-\nment learning from human feedback (RLHF). It operates\ndirectly on pairs of preferred (y+) and rejected (y−) responses\nfor the same input prompt x, adjusting model parameters\nθ to increase the relative likelihood of generating preferred\nresponses. Formally, the optimization objective is defined as:\nlog pθ(y+|x)\npθ(y−|x) > β log pref(y+|x)\npref(y−|x),\nwhere pref denotes the reference (pre-trained) model and β\ncontrols the update strength. This formulation encourages the\nmodel to align with preferred examples while constraining\nexcessive deviation from the base distribution.\n1) Base Model Selection: We use the open-source Llama\n3.1-8B Instruct as the foundational model for this frame-\nwork, chosen for its balance of performance and effi-\nciency. It is ideal for edge devices and low-latency appli-\ncations due to reduced memory and compute demands.\n2) Preference Dataset Construction: For each question, we\ngenerate two responses using a LLM: a chosen answer\nthat is simple, short, empathetic, and avoids prescriptive\nmedical advice, and a rejected answer that is technical,\nprescriptive, or delivered in an unfriendly tone. The model\nis trained to prefer the positive variant.\n3) Parameter-Efficient\nFine-Tuning\n(PEFT):\nTo\nre-\nduce computational overhead, we incorporate Low-Rank\nAdaptation (LoRA) within the DPO pipeline, enabling\nefficient specialization while only retraining a small per-\ncentage of the model’s parameters.\n4) Alignment Objective: The Direct Preference Optimiza-\ntion (DPO) loss function operates by comparing pairs\nof responses—one labeled as preferred and the other as\n"}, {"page": 5, "text": "non-preferred—for the same input prompt. It adjusts the\nmodel’s parameters so that the probability of generating\nthe preferred response increases relative to the non-\npreferred one, while a reference model constrains the fine-\ntuned policy to remain close to the original distribution.\nThrough repeated optimization over many such pairs, the\nmodel learns to internalize the stylistic and behavioral\nqualities represented in the preferred examples. In this\nwork, the preferred responses emphasize brevity, clarity,\nand empathy, guiding the model toward producing short,\nsimple, and supportive answers while avoiding prescrip-\ntive or overly technical explanations. The template for the\nDirect Preference Optimization tuning process is provided\nin the text box titled DPO-Tuning Example.\nDPO-Tuning Example\nInstruction:\n”You are a careful, evidence-based clinical assistant specialized\nin healthcare, geriatrics, dementia, and Alzheimer’s disease. Start\nyour answer with a brief, compassionate one-line sentence ac-\nknowledging the user’s concern, then provide a clear, simple, and\nfriendly explanation. Do not invent facts. Be polite and kind.”\nQuestion:\nWhat methods can improve caregiver confidence in managing\nbehavioral symptoms?\nChosen:\nI totally get it - caring for someone with behavioral symptoms can\nbe overwhelming. Here are some tips to boost your confidence...\nRejected:\nI suppose I can offer some advice. First off, you should familiarize\nyourself with the principles of...\nD. Empathetic and Factually Correct LLMs\nA central design objective is to ensure that the model’s\ngeneration behavior reflects both factual correctness and em-\npathetic communication. These qualities are directly encoded\nin the DPO fine-tuning process rather than imposed through\npost-hoc filtering.\nOperationalizing Empathy and Factuality. Each question\nin the curated dataset is paired with two answers that ex-\nemplify opposite attributes. Preferred responses demonstrate\nfactual precision, plain language, and emotional sensitivity;\nrejected responses contain technical jargon, unfriendly tone,\nor unsupported statements. The DPO loss function learns\nthese distinctions implicitly through comparative likelihoods,\nreinforcing desirable stylistic and factual features.\nImplicit Enforcement of Desired Attributes. Empathy\nand factual correctness emerge as part of the optimization\ndynamics: factuality is strengthened by penalizing inconsistent\nor contradictory content, while empathy is reinforced by\nincreasing the probability of supportive, user-aware phrasing.\nThis mechanism embeds human communication norms into\nthe model’s parameters, aligning generation with caregiver\nexpectations.\nIV. EVALUATION\nThe objective of this study is to evaluate and compare the\nperformance of several large language models (LLMs) and\ntheir preference-optimized counterparts for healthcare question\nanswering (QA). This evaluation seeks to understand how\nDirect Preference Optimization (DPO) fine-tuning influences\nmodel behavior, particularly in producing responses that are\nempathetic, clear, and clinically appropriate for caregiver-\noriented communication.\nWe assess the models across three primary evaluation fronts:\nsemantic accuracy, factuality, and human-centric metrics. Se-\nmantic accuracy measures the degree to which generated\nanswers align meaningfully with ground-truth references and\nmaintain coherence with the original question intent. Fac-\ntuality evaluates whether responses are consistent with es-\ntablished medical knowledge and free from hallucinated or\nmisleading information. Finally, human-centric metrics cap-\nture the qualitative and interpersonal aspects of communica-\ntion—specifically empathy, simplicity, and formality—which\nare essential in ensuring that model outputs are both under-\nstandable and emotionally attuned to caregiver needs. To-\ngether, these criteria form a comprehensive framework for\nassessing both the technical and communicative quality of\nLLM-generated healthcare dialogue.\nWe evaluate the following two model configurations:\n• Base LLMs: Llama 3.1-8B, DeepSeek-R1-Distill-Qwen-\n7B, and Mistral-7B-v0.3 evaluated in their instruction-\ntuned form, representing the general capabilities of cur-\nrent open models.\n• LLMs + DPO: The same models fine-tuned using pref-\nerence data emphasizing empathy, simplicity, and non-\nprescriptive tone, aiming to align output style and content\nwith caregiver communication standards.\nThrough this comparative analysis, we aim to uncover how\npreference optimization affects both the factual reliability\nand emotional intelligence of model responses. By jointly\nanalyzing semantic, factual, and human-centric dimensions,\nthis study provides a nuanced understanding of how DPO\nfine-tuning can enhance the trustworthiness, accessibility, and\ncaregiver alignment of LLM-powered healthcare assistants.\nA. Experimental Setup\n• Supervision Format: For generative models such as\nLlama 3.1, we followed an instruction–response format\nconsistent with the QA task. Prompts were framed as\ncaregiver queries, and the outputs were short, supportive\nanswers aligned with DPO constraints.\n• Optimization: We used AdamW with linear warmup and\ncosine learning rate decay. Parameter-efficient fine-tuning\n(PEFT) was applied using LoRA adapters, updating only\na small subset of weights. Hyperparameters were tuned\nvia grid search, with early stopping based on validation\nloss and semantic alignment metrics.\n• Infrastructure: Training and evaluation were performed\non NVIDIA RTX 6000 GPUs with 48GB of memory.\n"}, {"page": 6, "text": "Moreover, quantization is used to reduce memory usage\nand latency with a negligible decline in model perfor-\nmance.\nB. Datasets\nThe datasets used in this study were developed to evalu-\nate factual reliability, semantic alignment, and human-centric\nquality within caregiver-focused dialogue systems. The testing\ncorpus consists of 500 question–answer (QA) pairs generated\nby large language models (LLMs) based on verified, peer-\nreviewed online sources specializing in geriatrics and care-\ngiving (we have to include citation here). These QA pairs\nwere curated to reflect realistic caregiver inquiries and ensure\ncoverage of diverse healthcare topics, emotional contexts, and\ncommunicative tones.\nFor model alignment through Direct Preference Optimiza-\ntion (DPO), we constructed a separate dataset comprising\n1,000 base QA pairs, also derived from the same peer-\nreviewed caregiving sources [citation]. Each base QA pair\nwas provided as input to an LLM to generate two candidate\nresponses per question: one labeled as chosen (preferred)\nand the other as rejected (non-preferred), following criteria\nemphasizing empathy, clarity, and factual soundness. It is\nimportant to mention that, the questions and answers used\nfor testing are disjoint from those used in the DPO training\ndataset, ensuring an unbiased evaluation of generalization and\nalignment performance.\nC. Semantic Evaluation\nThe main objective of this evaluation is to show that\ngenerated responses are semantically similar to the ground\ntruth. Our focus is on demonstrating that the outputs and\nreference answers convey the same meaning, even when\nphrased differently. To evaluate semantic alignment, we em-\nploy two complementary embedding models, each combined\nwith cosine similarity to quantify overlap in meaning.\nText Embedding Model: We generate embeddings for\nboth the model outputs and the ground truth using the text-\nembedding-ada-002 model. Cosine similarity between embed-\ndings measures the semantic alignment of generated responses\nwith the expected answers.\nSentence Transformer Model: or an additional perspec-\ntive, we use the all-mpnet-base-v2 sentence transformer to\nembed responses. Cosine similarity is again computed to cap-\nture alignment from a different embedding space. Both met-\nrics consistently assign higher scores to semantically faithful\nresponses and lower scores to misaligned answers. This dual\nevaluation approach ensures robustness and provides nuanced\ninsights into how well the proposed framework preserves\nmeaning in healthcare QA.\nD. Factual Evaluation Metrics\nTo conduct a rigorous assessment of factual correctness\nin the healthcare QA context, we apply three distinct yet\ncomplementary metrics. Each metric targets a different facet\nof factual alignment between a generated answer (candidate)\nand the reference (ground-truth) answer.\nG-Eval Correctness: We leverage the evaluation framework\nG-Eval, which uses an LLM as a judge to assess generated\nresponses based on a defined criterion. In our study, the\ncriterion was:\n“Determine whether the actual output is factually\ncorrect based on the expected output.”\nUnder this metric, the evaluator judges whether the candidate\nanswer is factually consistent with the reference, without\nconsideration for style or tone. This approach yields a coarse\nbut high-level indicator of factual soundness: it identifies\nresponses that are clearly incorrect or misleading with respect\nto the expected answer.\nNLI Consistency: To capture a finer-grained measure\nof\nfactual\nalignment,\nwe\nemploy\na\nzero-shot\nnatural\nlanguage inference (NLI) classifier. We adopt the model\nMoritzLaurer/deberta-v3-base-zeroshot-v2.0,\nwhich is designed to support universal classification tasks\nby reformulating them into an entailment vs. non-entailment\nformat. For each reference–candidate pair, the classifier\nestimates the degree to which the candidate is entailed by\nthe reference. We compute a continuous “NLI score” by\naveraging these entailment scores across all examples. This\nmetric allows us to detect subtle factual divergences such as\nomissions, distortions, or unsupported claims which might\nnot be flagged by binary correctness metrics.\nModified BERT-Score: Finally, we implement a modified\nversion of the BERT-Score metric adapted for factual correct-\nness and completeness rather than mere lexical overlap. In\nthis method, we prompt an LLM to critically evaluate how\nfactually consistent the candidate answer is with the reference\nanswer. The prompt directs the evaluator to ignore tone,\ngrammar or style, and to focus solely on factual correctness\nand completeness: awarding a numerical score between 0.0\nand 1.0 (in increments of 0.1) based on the degree of factual\nalignment, omissions, unsupported claims, or contradictions.\nHigh scores (≥0.9) are awarded only when every factual\nstatement in the candidate is supported by the reference.\nBy capturing both the presence and accuracy of facts, this\nmodified metric provides a more nuanced assessment of factual\ncompleteness.\nBy combining these three metrics—G-Eval Correctness,\nNLI Consistency and Modified BERT-Score—we span a spec-\ntrum from coarse binary judgment through entailment-based\nscoring to detailed prompt-based evaluation of factual com-\npleteness. Together, they provide a robust framework for eval-\nuating factual reliability of generated responses in caregiver-\noriented healthcare QA scenarios.\nE. Human-Centric Evaluation Metrics\nIn addition to technical correctness and semantic alignment,\nit is essential in caregiver-oriented healthcare QA to assess\nhow accessible and emotionally attuned the responses are.\n"}, {"page": 7, "text": "Fig. 2: Overview of the Modified BERT-Score evaluation process. Instead of measuring lexical overlap, this version prompts\na large language model to assess factual consistency between a candidate and reference answer. The evaluator ignores style\nor grammar and focuses on factual accuracy, completeness, and contradictions, assigning a score from 0.0 to 1.0 in 0.1 steps.\nScores ≥0.9 indicate full factual alignment. This prompt-based metric complements G-Eval Correctness and NLI Consistency\nto capture fine-grained factual reliability in caregiver-oriented healthcare QA.\nWe therefore incorporate two human-centric metrics: one\nmeasuring readability and simplicity, and the other measuring\nempathy in tone and engagement.\nReadability (Flesch–Kincaid Grade Level) To evaluate\nhow easily a model’s response can be understood by a lay\ncaregiver, we apply the widely-used Flesch–Kincaid Grade\nLevel (FKGL) metric. This formula estimates the U.S. school-\ngrade level required to comprehend a given text, based on\nfactors such as average sentence length and syllables per word.\nIn the context of healthcare QA for caregivers, readability\nis critical: overly complex or technical language can impede\ncomprehension, reduce trust, and increase the risk of mis-\nunderstanding. By quantitatively measuring grade-level read-\nability of generated responses, we can compare how different\nmodel configurations produce caregiver-friendly language and\nidentify whether fine-tuning improves accessibility.\nG-Empathic We also assess how well the model’s responses\nconvey empathy, support, and emotional awareness. For this\nwe adopt an evaluator configured with the criterion:\n“Assess how well the actual output demonstrates\nempathy toward the user’s message. An empathetic\nresponse should acknowledge the user’s perspec-\ntive or feelings, show understanding and emotional\nawareness...”\nThis metric is important in the caregiving context because\ncorrect medical information is necessary but not sufficient;\ncaregivers also need emotionally supportive, respectful and\ntrustworthy communication. Responses that feel impersonal or\noverly formal may undermine user engagement or perceived\nreliability—even when factually correct. By measuring empa-\nthy explicitly, we capture a dimension of model output that\naligns with the real-world expectations of caregiver users.\nOverall, these two human-centric metrics complement our\nprior semantic and factual metrics by ensuring that model\nresponses are not only accurate and aligned, but also accessible\nand emotionally appropriate for caregiving contexts.\nFormality/Polite Politeness is a preferable property in di-\nalogue models designed to handle sensitive and emotionally\ncharged topics, such as conversations related to Alzheimer’s\ndisease. In this context, we regard formality as a closely related\nand more objectively quantifiable proxy for politeness in\nwritten dialogue. To quantify the formality of model-generated\nresponses, we utilize the roberta-base-formality-ranker model,\nwhich is fine-tuned to classify English sentences, distinguish-\ning between formal and informal replies [24]. Given a sample\nresponse generated by our model, the classifier returns a label\n(formal or in-formal) and a score (a number between 0 and\n1). Assuming that we implement our techniques properly, we\nwould see an increase in formality from our base model’s\nresponses to the fine-tuned model’s replies.\nV. RESULTS AND ANALYSIS\nAcross all evaluation fronts, DPO fine-tuning consistently\nenhances the linguistic, factual, and emotional dimensions of\nhealthcare-oriented language models. As shown throughout\nthe following subsections, aligning models with human pref-\nerences fosters holistic improvement—bridging factual pre-\ncision, semantic alignment, and empathetic communication.\nCompared to their baseline and other adaptation strategies,\nDPO-aligned models achieve higher contextual fidelity, gen-\nerate more accurate and concise responses, and express infor-\nmation in a form that is accessible and emotionally attuned\nto non-professional caregivers. These improvements highlight\nthe potential of preference optimization to create language\nmodels that are not only knowledgeable but also supportive\nand relatable in healthcare communication scenarios.\nA. Semantic Alignment\nAs summarized in Figure 2, DPO-aligned models consis-\ntently outperform their baseline counterparts in semantic sim-\nilarity, demonstrating stronger contextual alignment between\ngenerated and reference responses. Across models, we observe\nan average increase of approximately 10% in embedding-\nbased similarity (SS:E) and 8% in sentence-level coherence\n(SS:T). For instance, the DPO-tuned Llama3.1-8B improves\nfrom 0.793 to 0.834 in SS:E and from 0.798 to 0.847 in SS:T,\n"}, {"page": 8, "text": "while Mistral-7B-v0.3 shows similar gains (from 0.778 to\n0.821 in SS:E). Even the smaller DeepSeek-R1-Distill-Qwen-\n7B benefits notably, confirming that preference optimization\nimproves contextual embedding alignment regardless of model\nscale. These improvements reflect reduced semantic drift and\nmore faithful adherence to the reference meaning, suggesting\nthat DPO enables models to maintain topic relevance while\ngenerating fluent, domain-appropriate responses.\nTABLE I: Semantic Similarity Measures Across Categories\nand LLM Models. SS:E denotes semantic similarity computed\nusing the text-embedding-ada-002 model, and SS:T represents\nsemantic similarity calculated with the all-mpnet-base-v2 sen-\ntence transformer. Higher scores indicate stronger align-\nment between generated answers and their corresponding\nreference responses\nModel\nSS:E\nSS:T\nBaseline\nLlama3.1-8B\n0.793\n0.798\nDeepSeek-R1-Distill-Qwen-7B\n0.664\n0.698\nMistral-7B-v0.3\n0.778\n0.770\nDPO-Tuned\nLlama3.1-8B\n0.834\n0.847\nDeepSeek-R1-Distill-Qwen-7B\n0.732\n0.714\nMistral-7B-v0.3\n0.821\n0.847\nB. Factual Consistency\nIn terms of factual correctness, presented in Table 2,\nDPO-tuned models achieve consistently higher scores across\nall factuality metrics—G-Eval, NLI Consistency, and Modi-\nfied BERT-Score. Compared to baseline versions, DPO fine-\ntuning improves factual consistency by approximately 7–10%\non average. For example, Llama3.1-8B-DPO improves from\n0.730 to 0.782 in G-Eval correctness and from 0.801 to\n0.844 in NLI entailment. Similarly, Mistral-7B-DPO achieves\nthe highest overall NLI consistency (0.874), demonstrating\nthe model’s strengthened ability to produce logically coher-\nent and evidence-aligned outputs. These results confirm that\naligning generation through human preference signals can\nreinforce factual accuracy without explicit factual supervision.\nNotably, even models with smaller parameter counts such\nas DeepSeek-Qwen-DPO exhibit substantial improvements,\nsuggesting that preference-based fine-tuning enhances factual\ngrounding across architectures.\nC. Human-Centric Evaluation\nTable 3 highlights human-centric metrics—empathy, read-\nability, and formality—where DPO-tuned models show clear\ngains in affective quality and accessibility. For instance,\nLlama3.1-8B-DPO improves empathy from 0.71 to 0.83, re-\nflecting more emotionally attuned and supportive responses.\nMeanwhile, readability (measured by the Flesch–Kincaid\nModel\nG-Eval\nNLI\nMod-BERT\nBaseline\nLlama3.1-8B\n0.730\n0.801\n0.801\nDeepSeek-R1-Distill-Qwen-7B\n0.522\n0.782\n0.634\nMistral-7B-v0.3\n0.706\n0.793\n0.838\nDPO-Tuned\nLlama3.1-8B\n0.782\n0.844\n0.816\nDeepSeek-R1-Distill-Qwen-7B\n0.720\n0.821\n0.792\nMistral-7B-v0.3\n0.755\n0.874\n0.847\nTABLE II: Factual Correctness Evaluation Across Baseline\nand DPO-Tuned Models. This table reports model performance\nusing three factuality metrics: G-Eval Correctness, which\nassesses factual consistency via LLM-based evaluation. NLI\nConsistency, which measures entailment between generated\nand reference statements using a zero-shot DeBERTa-based\nclassifier, and Modified BERT-Score, which quantifies factual\ncompleteness and correctness through a tertiary LLM eval-\nuation. Higher scores for all metrics indicate a higher\ncorrectness score.\nTABLE III: Human-Centric Evaluation Metrics for Caregiver-\nOriented Dialogue Generation. This table presents results for\nthree complementary measures: Empathy, assessed through\nan LLM-based evaluator. Readability, measured via the\nFlesch–Kincaid Grade Level (FKGL) to determine linguis-\ntic accessibility. Formality, evaluated using the roberta-base-\nformality-ranker model to quantify politeness and conver-\nsational appropriateness. Higher empathy and formality\nscores, combined with lower FKGL values, indicate re-\nsponses that are emotionally attuned, polite, and easier to\nunderstand.\nModel\nFK-GL\nG-Empathic\nFormal\nBaseline\nLlama3.1-8B\n12.654\n0.696\n0.8781\nDeepSeek-R1-Distill-Qwen-7B\n10.372\n0.522\n0.918\nMistral-7B-v0.3\n11.925\n0.707\n0.964\nDPO-Tuned\nLlama3.1-8B\n11.975\n0.725\n0.887\nDeepSeek-R1-Distill-Qwen-7B\n9.30\n0.720\n0.976\nMistral-7B-v0.3\n9.34\n0.756\n0.856\nGrade Level) decreases from 12.65 to 11.97, implying sim-\npler and more accessible language, better suited for non-\nprofessional caregivers. Formality scores also increase across\nmodels, indicating smoother and more respectful conversa-\ntional tone—essential for trust and rapport in caregiving\ncontexts. Taken together, these findings demonstrate that DPO\ntuning not only enhances factual reliability but also strengthens\nthe communicative sensitivity of the model, enabling it to\nbalance empathy and professionalism effectively.\n"}, {"page": 9, "text": "D. Cross-Adaptation Benchmark Comparison\nTo better contextualize our findings, Figure 5 bench-\nmarks our DPO-tuned Llama3.1-8B against other adaptation\nstrategies including SFT (Supervised Fine-Tuning) and RAG\n(Retrieval-Augmented Generation). While RAG introduces\nfactual improvements through retrieval integration, its per-\nformance on readability (FKGL = 13.64) suggests higher\nlinguistic complexity—potentially hindering accessibility for\nlay users. Conversely, DPO yields more balanced results,\nachieving the highest semantic alignment (SS:E = 0.834) and\nfactual correctness (G-Eval = 0.782) while maintaining simpler\nand more empathetic phrasing (FKGL = 11.97). SFT models\n(values omitted for brevity) tend to improve over base models\nbut remain below DPO in both semantic and human-centric\ndomains, reaffirming that preference optimization produces\nresponses that are not only accurate but also contextually\nadaptive and emotionally appropriate.\nFig. 3: Benchmark of Llama 3.1-8B and its adaptations across\nmetrics for each evaluation type. For visual consistency, se-\nmantic similarity scores (SS:E) and G-Eval, originally ranging\nfrom 0–1, were multiplied by 10 to align with the scale\nof readability (FK-GL) metrics. Higher scores for SS:E\nindicate stronger alignment between generated answers and\ntheir corresponding reference responses. Higher scores for\nGEval indicate a higher correctness score. Lower FK-GL\nscores indicate responses that are linguistically accessible.\nVI. LIMITATIONS\nWhile our DPO-based alignment framework demonstrates\nmeasurable improvements in empathy, politeness, and factual\nreliability, several limitations remain. First, the preference data\nused for fine-tuning, though carefully curated, reflects a limited\nrange of linguistic and cultural expressions of politeness and\nempathy. As a result, the model may underperform when\ninteracting with caregivers or patients from diverse sociocul-\ntural backgrounds. Second, our evaluation primarily focuses on\nlinguistic and semantic metrics rather than clinical outcomes.\nThe model’s effectiveness in real-world caregiving scenarios\nwhere emotional context, user stress, and task complexity vary\nwidely, remains to be validated through longitudinal or human-\nin-the-loop studies. Third, despite improved factual accuracy\nrelative to baseline models, the system is still vulnerable to\nsubtle hallucinations and incomplete medical reasoning, par-\nticularly for rare conditions or ambiguous caregiver prompts.\nAdditionally, because the fine-tuning data emphasize simplic-\nity and reassurance, there is a potential trade-off between\nlinguistic simplicity and medical precision, which must be\ncarefully balanced in future iterations. Finally, our study is\nconstrained by limited access to proprietary healthcare LLMs\nand restricted clinical datasets, which limits the scope of\ncomparative benchmarking and domain adaptation.\nVII. CONCLUSION AND FUTURE WORK\nThis paper presented a DPO framework designed to improve\nthe semantic alignment, factual accuracy, and human-centric\nquality of large language models in healthcare communica-\ntion. Systematic evaluation showed consistent gains across\nthese dimensions, producing more coherent, empathetic, and\ntrustworthy responses. Testing the framework across multiple\nopen LLMs and comparing it with leading industry models,\nincluding those from Google, demonstrated that the proposed\nmethod achieves stronger alignment with human preferences.\nThis improvement arises from DPO’s ability to optimize\npreference signals directly without separate reward models,\nenabling more efficient and stable alignment. Overall, the\nfindings highlight DPO’s potential as a scalable and transpar-\nent method for aligning healthcare-oriented language models\nwith factual correctness and human-centered communication\nstandards, advancing safer and more trustworthy AI systems\nfor caregiver support. Future research will focus on broadening\nthe framework’s scope and strengthening its human-centered\nalignment. A major priority is expanding and refining the\nquestion–answer dataset used for fine-tuning. Increasing the\nquantity, diversity, and linguistic variety of QA pairs—while\nmaintaining strict quality control—will enhance domain cover-\nage and improve generalization across caregiving contexts and\nhealth conditions. We also plan to incorporate direct feedback\nfrom caregivers and healthcare professionals to better capture\nuser perspectives on tone, clarity, and usefulness. Finally,\nwe aim to strengthen factual reliability by exploring com-\nplementary mechanisms that improve grounding in verified\nmedical knowledge, ensuring responses remain accurate and\ninterpretable for non-experts.\nVIII. ACKNOWLEDGEMENT\nThis project was funded by Health Resources Services\nAdministration—Geriatrics Workforce Enhancement Program-\ngrant number [U1QHP53051]. The University of Texas at El\nPaso Geriatrics Workforce Enhancement Program is supported\nby the Health Resources and Services Administration (HRSA)\nof the U.S. Department of Health and Human Services (HHS)\nas part of an award totaling $5 million with 0% percentage\nfinanced with non-governmental sources. The contents are\nthose of the authors and do not necessarily represent the\nofficial views of, nor are an endorsement, by HRSA, HHS,\nor the U.S. Government.\n"}, {"page": 10, "text": "REFERENCES\n[1] A. Yankouskaya, M. Liebherr, and R. Ali, “Can chatgpt be addictive? a\ncall to examine the shift from support to dependence in ai conversational\nlarge language models,” Human-Centric Intelligent Systems, pp. 1–13,\n2025.\n[2] A. Pal, T. Wangmo, T. Bharadia, M. Ahmed-Richards, M. B. Bhanderi,\nR. Kachhadiya, S. S. Allemann, and B. S. Elger, “Generative ai/llms\nfor plain language medical information for patients, caregivers and\ngeneral public: Opportunities, risks and ethics,” Patient preference and\nadherence, pp. 2227–2249, 2025.\n[3] U.S.\nFood\nand\nDrug\nAdministration,\n“Artificial\nintelligence-enabled\nmedical\ndevices,”\nhttps://www.\nfda.gov/medical-devices/software-medical-device-samd/\nartificial-intelligence-enabled-medical-devices, 2025, accessed: 2025-\n09-10.\n[4] D. deBronkart, “Three years, 17 doctors, suffering kid, no diagnosis.\nthen mom tried chatgpt,” Substack: Patients Use AI, September\n2024, accessed: 2025-09-10. [Online]. Available: https://patientsuseai.\nsubstack.com/p/three-years-17-doctors-suffering\n[5] M. A. Grasso, A. Rogalski, N. Farrukh, A. Kotal, and E. Calleros, “When\npatients go to” dr. google” before they go to the emergency department,”\narXiv preprint arXiv:2510.03329, 2025.\n[6] R. Chouffani El Fassi, B. E. Himes, A. R. Habib, W. Alhazzani,\nS. B. Naidu, J. Shahin, R. Mathur, M. M. McDermott, T. W.\nLee, M. P. Abrams, E. J. Topol, and A. Soroush, “Not all ai\nhealth tools with regulatory authorization are clinically validated,”\nNature Medicine, vol. 30, pp. 1862–1865, 2024. [Online]. Available:\nhttps://www.nature.com/articles/s41591-024-03203-3\n[7] J. P. Nielsen, C. von Buchwald, and C. Grønhøj, “Validity of the\nlarge language model chatgpt (gpt4) as a patient information source in\notolaryngology by a variety of doctors in a tertiary otorhinolaryngology\ndepartment,” pp. 779–782, 2023.\n[8] T. Zada, N. Tam, F. Barnard, M. Van Sittert, V. Bhat, S. Rambhatla et al.,\n“Medical misinformation in ai-assisted self-diagnosis: Development of\na method (evalprompt) for analyzing large language models,” JMIR\nFormative Research, vol. 9, no. 1, p. e66207, 2025.\n[9] M. Deist and A. P. Greeff, “Resilience in families caring for a family\nmember diagnosed with dementia,” Educational Gerontology, vol. 41,\nno. 2, pp. 93–105, 2015.\n[10] C. Palacio G, A. Krikorian, M. J. G´omez-Romero, and J. T. Limonero,\n“Resilience in caregivers: A systematic review,” American Journal of\nHospice and Palliative Medicine®, vol. 37, no. 8, pp. 648–658, 2020.\n[11] R. Srinivasan and B. S. M. Gonz´alez, “The role of empathy for artificial\nintelligence accountability,” Journal of Responsible Technology, vol. 9,\np. 100021, 2022.\n[12] N. T. K. Chi and N. Hoang Vu, “Investigating the customer trust in\nartificial intelligence: The role of anthropomorphism, empathy response,\nand interaction,” CAAI Transactions on Intelligence Technology, vol. 8,\nno. 1, pp. 260–273, 2023.\n[13] V. Sorin, D. Brin, Y. Barash, E. Konen, A. Charney, G. Nadkarni, and\nE. Klang, “Large language models and empathy: systematic review,”\nJournal of medical Internet research, vol. 26, p. e52597, 2024.\n[14] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[15] I. L. Alberts, L. Mercolli, T. Pyka, G. Prenosil, K. Shi, A. Rominger,\nand A. Afshar-Oromieh, “Large language models (llm) and chatgpt: what\nwill the impact on nuclear medicine be?” European journal of nuclear\nmedicine and molecular imaging, vol. 50, no. 6, pp. 1549–1552, 2023.\n[16] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,\nA. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient\nfoundation\nlanguage\nmodels,”\n2023.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2302.13971\n[17] T. Y. C. Tam, S. Sivarajkumar, S. Kapoor, A. V. Stolyar, K. Polanska,\nK. R. McCarthy, H. Osterhoudt, X. Wu, S. Visweswaran, S. Fu et al., “A\nframework for human evaluation of large language models in healthcare\nderived from literature review,” NPJ digital medicine, vol. 7, no. 1, p.\n258, 2024.\n[18] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and\nC. Finn, “Direct preference optimization: Your language model is\nsecretly a reward model,” Advances in neural information processing\nsystems, vol. 36, pp. 53 728–53 741, 2023.\n[19] B. Parmanto, B. Aryoyudanta, W. Soekinto, I. M. A. Setiawan, Y. Wang,\nH. Hu, A. Saptono, and Y. K. Choi, “Development of a reliable and\naccessible caregiving language model (calm),” 2024.\n[20] T. Tam et al., “Dr.knows: A knowledge-graph-enhanced framework for\ndiagnostic reasoning with large language models,” in Proceedings of the\nAAAI Conference on Artificial Intelligence.\nAAAI, 2025.\n[21] S. M. Alam, H. Zou, R. Vir, and N. Salehi, “Sage: System for accessible\nguided exploration of health information,” in AAAI-2024 Workshop on\nPublic Sector LLMs: Algorithmic and Sociotechnical Design, 2024.\n[22] L. Amugongo, N. Matabaro et al., “Retrieval-augmented generation for\nhealthcare: A systematic review of applications and evaluation gaps,”\nFrontiers in Digital Health, 2024.\n[23] S. Yang, L. Zhao, and Y. Chen, “Medalign: Aligning medical large\nlanguage models for safe and effective clinical use,” in Springer Lecture\nNotes in Computer Science.\nSpringer, 2024, pp. 317–329.\n[24] N. Babakov, D. Dale, I. Gusev, I. Krotova, and A. Panchenko, “Don’t\nlose the message while paraphrasing: A study on content preserving style\ntransfer,” in Natural Language Processing and Information Systems,\nE. M´etais, F. Meziane, V. Sugumaran, W. Manning, and S. Reiff-\nMarganiec, Eds.\nCham: Springer Nature Switzerland, 2023, pp. 47–61.\n[25] Z. Yan, J. Liu, L. Shuang, D. Xu, Y. Yang, H. Wang, J. Mao, H. Tseng,\nT. Chang, Y. Chen et al., “Large language models (llms) vs. specialist\ndoctors: A comparative study on health information in specific medical\ndomains,” J. Med. Internet Res, 2024.\n[26] G. Hembroff and S. Naseem, “Advancing health literacy through gen-\nerative ai: The utilization of open-source llms for text simplification\nand readability,” in World Congress in Computer Science, Computer\nEngineering & Applied Computing.\nSpringer, 2024, pp. 326–339.\n[27] H. Su, Y. Sun, R. Li, A. Zhang, Y. Yang, F. Xiao, Z. Duan, J. Chen,\nQ. Hu, T. Yang et al., “Large language models in medical diagnostics:\nScoping review with bibliometric analysis,” Journal of Medical Internet\nResearch, vol. 27, p. e72062, 2025.\n[28] V. Karpukhin, B. Oguz, S. Min, P. S. Lewis, L. Wu, S. Edunov,\nD. Chen, and W.-t. Yih, “Dense passage retrieval for open-domain\nquestion answering.” in EMNLP (1), 2020, pp. 6769–6781.\n[29] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of\nmodels,” arXiv e-prints, pp. arXiv–2407, 2024.\n[30] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nneural information processing systems, vol. 33, pp. 9459–9474, 2020.\n[31] M. Agrawal, I. Y. Chen, F. Gulamali, and S. Joshi, “The evaluation\nillusion of large language models in medicine,” npj Digital Medicine,\nvol. 8, no. 1, p. 600, 2025.\n[32] K. He, R. Mao, Q. Lin, Y. Ruan, X. Lan, M. Feng, and E. Cambria, “A\nsurvey of large language models for healthcare: from data, technology,\nand applications to accountability and ethics,” Information Fusion, vol.\n118, p. 102963, 2025.\n[33] M. Luo, C. J. Warren, L. Cheng, H. M. Abdul-Muhsin, and I. Banerjee,\n“Assessing empathy in large language models with real-world physician-\npatient interactions,” in 2024 IEEE International Conference on Big\nData (BigData).\nIEEE, 2024, pp. 6510–6519.\n"}]}