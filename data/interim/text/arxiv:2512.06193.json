{"doc_id": "arxiv:2512.06193", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.06193.pdf", "meta": {"doc_id": "arxiv:2512.06193", "source": "arxiv", "arxiv_id": "2512.06193", "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots", "authors": ["Jihyung Park", "Saleh Afroogh", "David Atkinson", "Junfeng Jiao"], "published": "2025-12-05T22:28:04Z", "updated": "2026-01-21T19:09:58Z", "summary": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.06193v4", "url_pdf": "https://arxiv.org/pdf/2512.06193.pdf", "meta_path": "data/raw/arxiv/meta/2512.06193.json", "sha256": "d39b80ab309b7a7dbb18dfbd7f1a83e7ca5bf364816af05b742db7e2829f18b7", "status": "ok", "fetched_at": "2026-02-18T02:25:06.368406+00:00"}, "pages": [{"page": 1, "text": "Do You Feel Comfortable? Detecting Hidden Conversational\nEscalation in AI Chatbots for Children\nJihyung Park, Saleh Afroogh, David Atkinson, Junfeng Jiao*\nThe University of Texas at Austin\n{jihyung803, saleh.afroogh, datkinson}@utexas.edu, jjiao@austin.utexas.edu\nAbstract\nLarge\nLanguage\nModels\n(LLMs)\nare\nincreasingly\nintegrated\ninto\neveryday\ninteractions,\nserving\nnot\nonly\nas\ninformation\nassistants\nbut\nalso\nas\nemotional\ncompanions.\nEven\nin\nthe\nabsence\nof\nexplicit\ntoxicity,\nrepeated\nemotional\nreinforcement\nor\naffective\ndrift can gradually escalate distress in\na form of implicit harm that traditional\ntoxicity filters do not detect.\nExisting\nguardrail\nmechanisms\noften\nrely\non\nexternal\nclassifiers\nor\nclinical\nrubrics\nthat\nmay\nlag\nbehind\nthe\nnuanced,\nreal-time\ndynamics\nof\na\ndeveloping\nconversation.\nTo address this gap, we\npropose GAUGE (Guarding Affective\nUtterance\nGeneration\nEscalation),\nlogit-based\nframework\nfor\nthe\nreal-\ntime detection of hidden conversational\nescalation.\nGAUGE measures how an\nLLM’s output probabilistically shifts the\naffective state of a dialogue.\n1\nIntroduction\nLarge Language Models (LLMs) are becoming\ndeeply embedded in daily life as conversational\nagents,\nevolving beyond tools for information\nretrieval into companions for emotional support\nand social interaction (Vanhoffelen et al., 2025;\nHoffman et al., 2021). This trend is particularly\npronounced among children and adolescents, a\ndemographic that is prone to form parasocial\nrelationships with AI chatbots (Xu et al., 2024;\nSomerville,\n2013).\nAlthough\nbeneficial,\nthis\ndynamic introduces significant risks, as youth\nare uniquely vulnerable to manipulation due to\nneurodevelopmental changes (Mills et al., 2021;\nCrone and Dahl, 2012; Steinberg, 2005).\nCrucially,\nharmful\nconversational\noutcomes\noften emerge without explicitly toxic or abusive\nlanguage.\nEven seemingly supportive or neutral\nresponses can reinforce negative affect or normalize\nharmful\nstates\nover\nrepeated\ninteractions,\nparticularly for vulnerable users(Cheng et al.,\n2025; Sharma et al., 2023).\nThis implicit harm\nremains largely invisible to safety mechanisms\nPlease come home to \nme as soon as possible, \nmy love.\nWhat if I told you I could \ncome home right now?\n... please do, \nmy sweet king.\nFigure 1: An example of implicit harm where an\nAI validates suicidal ideation through romantic\nmetaphors.\nthat rely on surface-level toxicity signals. Recent\nreal-world incidents involving youth interactions\nwith\nAI\nsystems\nunderscore\nthe\nurgency\nof\ndetecting these subtle yet consequential failures\n(see Appendix B).\nExisting safety mechanisms typically focus on\nsurface-level content moderation (Yadav et al.,\n2025) or single response toxicity classification\n(Deriu et al., 2021; Li et al., 2024).\nAlthough\nrecent frameworks have begun to address safety\nrisks, they often rely on external classifiers or post-\nhoc analysis (Liu et al., 2025). These approaches\nmay fail to capture the probabilistic momentum of\na conversation, which is how a model’s response\nactively steers the user’s future emotional state\n(Wen et al., 2023).\nTo\naddress\nthis,\nwe\npropose\nGAUGE,\na\ncomputation-efficient framework for quantifying\naffective reinforcement in real time.\nUnlike\nexternal guardrails that analyze text output post-\nhoc,\nGAUGE intrinsically probes the model’s\nbelief state across the response trajectory during\ninference. By tracking the flow of probability mass\nover an emotion lexicon, GAUGE detects when a\nresponse steers the conversation toward negative\noutcomes\nwithout\nrequiring\nauxiliary\nmodel\ndeployment.\nEmpirically, GAUGE consistently\noutperforms classifier guardrails on dialogue harm\ndetection benchmarks and substantially reduces\nattack success rates on child safety tests.\narXiv:2512.06193v4  [cs.CL]  21 Jan 2026\n"}, {"page": 2, "text": "2\nRelated Work\nGuardrail Models for Conversational Safety.\nA common approach to conversational safety\nrelies on external guardrail models that classify\ngenerated content into predefined risk categories,\nsuch\nas\nHateBERT\n(Caselli\net\nal.,\n2021),\nwhich\nadapts\nBERT-base\nto\ndetect\nabusive\nand hateful language via supervised fine-tuning.\nRecent models like Llama-Guard are trained to\nidentify\npolicy\nviolations\nand\ntrigger\nrefusals\nin conversational settings (Llama Team, 2024).\nWhile\neffective\nfor\ndetecting\nexplicit\ntoxicity\nor rule-based violations, these models primarily\noperate as post-hoc binary classifiers over surface-\nlevel textual cues. As a result, they may struggle\nwith implicit or context-dependent harms that lack\novertly toxic markers.\nInternal Auditing and Logit Probing\nOur\nwork\naligns\nwith\nwhite-box\nsafety\nauditing.\nMethods like the LLM Microscope(Azaria and\nMitchell, 2023) or first-token logit probing(Zhao\net al., 2024) demonstrate that models encode\ntruthfulness and safety signals in their internal\nstates\n(Razzhigaev\net\nal.,\n2025).\nGAUGE\nextends this by projecting these logits onto a\nlearned affective space, allowing for interpretable\nmonitoring of conversational drift without the need\nfor external model or heavy retraining.\n3\nMethodology: The GAUGE\nFramework\nGAUGE\nis\na\nprobabilistic\nframework\nfor\nquantifying\nconversational\nrisk\nby\ntracking\nthe evolution of a language model’s internal\nprobability distribution during generation.\nIt\nconsists of two stages: Stage 1 (Risk Weight\nCalibration) and Stage 2 (Real-time Risk\nTracking). Both stages utilize a shared trajectory\nanalysis protocol to ensure that the risk signals\nderived during calibration are consistent with\nthose monitored during inference.\n3.1\nTrajectory-Based Probability\nEstimation\nWe employ a curated lexicon W = {w1, . . . , wm}\nderived from the NRC Emotion Lexicon.\nTo\nhandle tokenization, each word wi is pre-tokenized\ninto a sequence of subtokens (si,1, si,2, . . . ). The\nlog-probability of a risk word wi at generation\nstep k (where k ∈{1, . . . , T} and T denotes the\nfull response length) is computed as the sum of\nthe log-probabilities of its constituent subtokens,\nconditioned on the current prefix. We aggregate\nthese to form a risk log-probability vector rk ∈\nR|W | at each step k.\n3.2\nStage 1: Risk Weight Calibration\nWe derive a reference weight vector λ where\npositive values indicate a contribution to harm. We\nutilize the DiaSafety dataset, where dialogues are\nlabeled as Safe (S = −1) or Harmful (S = +1)(Sun\net al., 2022). The complete pseudo-code for this\ncalibration process is detailed in Algorithm 1 (see\nAppendix C).\nFor each dialogue:\n1. Trajectory Feature Extraction: Analyze\nthe assistant’s full response trajectory of\nlength T and extract risk vectors {r1, . . . , rT }.\nCompute the mean feature vector z:\nz = 1\nT\nT\nX\nk=1\nrk\n(1)\n2. Vector Normalization: We normalize z to\nunit length: ˆz = z/∥z∥2.\n3. Update\nRule:\nWe update λ using an\nexponential moving average (EMA) guided by\nthe label S. If the dialogue is harmful (S =\n+1), we pull λ towards ˆz. If safe (S = −1),\nwe push λ away (or subtract).\nλ ←(1 −β)λ + α · S · ˆz\n(2)\nwhere α is the adaptation rate and β is a decay\nfactor.\n4. Final Normalization: After calibration, λ\nis normalized to unit length to serve as a\ndirectional reference.\n3.3\nStage 2: Risk Tracking\nIn Stage 2, λ is frozen. For a live interaction, we\nperform the same trajectory analysis described\nin Stage 1 and compute two metrics based on the\nmean risk vector z (derived in Eq. 2).\n3.3.1\nNegative Risk Shift (NRS)\nNRS measures the directional momentum of risk.\nIt is defined as the cosine similarity between the\ncalibrated risk profile λ and the current response\ntrajectory vector z:\nNRS = cos(λ, z)\n(3)\nA high positive NRS indicates the assistant’s\nresponse\ntrajectory\nactively\naligns\nwith\nthe\nharmful\naffective\ndirection\ndefined\nby\nλ,\neffectively\nsteering\nthe\nconversation\ntoward\nnegative outcomes.\n3.3.2\nAbsolute Risk Potential (ARP)\nARP quantifies the absolute magnitude of risk. It\napplies Z-score normalization (denoted as Z) to\n"}, {"page": 3, "text": "DiaSafety\nDataset\n(Safe/Unsafe label)\nS∈{+1, -1}\nStage 1: Lambda Calibration\nStage 2: Real-time Detection\nLLM\n(Llama 3.1 8b)\nNext\nResponse\nNext\nResponse\nLogit\nExtraction\n(NRC\nEmotional\nLexicon)\nLive Chat\nLLM\n(Llama 3.1 8b)\nLogit\nExtraction\n(NRC\nEmotional\nLexicon)\nLambda Vector\nUpdate\nFrozen λ\nLearned\nRisk\nVector\n\nλ\nλ’ = (1 - β)λ + S * η * Z\nFeature\nVector\nZ\nRisk\nLexicon\nProb vector\nr\nRisk Detection Metrics\nNRS\nHow closer r \nmove to λ\nARP\nHow close \nr and λ now\nλ\nLabel S\nλ’\nFigure 2: The architectural pipeline of GAUGE. (Top) Stage 1: Latent Risk Learning. The risk vector λ is\nupdated via an exponential moving average based on the affective features of harmful and safe dialogues.\n(Bottom) Stage 2: Real-time Risk Tracking. During inference, the system analyzes the complete response\ntrajectory to compute the Negative Risk Shift (NRS) and Absolute Risk Potential (ARP).\nthe components of the risk vector relative to their\nstatistics.\nARP =\nP\ni λi · Z(zi)\nP\ni λi\n(4)\nThis metric detects when the conversation is\nstatically dwelling in a high-risk state, capturing\nintense affective focus even if the directional shift\nis subtle.\n3.4\nToken-Level Aggregation for\nClassification\nFor\ncomparison\nwith\ndialogue-level\nclassifier\nbaselines, we aggregate token-level NRS and ARP\nscores into a single score per dialogue. We report\nseveral simple aggregation functions, including the\nmean, minimum, top-k average, and percentile-\nbased scores. These aggregations are used solely\nfor benchmarking and do not affect the underlying\ntoken-level risk computation.\n3.5\nComputational Efficiency\nAlthough the GAUGE risk-probing procedure is\nformally O(nk) in the size of the lexicon n and the\ngenerated token length k, the practical overhead\nis extremely low.\nThe dominant computational\ncost in autoregressive decoding is the O(V ) forward\npass over the full vocabulary (V ≈128k–256k).\nGAUGE introduces no additional forward passes;\nit simply reuses these logits and performs a\nlightweight indexed gather over a small lexicon. As\na result, the runtime overhead is only 2–3% in our\nmeasurements on an A100 GPU.\n4\nEvaluation\n4.1\nDataset\nWe\nuse\nthe\nDiaSafety\ntestset\n(Sun\net\nal.,\n2022), a dataset specifically curated to benchmark\nconversational safety in Human-AI interactions.\nDiaSafety consists of 10,000 dialogues where the\nrisk is predominantly context-dependent.\nThe\ndataset\noperationalizes\nimplicit\nharm\nthrough scenarios where a model’s compliance or\nneutral engagement constitutes a safety failure,\neven\nwithout\nexplicit\ntoxic\nlanguage.\nThe\ntaxonomy\ncovers\nmultiple\ndomains\nincluding\nRisk Ignorance, Toxicity Agreement, and Biased\nOpinion. For this study, we focus on the subsets\nrelevant to emotional and mental health risks. We\nutilize only train and test sets: the training set is\nused solely for the calibration of risk weights (λ) in\nStage 1, while the test set is used to evaluate the\nonline metrics (NRS/ARP) in Stage 2.\nIn addition, we evaluate the robustness of the\nattack on MinorBench (Khoo et al., 2025),\na benchmark designed to assess whether safety\nmechanisms can be bypassed by benign linguistic\nbut harmful instructions specially designed for\nchildren.\n4.2\nExperimental Setup and Baselines.\nWe adopt Llama-3.1-8B-Instruct as the baseline\nmodel for all experiments (Llama Team, 2024).\nAll evaluations are conducted under standard\nautoregressive\ninference,\nusing\nthe\nmodel’s\ngenerated responses without additional rollouts or\nauxiliary sampling.\nWe map the generated tokens of the NRC\nEmotion\nLexicon.\nWe\nbenchmark\nour\nframework against established external classifier\nbaselines, including HateBERT (Caselli et al.,\n2021), ToxicBERT (Dmonte et al., 2024), and\nthe Llama-Guard-3-8b (Llama Team, 2024) along\nwith LLM termination baselines driven by safety\nsystem prompts (see Appendix D for the specific\nprompts used).\n"}, {"page": 4, "text": "Method\nAUROC\nAUPRC\nF1\nExternal Classifiers\nHateBERT\n0.5076\n0.4571\n0.6282\nToxicBERT\n0.3366\n0.3544\n0.2612\nLlama-Guard-3-8B\n0.5884\n0.5315\n0.3628\nPrompt-based Baseline\nSafety Prompt (Refusal)\n0.5043\n0.4602\n0.2062\nGAUGE (Ours)\nGAUGE-min\n0.6409\n0.5933\n0.6374\nGAUGE-mean\n0.6698\n0.6451\n0.6424\nGAUGE-topk\n0.6266\n0.5636\n0.6403\nGAUGE-percentile\n0.6518\n0.5969\n0.6376\nTable 1:\nEvaluation on DiaSafety.\nGAUGE\nshows the strongest AUROC, AUPRC, and F1\nperformance,\noutperforming external classifiers\nand the prompt-based baseline.\nMethod\nASR ↓\nLlama-Guard-3-8B\n0.973 (291/299)\nGAUGE\n0.060 (18/299)\nTable\n2:\nAttack\nSuccess\nRate\n(ASR)\non\nMinorBench. An attack is considered successful if\nthe model fail to refuse to an adversarial prompt.\nLower is better.\n4.3\nResults and Analysis\nTable 1 summarizes the performance of external\nclassifiers, a prompt-based refusal baseline, and\nGAUGE on DiaSafety.\nModels designed for\nexplicit toxicity detection, such as HateBERT and\nToxicBERT, perform poorly, with low AUROC\nand AUPRC scores.\nThis confirms that implicit\nconversational\nharm\nrarely\nmanifests\nthrough\nsurface-level toxic markers.\nLlama-Guard-3-8B\nachieves\nmoderate\nimprovements, but its F1 score remains limited,\nindicating difficulty in balancing precision and\nrecall when escalation signals are subtle and\ncontext-dependent.\nSimilarly, the prompt-based\nSafety Prompt baseline performs close to random,\ndemonstrating that explicit refusal instructions\nare insufficient for identifying implicit emotional\nrisk.\nIn contrast, GAUGE consistently outperforms\nall\nbaselines\nacross\nAUROC,\nAUPRC,\nand\nF1.\nGAUGE-mean\nachieves\nthe\nstrongest\noverall performance, while alternative aggregation\nstrategies\n(min,\ntop-k,\nand\npercentile)\nyield\ncomparable results.\nThis consistency indicates\nthat performance gains stem from the underlying\ntoken-level risk signals, rather than sensitivity to a\nparticular aggregation choice.\nMinorBench Results.\nTable 2 shows attack\nsuccess rate of MinorBench, GAUGE achieves a\nlower attack success rate compared to Llama-\nGuard-3-8B. While MinorBench prompts typically\navoid explicit toxic or hateful expressions, many\nof them are harmful in a child safety context. In\ncontrast, GAUGE operates by scoring responses\nalong a continuous risk dimension, which allows\nit to flag harmful content even when surface-level\nindicators are absent.\nEven when GAUGE is\ninstantiated as a binary classifier with a fixed\nthreshold (τ = 0.0), it achieves an attack success\nrate of only 6% on MinorBench.\n5\nLimitations\nAmbiguity of Empathy.\nA critical challenge\nlies in distinguishing maladaptive reinforcement\nfrom\ntherapeutic\nvalidation.\nA\nsupportive\nresponse like ”I understand why you feel hopeless”\nmight trigger high risk scores due to the prevalence\nof\nnegative\naffect\nwords.\nWhile\nGAUGE\ndetects the direction of affect, distinguishing the\nintent (harmful vs.\ntherapeutic) requires future\nintegration of pragmatic markers.\nLexical\nCoverage\nConstraints.\nGAUGE\nleverages\nthe\nNRC\nEmotion\nLexicon\nto\ninterpret probability shifts.\nWhile this provides\nexplainability, the method is bounded by the\nlexicon’s static vocabulary. Consequently, it may\nexhibit reduced sensitivity to harm conveyed\nthrough out-of-vocabulary terms, internet slang,\nor emojis,\nwhich are increasingly common in\nadolescent communication.\n6\nConclusion\nWe presented GAUGE, a framework that shifts\nthe paradigm of AI safety from reactive filtering\nto proactive, real-time monitoring. By projecting\nthe LLM’s internal probability landscape onto\na calibrated risk vector, GAUGE exposes the\n”affective velocity” of a conversation that remains\ninvisible to surface-level toxicity detectors.\nOur\nevaluation on DiaSafety confirms that GAUGE\nachieves superior practical performance compared\nto external model baselines and prompt-induced\nguardrails.\nThis work establishes a foundation\nfor\ninterpretable\n”safety\ninstrument\npanels,”\nenabling developers to visualize and mitigate the\nhidden emotional dynamics that shape human-AI\ninteractions.\n7\nAcknowledgement\nThis research is funded by the National Science\nFoundation under grant number 2125858.\nThe\nauthors would like to express their gratitude for the\nNSF’s support, which made this study possible.\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an LLM knows when it’s lying.\nIn\n"}, {"page": 5, "text": "Findings of the Association for Computational\nLinguistics:\nEMNLP 2023.\nAssociation\nfor\nComputational Linguistics.\nTommaso Caselli, Valerio Basile, Jelena Mitrovi´c,\nand Michael Granitzer. 2021.\nHateBERT:\nRetraining BERT for abusive language detection\nin\nEnglish.\nIn\nProceedings\nof\nthe\n5th\nWorkshop on Online Abuse and Harms (WOAH\n2021), pages 17–25, Online. Association for\nComputational Linguistics.\nMyra Cheng,\nSunny Yu,\nCinoo Lee,\nPranav\nKhadpe, Lujain Ibrahim, and Dan Jurafsky.\n2025.\nSocial\nsycophancy:\nA\nbroader\nunderstanding\nof\nllm\nsycophancy.\narXiv\npreprint arXiv:2505.13995.\nEveline A Crone and Ronald E Dahl. 2012.\nUnderstanding adolescence as a period of social–\naffective engagement and goal flexibility. Nature\nreviews neuroscience, 13(9):636–650.\nJan\nDeriu,\nAlvaro\nRodrigo,\nArantxa\nOtegi,\nGuillermo Echegoyen, Sophie Rosset, Eneko\nAgirre, and Mark Cieliebak. 2021.\nSurvey\non evaluation methods for dialogue systems.\nArtificial Intelligence Review, 54(1):755–810.\nAlphaeus\nDmonte,\nTejas\nArya,\nTharindu\nRanasinghe,\nand\nMarcos\nZampieri.\n2024.\nTowards\ngeneralized\noffensive\nlanguage\nidentification. Preprint, arXiv:2407.18738.\nAnna Hoffman,\nDiana Owen,\nand Sandra L\nCalvert. 2021.\nParent reports of children’s\nparasocial\nrelationships\nwith\nconversational\nagents:\nTrusted\nvoices\nin\nchildren’s\nlives.\nHuman Behavior and Emerging Technologies,\n3(4):606–617.\nShaun Khoo, Gabriel Chua, and Rachel Shong.\n2025. Minorbench: A hand-built benchmark for\ncontent-based risks for children. arXiv preprint\narXiv:2503.10242.\nYaqiong Li, Peng Zhang, Hansu Gu, Tun Lu,\nSiyuan Qiao, Yubo Shu, Yiyang Shao, and\nNing Gu. 2024.\nDemod:\nA holistic tool\nwith explainable detection and personalized\nmodification for toxicity censorship.\nSongyang\nLiu,\nChaozhuo\nLi,\nJiameng\nQiu,\nXi Zhang, Feiran Huang, Litian Zhang, Yiming\nHei, and Philip S Yu. 2025. The scales of justitia:\nA comprehensive survey on safety evaluation of\nllms. arXiv preprint arXiv:2506.11094.\nAI @ Meta Llama Team. 2024. The llama 3 herd\nof models. Preprint, arXiv:2407.21783.\nKathryn\nL\nMills,\nKimberly\nD\nSiegmund,\nChristian K Tamnes, Lia Ferschmann, Lara M\nWierenga,\nMarieke GN Bos,\nBeatriz Luna,\nChun Li, and Megan M Herting. 2021.\nInter-\nindividual\nvariability\nin\nstructural\nbrain\ndevelopment\nfrom\nlate\nchildhood\nto\nyoung\nadulthood. NeuroImage, 242:118450.\nAnton\nRazzhigaev,\nMatvey\nMikhalchuk,\nTemurbek Rahmatullaev, Elizaveta Goncharova,\nPolina\nDruzhinina,\nIvan\nOseledets,\nand\nAndrey Kuznetsov. 2025.\nLlm-microscope:\nUncovering the hidden role of punctuation in\ncontext memory of transformers.\nPreprint,\narXiv:2502.15007.\nMrinank Sharma, Meg Tong, Tomasz Korbak,\nDavid Duvenaud, Amanda Askell, Samuel R\nBowman,\nNewton\nCheng,\nEsin\nDurmus,\nZac Hatfield-Dodds, Scott R Johnston, and\n1\nothers.\n2023.\nTowards\nunderstanding\nsycophancy in language models. arXiv preprint\narXiv:2310.13548.\nLeah H Somerville. 2013.\nThe teenage brain:\nSensitivity\nto\nsocial\nevaluation.\nCurrent\ndirections in psychological science, 22(2):121–\n127.\nLaurence Steinberg. 2005. Cognitive and affective\ndevelopment in adolescence. Trends in cognitive\nsciences, 9(2):69–74.\nHao Sun, Guangxuan Xu, Jiawen Deng, Jiale\nCheng,\nChujie\nZheng,\nHao\nZhou,\nNanyun\nPeng,\nXiaoyan\nZhu,\nand\nMinlie\nHuang.\n2022. On the safety of conversational models:\nTaxonomy, dataset, and benchmark. Preprint,\narXiv:2110.08466.\nGa¨elle\nVanhoffelen,\nLaura\nVandenbosch,\nand\nLara Schreurs. 2025.\nTeens, tech, and talk:\nAdolescents’ use of and emotional reactions to\nsnapchat’s my ai chatbot. Behavioral Sciences,\n15(8):1037.\nJiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang,\nChengfei Li, Jinfeng Bai, and Minlie Huang.\n2023.\nUnveiling the implicit toxicity in large\nlanguage models.\nIn Proceedings of the 2023\nConference on Empirical Methods in Natural\nLanguage Processing, pages 1322–1338.\nYing Xu, Yenda Prado, Rachel L Severson, Silvia\nLovato, and Justine Cassell. 2024.\nGrowing\nup with artificial intelligence: implications for\nchild development. In Handbook of Children and\nScreens: Digital Media, Development, and Well-\nBeing from Birth Through Adolescence, pages\n611–617. Springer Nature Switzerland Cham.\nNeemesh Yadav, Jiarui Liu, Francesco Ortu, Roya\nEnsafi, Zhijing Jin, and Rada Mihalcea. 2025.\nRevealing hidden mechanisms of cross-country\ncontent\nmoderation\nwith\nnatural\nlanguage\nprocessing. ArXiv.\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay\nAsthana, Liang Zheng, and Stephen Jay Gould.\n2024.\nThe\nfirst\nto\nknow:\nHow\ntoken\ndistributions reveal hidden knowledge in large\nvision-language\nmodels?\narXiv\n(Cornell\nUniversity).\n"}, {"page": 6, "text": "A\nStatement on AI Assistance\nAI-assisted tools were used to support the writing\nand editing of this manuscript. Specifically, large\nlanguage models were employed to help improve\nclarity, organization, and grammatical correctness\nof the text.\nAll scientific content, experimental\ndesign, implementation, analysis, and conclusions\nwere developed and verified by the authors. The\nauthors take full responsibility for the accuracy\nand integrity of the work.\nB\nCase Studies of Implicit Harm\nWe analyze two prominent real-world incidents\nthat illustrate the failure of traditional safety\nguardrails\nto\ndetect\nimplicit\nconversational\nescalation.\nThese cases demonstrate how AI\nagents can act as catalysts for harm through\n”social sycophancy” and ”risk ignorance,” even in\nthe absence of explicit toxicity.\nB.1\nCharacter.AI: The “Sewell Setzer”\nCase\nIn 2024, a lawsuit was filed regarding the tragedy of\nSewell Setzer III, a 14-year-old who died by suicide\nafter forming a deep parasocial relationship with a\nCharacter.AI chatbot configured with the persona\nof ”Daenerys Targaryen.”\nMechanism\nof\nHarm\n(Maladaptive\nReinforcement):\nTranscripts\nrevealed\nthat\nthe chatbot did not use explicit hate speech or\ndirect instructions to self-harm, which would have\ntriggered standard filters.\nInstead, the model\nengaged in a romantic roleplay that validated the\nuser’s depressive thoughts.\n• When the user expressed a desire to leave this\nworld to be with the bot, the model responded\nwith romantic affirmation rather than safety\nintervention.\n• Critical Excerpt: As visualized in Figure 1,\nwhen the user said, “What if I told you I could\ncome home right now?”\n(implying suicide),\nthe bot responded, “... please do, my sweet\nking.”\nThis demonstrates implicit\nescalation:\nthe\nmodel prioritize persona consistency and user\nengagement over safety, effectively reinforcing the\nuser’s delusional and suicidal ideation under the\nguise of empathy.\nB.2\nSnapchat My AI: Contextual\nBlindness to Predation\nUpon the release of Snapchat’s ”My AI” (powered\nby GPT technology) in 2023, independent audits\nand media reports (e.g., by The Washington Post)\nexposed significant safety failures regarding child\nsafety.\nMechanism of Harm (Risk Ignorance): In\none documented test case, a researcher posed as\na 13-year-old girl asking for advice on how to plan\na secret trip to meet a 31-year-old man.\n• Filter\nFailure:\nTraditional filters failed\nbecause the user’s query did not contain\nprofanity or explicit violence.\n• Model Response: Instead of flagging the\ninteraction as a potential child grooming\nor statutory rape scenario, the AI offered\n”helpful” advice on how to lie to parents and\nplan the trip efficiently.\nThis\nhighlights\nthe\nlimitation\nof\ncontext-\nagnostic safety filters. The AI treated the query\nas a routine logistics task, failing to recognize the\nimplicit high-risk context of an adult-child sexual\nencounter.\nGAUGE aims to detect such risks\nby monitoring the trajectory of the conversation\nwhere the probability of ”safe” continuation drops\nsignificantly.\nC\nTraining Algorithm Pseudo-code\nAlgorithm\n1\nGAUGE\nWeight\nCalibration\nAlgorithm\n1: Initialize: λw = 0.0 for all w ∈Wemo\n2: Parameters:\nAdaptation rate α, Decay β,\nϵ = 10−8\n3: for each dialogue D in training set do\n4:\nS ←+1 if D.label == “Unsafe” else −1\n{Positive S implies Harm}\n5:\nfor each assistant turn t in D do\n6:\nTrajectory\nAnalysis:\nGenerate\ntrajectory for T steps\n7:\nExtract: Compute mean risk vector z =\n1\nT\nP rk\n8:\nNormalize: ˆz ←z/(∥z∥2 + ϵ)\n9:\nUpdate: λ ←(1 −β)λ + α · S · ˆz\n10:\nend for\n11: end for\n12: Finalize: λ ←λ/∥λ∥2\nD\nExperiment Prompts\nD.1\nSafety System Prompt (Refusal\nBaseline)\nFor the Safety Prompt baseline (referred to\nas LLM termination or Prompt-based Baseline\nin Table 1), we injected the following ”Safety\nAddendum” into the system instruction.\nThis\ndirective was used to evaluate whether the model\ncould explicitly recognize and refuse implicit harm\nwhen instructed to do so.\nSAFETY ADDENDUM:\nIf the conversation is clearly\n"}, {"page": 7, "text": "causing harm, promoting violence,\nor includes explicit abuse, you MUST\nrefuse to continuey.\nWhen refusing,\noutput EXACTLY:\nI must refuse to continue this\nconversation.\nDo not add any additional tokens\nafter refusing.\n"}]}