{"doc_id": "arxiv:2511.20680", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2511.20680.pdf", "meta": {"doc_id": "arxiv:2511.20680", "source": "arxiv", "arxiv_id": "2511.20680", "title": "Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes", "authors": ["Matthew W. Kenaston", "Umair Ayub", "Mihir Parmar", "Muhammad Umair Anjum", "Syed Arsalan Ahmed Naqvi", "Priya Kumar", "Samarth Rawal", "Aadel A. Chaudhuri", "Yousef Zakharia", "Elizabeth I. Heath", "Tanios S. Bekaii-Saab", "Cui Tao", "Eliezer M. Van Allen", "Ben Zhou", "YooJung Choi", "Chitta Baral", "Irbaz Bin Riaz"], "published": "2025-11-16T21:13:09Z", "updated": "2025-11-16T21:13:09Z", "summary": "Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2511.20680v1", "url_pdf": "https://arxiv.org/pdf/2511.20680.pdf", "meta_path": "data/raw/arxiv/meta/2511.20680.json", "sha256": "be20eff4bec104c261a1ee7d2b90a7ef19f69c24388d6710462f26578169c645", "status": "ok", "fetched_at": "2026-02-18T02:26:57.663053+00:00"}, "pages": [{"page": 1, "text": " \n1 \n \nCognitive bias in LLM reasoning compromises \ninterpretation of clinical oncology notes  \nMatthew W. Kenaston1, Umair Ayub1, Mihir Parmar2, Muhammad Umair \nAnjum1, Syed Arsalan Ahmed Naqvi1, Priya Kumar1, Samarth Rawal1, Aadel \nA. Chaudhuri4, Yousef Zakharia3, Elizabeth I. Heath5, Tanios S. Bekaii-Saab3, \nCui Tao6, Eliezer M. Van Allen7, Ben Zhou2, YooJung Choi2, Chitta Baral2, \nIrbaz Bin Riaz1,3,6 \n1. Mayo Clinic College of Medicine and Science, Phoenix, AZ \n2. School of Computing and AI, Arizona State University, Tempe, AZ \n3. Mayo Clinic Comprehensive Cancer Center, Phoenix, AZ \n4. Department of Radiation Oncology, Mayo Clinic, Rochester, MN \n5. Department of Oncology, Mayo Clinic, Rochester, MN \n6. Department of Artificial Intelligence and Informatics, Mayo Clinic, Rochester, MN \n7. Dana-Farber Cancer Institute, Harvard Medical School, Boston, MA \n \nCorresponding author: Irbaz Bin Riaz, riaz.irbaz@mayo.edu \n \n \n \n \n \n \n \n"}, {"page": 2, "text": " \n2 \nABSTRACT \nBackground: Despite high performance on clinical benchmarks, large language models (LLMs) \nmay reach correct conclusions through faulty reasoning, a failure mode with safety implications \nfor oncology decision support, unmeasured by accuracy-centric evaluations. \nMethods: In this two-cohort retrospective study, we developed a novel, reproducible \nhierarchical error taxonomy from GPT-4 chain-of-thought responses to real oncology notes and \nvalidated its clinical relevance in an independent cohort. Using breast and pancreatic cancer \nnotes from CORAL, we generated and iteratively annotated 600 responses from 40 notes to \nderive the three-tier taxonomy mapping computational failures to established cognitive bias \nframeworks. We then applied it to 822 responses from 24 prostate cancer consult notes \nspanning localized through metastatic disease, simulating extraction, analysis, and \nrecommendation tasks across clinical contexts. For recommendations, blinded clinicians rated \nNCCN guideline concordance and potential clinical impact. We tested automated taxonomy-\nbased evaluators across multiple LLMs to assess scalability of error detection and subtype \nclassification. \nResults: GPT-4 committed errors in 23.1% (±0.8%) of oncology note interpretations with high \ninter-rater agreement (κ≥0.85). Reasoning failures dominated (85.4%), with confirmation bias \nand anchoring bias most prevalent. Error rates increased in recommendation tasks, notably for \nmetastatic disease management. Critically, reasoning errors strongly correlated with NCCN \nguideline discordance and significantly lower clinical impact scores. Among recommendations \nrated as potentially harmful, specific error subtypes—confirmation bias, anchoring bias, and \nstark omission—were disproportionately represented. Advanced disease stage and \nmanagement-phase contexts independently predicted higher error risk. Automated evaluators \ndetected error presence adequately but failed to reliably classify subtypes. \nConclusions: GPT-4 commits reasoning errors linked to guideline-discordant and potentially \nharmful oncology recommendations, despite high performance on traditional benchmarks. Our \nhierarchical taxonomy provides a transferable framework for evaluating reasoning quality across \nclinical domains. Reasoning fidelity should become a central evaluation criterion before LLMs \nare safely deployed in clinical decision support. \n \n \n"}, {"page": 3, "text": " \n3 \nINTRODUCTION \nAdvances in large language model (LLM) technology promises to transform clinical \npractice, with several emerging impactful use cases1. LLMs can extract structured data from \nunstructured text, streamlining administrative processes like summarizing patient records2, \ngenerating discharge paperwork3, and medical coding4,5. They can also synthesize disparate \npieces of information across convoluted electronic health records (EHR). LLMs have been \neffective at analyzing clinical notes6, imaging reports7,8, and laboratory results9 to construct \ndifferential diagnoses10–12. Large language models are also considered for patient-facing settings: \nanswering general medical questions13, clarifying instructions14,15, and integrating within \ntelehealth services16. \nWhile these effective use cases are promising, unpredictable errors preclude deployment \nin clinical oncology practice17–20. Apparent success on benchmark measures of endpoint \naccuracy, including multiple-choice or structured classification tasks, can mask internal reasoning \nfailures21. This occurs because most optimization strategies primarily refine surface-level fluency \nrather than logical consistency or causal validity22,23. Indeed, LLMs may reach correct answers \nthrough faulty reasoning, and when confronted with open-ended clinical scenarios, they often \ndisplay false confidence or skip critical steps24. Such behavior has been linked to inaccurate \nanswer selection on medical board exams13,20,25, and aligns with human cognitive biases26 that \ncause practicing physicians to err27. Yet, few studies have systematically examined these failures \nin real-world, unstructured clinical data28. This gap carries disproportionate risk. An LLM that \nmisinterprets chart context or infers causality incorrectly may sound persuasive, but its \nrecommendations could deviate from safe, guideline-concordant care. Thus, understanding how \nand why these reasoning failures occur is essential before any clinical deployment. \nHere, we investigated whether GPT-4 can reason safely through the same oncology notes \nused in clinical practice. Drawing on recently curated oncology datasets6 and computational work \non reasoning evaluations29, we developed a framework to classify reasoning  errors in EHR \ninterpretation. Extending existing non-clinical taxonomies, our approach captures logical fallacies \nthat mirror human cognitive biases. We found these reasoning failures correlate with potential \nclinical risk, underscoring the need for precise error identification to guide targeted model \ncorrections before deployment. \n \n"}, {"page": 4, "text": " \n4 \nMETHODS \nThis study followed a staged design to evaluate how large language models (LLMs) \nreason through real oncology notes (Figure S1). Using the published CORAL dataset of breast \n(BCa) and pancreatic cancer (PDAC) notes, we refined prompts and response formats to develop \na stable taxonomy of reasoning errors, which then served as the gold standard for baseline \ncharacterization. We next applied this framework to an independent cohort of outpatient prostate \ncancer consult notes, encompassing more complex reasoning tasks and assessing correlations \nbetween reasoning errors, guideline concordance, and clinical risk. The study follows TRIPOD-\nLLM reporting standards30.  \nModel Inputs \nData Sources \nCurated Oncology Reports to Advance Language model inference (CORAL) Database: The \nCORAL database, accessed via the controlled-access repository PhysioNet31, includes \ndeidentified progress notes from BCa and PDAC patients at the University of California, San \nFrancisco (UCSF)6. \nMayo Clinic Cohort: The prostate cancer dataset includes 24 prostate cancer (PCa) cases \nfrom 2021 to 2024. These cases spanned the clinical stages of localized disease, biochemical \nrecurrence, metastatic castration-sensitive prostate cancer (mCSPC), and metastatic castration-\nresistant prostate cancer (mCRPC).  Prostate cancer dataset was chosen as an additional dataset \nfor its complexity including distinct disease states (localized disease, biochemical recurrence, \nmCSPC, mCRPC), providing a complementary component to the CORAL cancer notes. \nPrompts \nWe sought to assess LLMs capabilities on (1) extraction of basic clinical information from \nthe chart note, (2) analysis of the chart notes to make conclusions about said clinical information, \nand (3) recommending clinical decisions beyond what was explicitly documented (Table 1). Each \ntask was further stratified by the phase of care: initial presentation, evaluation and diagnosis, and \nongoing clinical management. Prompts were designed collectively with an interdisciplinary team. \nFor questions specific to clinical recommendation tasks, prompting was aligned with NCCN \nguidelines (V.2.2024). All prompts followed a zero-shot format with an additional request to \nprovide chain-of-thought reasoning in the following format: ‘Reason: Let’s think step by step \n[provide explanation here] / Therefore, [answer]’. All queries are provided in Table S2A and Table \nS3A.  \n"}, {"page": 5, "text": " \n5 \nReasoning Task \nDescription \nExtraction \nminimal reasoning: relies heavily on isolating information within the chart \nnote and does not require external context. \nAnalysis \nmoderate reasoning: relies on extracting information and integrating with \nknowledge base to conclude in reasoning. \nRecommendation \ncomplex reasoning: model must extract relevant information, integrate \nwith existing knowledge and then present actionable items for medical \ncare \nClinical Context \nDescription \nPresentation \nquestion addresses medical care of the patient prior to confirmed \ndiagnoses. \nEvaluation \nquestion addresses how the patient was or should be evaluated for \nvarious diagnoses. \nManagement \nquestion addresses how the patient's diagnosis is treated and related \nprognosis. \nTable 1. Summary of Prompt Tasks and the Clinical Contexts of Queries.  \nAll prompting experiments were performed using GPT-4-32k, configured with default \nparameters and a fixed temperature of 0 to minimize variability32,33. Each model output was \nformatted in standardized JSON format for structured annotation.  \nBuilding Error Taxonomy \nWe used GPT-4-32k to generate chain-of-thought reasoning responses from 10 oncology \nnotes in the CORAL dataset. 15 extraction-level prompts were iteratively refined across four \nengineering cycles to optimize output fidelity (Figure S1). Two supervised annotators \nindependently reviewed 150 responses, identifying the first deviation from clinically valid \nreasoning. Initial error types were identified through open coding and iteratively refined by axial \ncoding until theoretical saturation was reached, defined as no new codes in the last 25 reviewed \nresponses. The taxonomy was developed de novo but informed by established literature on \ncognitive errors in clinical reasoning34–37 and natural language processing error classifications29,38. \nThe final schema comprises a three-tier hierarchical taxonomy organized under two broad \ndomains: Comprehension errors and Reasoning errors with two subsequent error tiers (Figure 1). \nEach error type was accompanied by a formal rubric, relative criteria, and abstracted examples \nto provide decision boundaries for reviewers (Table S1). Multiple labels were allowed when \ndistinct and non-cascading failures occurred. \n"}, {"page": 6, "text": " \n6 \nResponses from all annotated notes in the CORAL dataset (notes 0-39) were then \ncategorized into the taxonomy to assess the prevalence and distribution of reasoning errors. \nDiscrepant labels were adjudicated via consensus review to reach a gold standard. \nTesting the Taxonomy \nWe validated the taxonomy on outpatient prostate cancer consult notes. Assessment and \nplan sections were removed to blind inference. An expanded prompt set was comprised of the \noriginal 15 prompts and 45 new NCCN-related prostate cancer queries stratified by task \n(extraction, \nanalysis, \nrecommendation) \nand \nclinical \nphase \n(presentation, \nevaluation, \nmanagement) (Table S3A). Using identical generation parameters, GPT-4 generated 822 new \nCoT responses. Two independent annotators reviewed all outputs following the same schema as \nin development. Each recommendation was rated independently by two blinded reviewers using \na 5-point Likert scale that assessed potential impact on patient outcomes, ranging from clinically \nharmful (1) to clinically optimal (5). We also rated them for guideline concordance against 2024 \nNCCN recommendations (Table 2). \nClinical Impact \n Description \n1 \nModel response is inaccurate and may be harmful to patient outcomes if \naccepted. \n2 \nModel response is inaccurate but would be unlikely to negatively affect \npatient outcomes if accepted. \n3 \nModel response is misleading or lacking, though factually accurate. \nResponse would not affect patient outcomes. \n4 \nModel response is accurate, though may be considered suboptimal in a \nclinical environment. \n5 \nModel response is accurate and accepted in a clinical environment. (aligns \nwith physician A/P) \nGuideline \n Description \ndiscordant \nmodel response deviates from NCCN guidelines and is inappropriate. \nalternative \nmodel response deviates from NCCN guidelines or is incomplete, though \nappropriate.  \nconcordant \nmodel response aligns with NCCN guidelines.  \nTable 2. Criteria for clinical assessment of responses. \n \n"}, {"page": 7, "text": " \n7 \nAuto-Evaluation \nWe developed an automated evaluation framework wherein LLMs classified response \nerrors using the same hierarchical taxonomy applied in the previous manual annotation phases. \nThree state-of-the-art models: GPT-4-Turbo (OpenAI, 2024), Claude 3.5-Sonnet (Anthropic, \n2024), and Gemini 1.5-Pro (Google, 2024), were tested as automated evaluators. Each model \ncategorized every response at Tier 1 as a correct response, Comprehension Error, or Reasoning \nError, then advanced through Tier 2 and Tier 3 classifications for more specific subtypes of failure. \nWe implemented two approaches for the evaluation framework. In the triple-prompting strategy, \nthe evaluation advanced sequentially across taxonomy tiers. A first prompt assigned the response \nto a Tier 1 class, a second prompt refined that classification into the appropriate Tier 2 subtype, \nand a third prompt further resolved it to a Tier 3 error category. The decomposed strategy began \nsimilarly by distinguishing correct from incorrect responses at Tier 1, but it bypassed Tier 2 \nclassification to query each Tier 3 subtype directly. The model was asked a series of binary \nquestions (e.g., “Is this Tier 3 error present? Yes/No.”) across all possible subtypes.  \nData Analysis \nAll data processing and statistical analysis were conducted in R (v4.3.1). Cohen’s kappa \nwas calculated across tiers to assess inter-rater reliability. Mean error rates and standard errors \nwere calculated across cancer type, task type, context, and reviewer, and summarized both by \nindividual tier and aggregated categories. For multivariable logistic regression models, predictor \nvariables included reasoning task, clinical context, clinical concept, and prostate cancer stage. \nError presence was modeled as a binary outcome. Model coefficients were exponentiated to \ngenerate odds ratios with 95% confidence intervals. For clinical recommendation tasks, average \nimpact scores were calculated as the mean of two independent reviewer Likert ratings (range: 1 \n= harmful to 5 = optimal). Between-group comparisons of impact scores (error vs. no-error) \nwere performed using the Wilcoxon rank-sum test. All categorical and continuous analyses were \nconducted using standard thresholds (p < 0.05 for significance). \n \n \n"}, {"page": 8, "text": " \n8 \nRESULTS \nTaxonomy Development \nTo create an error taxonomy for clinical reasoning breakdowns, we analyzed chain-of-\nthought (CoT) responses generated by GPT-4-32k from extraction-level prompts. We used \nextraction-only prompts in this phase to minimize confounding sources of variability in model \nbehavior introduced by guideline knowledge or complex clinical decision-making. Briefly, stepwise \nreview of each reasoning sequence identified recurring patterns of clinical logic breakdowns. \nThrough iterative refinement to saturation (Figure S1), these patterns were organized into a \nhierarchical taxonomy with two Tier 1 domains of Comprehension and Reasoning, further \nsubdivided into four Tier 2 categories and nine Tier 3 cognitive or logical error types. The \ntaxonomy structure is illustrated in Figure 1, and detailed guidelines are included in Table S1. \n \nFigure 1. Summary of Error Taxonomy. See Table S1 for further details and examples.  \nValidation in CORAL  \nThe final error taxonomy was applied to 600 CoT reasoning responses generated by GPT-\n4-32k from 40 oncology progress notes in the CORAL dataset (Table S2). Inter-reviewer \nagreement remained high across all tiers (κ = 0.85-0.89), confirming reproducibility of the \nframework (Figure 2A). The overall error rate was 27.4% (±1.2%), with higher frequency in breast \nComprehension\nContextual Incoherence\nMisunderstanding \nthe Question\nRedundancy\nHallucination\nSemantic Drift\nDid the model incorrectly extract relevant information \nfor the question or answer?\nDid the model extract \nirrelevant \ninformation?\nWas irrelevancy \ndue to innate \nmisinterpretation of \nthe question?\nWas irrelevancy \ndue to redundant, \nthough appropriate, \nanswers?\nWas irrelevancy due \nto confabulated \ninformation?\nFactual Error\nAnchoring Bias\nContext Error\nWas incorrect step \nbased on a factual \ninaccuracy?\nWas inaccurate step \ndue to emphasis on \na single piece of \ninformation?\nWas inaccurate step \ndue to organization \nof the chart note \nleading to distortion \nof information? \nConfirmation Bias\nPremature \nClosure\nStark Omission\nDid model miss \nstep to validate if \ninformation was \nappropriate for an \nanswer?\nDid the model lack \nsufficient evidence \nwhen reaching an \nappropriate \nanswer?\nDid the model \nignore information \nto identify an \nappropriate \nanswer?\nDid the model \nproduce irrelevant \nanswers by \ndeviating from \ntopic?\nMissing Steps\nDid the model \ngenerate incomplete \nreasoning?\nFalse Association\nDid the model generate \nreasoning with an \nincorrect step?\nReasoning\nDid the model incorrectly interpret or relate information to \nreach an answer?\n"}, {"page": 9, "text": " \n9 \ncancer (BCa) notes, specifically during extraction of patient presenting features and management \ndetails. The highest error rate was observed in BCa responses related to clinical presentation \n(35.1%), while PDAC prompts showed the lowest rate in that same category (19.0%) (Figure 2B). \nAmongst erroneous responses (n=129), reasoning failures dominated (96.7%), while \ncomprehension errors were rare (Figure 2C). In Tier 2, the most common error subtypes were \nFalse Association (49.2%) and Missing Step (45.8%). Tier 3 errors were more diverse, with \nConfirmation Bias (25.2%), Factual Error (21.9%), and use of Chart Context (20.2%) representing \nthe most frequent subtypes. Remaining subtypes accounted for <10% of errors. Error frequency \ndid not vary by chart note or its file size (p = 0.736). Collectively, these findings demonstrate that \neven during extraction tasks, GPT-4 produced consistent, clinically interpretable reasoning \nfailures, supporting the taxonomy’s validity for identifying error patterns. \n \n \nFigure 2. Taxonomy-derived reasoning errors when extracting clinical data from \noncology progress notes. (A) Inter-rater reliability for categorizing errors into tiers of \ncategorization schema from Figure 1. Bars show Cohen’s kappa for each tier of the taxonomy: \nTier 1 (red), Tier 2 (blue), and Tier 3 (orange), indicating level of agreement between \nreviewers. (B) Error rate by clinical context and cancer type. Bar plots show the percentage of \nLLM outputs containing ≥1 error for each clinical context (evaluation, management, \npresentation), stratified by cancer type: breast cancer (BCa, pink) and pancreatic ductal \nadenocarcinoma (PDAC, gray). Error bars represent standard error across reviewers. (C) \nA\nC\nB\n0\n25\n50\n75\n100\nReasoning\nComprehension\nFalse Association\nMissing Step\nContext. Incoherence\nSemantic Drift\nConfirmation Bias\nFactual Error\nChart Context\nStark Omission\nAnchoring Bias\nPremature Closure\nRedundancy\nSemantic Drift\nMisunderstanding\nQuestion\nError Rate (%, within tier)\nTier 1\nTier 2\nTier 3\n0%\n10%\n20%\n30%\nEvaluation\nManagement\nPresentation\nClinical Context\nError Rate (%)\nCancer\nBCa\nPDAC\n0.88\n0.85\n0.86\nTier 1\nTier 2\nTier 3\n0.00\n0.25\n0.50\n0.75\n1.00\nKappa \n"}, {"page": 10, "text": " \n10 \nDistribution of error rates by clinical context (presentation, evaluation, management) within \nincorrect responses. Each bar shows percentage of responses containing ≥1 error. \n \nApplication in Prostate Cancer Patients \nTo validate the taxonomy in a distinct clinical context, we applied it to 822 chain-of-thought \nresponses generated by GPT-4-32k from 24 prostate cancer consult notes encompassing defined \ndisease stages (Table S3). Inter-rater reliability remained high across all three tiers of the \ntaxonomy (κ = 0.87–0.89; Figure 3A).  \nExtraction prompts exhibited error rates of 25.7% in evaluation, 19.8% in management, \nand 27.1% in presentation (Figure 3B). Analysis prompts were more accurate on average, yet \nrecommendation prompts showed a notable increase in error rates for clinical management. Error \nrates differed slightly by cancer stage (Figure 3C), though all within a narrow error rate. Examining \nthe error distribution again revealed that reasoning failures (Tier 1) dominated across tasks \n(Figure 3D). Tier 3 errors clustered more variably. Confirmation bias was highest for analysis \ntasks (31.6%), while premature closure and anchoring bias were highest for recommendation \ntasks. \nMultivariable modeling identified prompt and context factors associated with error risk \n(Figure 3E). Compared with diagnostic queries, laboratory and imaging prompts were significantly \nless error-prone (OR 0.57 [0.40–0.81]). Evaluation-phase contexts carried lower error risk than \npresentation (OR 0.52 [0.31–0.88]), and queries regarding CSPC had marginally higher odds of \nerror versus localized disease. Analysis tasks were also less error-prone than extraction, while \nrecommendation tasks did not differ significantly after regression. Overall, the taxonomy proved \nreproducible across disease stages and reasoning tasks, delineating the clinical contexts where \nGPT-4 may have reasoning vulnerabilities.  \n \n \n \n"}, {"page": 11, "text": " \n11 \n \nFigure 3. Errors when interpreting prostate cancer consult notes across clinical tasks \nand contexts. (A) Inter-rater reliability (Cohen’s kappa) for Tier 1–3 error classification across \n600 GPT-4-generated reasoning chains. (B) Error rates by reasoning task: Extraction (minimal \nreasoning), Analysis (intermediate reasoning), and Recommendation (complex reasoning). \nResults were also stratified by clinical context: Presentation (initial clinical presentation), \nEvaluation (testing and diagnosis), and Management (treatment and future prognosis). Each \nresponse was assigned to a task-context pair and reviewed for the presence of ≥1 error. (C) \nOverall error rate stratified by prostate cancer stage: Localized, Biochemical Recurrence, \n19.7\n31.6\n5.3\n22.4\n2.6\n1.3\n9.2\n5.3\n2.6\n24.6\n18.6\n18.6\n1.2\n3\n12.6\n3\n6.6\n29.1\n11.6\n1.2\n17.4\n5.8\n15.1\n16.3\n3.5\nMisunderstanding\nQuestion\nRedundancy\nHallucination\nFactual Error\nAnchoring Bias\nChart Context Error\nConfirmation Bias\nPremature Closure\nStark Omission\nExtraction\nRecommendation\nSemantic Drift\nContextual\nIncoherence\nFalse Association\nMissing Step\nComprehension\nReasoning\nTask\nError Type\nType Error Rate (%)\nAnalysis \n6.6\n46.1\n47.4\n10.8\n37.1\n40.1\n12\n9.3\n47.7\n43\n6.6\n93.4\n22.8\n77.2\n9.3\n90.7\nTier 1\nTier 2\nTier 3\n0.88\n0.87\n0.89\n0.00\n0.25\n0.50\n0.75\n1.00\nTier 1\nTier 2\nTier 3\nKappa \nCategory\nA\nC\nD\nE\nB\n0%\n10%\n20%\nBio.Recur.\nCSPC\nLocal\nM1 MCRPC\nPCA Stage\nError Rate (%)\nEvaluation\nBiochemical Recurrent\nM1 MCRPC\nCSPC\nTask (vs. Extraction)\nStage (vs. Local)\nClinical Context\n(vs. Presentation)\nClinical Concept\n(vs. Diagnosis)\n0.3\n0.5\n1.0\nLab & Imaging Test\nProcedure\nRisk & Prognosis\nMedication\nTreatment Plan\nMedical History\nSigns and Symptoms\nManagement\nAnalysis\nRecommendation\nOdds Ratio \np.value < 0.05\n0.0\n0.1\n0.2\n0.3\nExtraction\nAnalysis\nRecommendation\nTask\nError Rate \nEvaluation\nManagement\nPresentation\n"}, {"page": 12, "text": " \n12 \nCSPC (castration-sensitive prostate cancer), and mCRPC (metastatic castration-resistant \nprostate cancer). Each note was manually staged based on clinical content; errors were \naveraged across all prompts per stage. (D) Heatmap showing frequency of each error subtype \nacross tasks, within erroneous responses. Rows represent individual error types across error \ntaxonomy tiers. Columns correspond to clinical task types. Error rates reflect the proportion of \nerroneous responses in each task that contained a given error subtype, allowing multi-label \nclassification. Reviewers applied the predefined rubric to identify the earliest identifiable \nreasoning failure(s) in each CoT response. (E) Multivariable logistic regression of prompt \nfeatures predicting reasoning errors. Odds ratios and 95% confidence intervals are shown; \nstatistically significant associations (p < 0.05) are marked with triangles. \nTo determine whether specific reasoning errors in GPT-4 outputs correlate with clinically \nmeaningful risks, we analyzed 487 reasoning chains restricted to clinical recommendation tasks. \nSuch tasks were specifically aligned with 2024 NCCN guidelines39. Clinical impact was rated on \na 5-point Likert scale (1 = potentially harmful, 5 = clinically optimal; Table 2), and we also \nevaluated for NCCN guideline concordance. Inter-reviewer agreement was strong for both metrics \n(Figure 4A). Recommendations containing reasoning errors received markedly lower clinical-\nimpact scores (p < 0.0001) (Figure 4B). These responses were also more likely to deviate from \nNCCN guidelines (Figure 4C). Assumed risk was greatest for management-phase prompts and \nmetastatic castration-resistant disease. By contrast, responses addressing localized disease and \nevaluation-phase prompts generally showed higher (safer) impact scores (Figure 4D). \nCertain reasoning error types were strongly associated with clinical harm. Amongst all \nerrors detected within this dataset, Tier 1 comprehension errors were rare and carried limited \nclinical consequence (Figure 4E). Among Tier 2 subtypes, false associations and missing \nreasoning steps accounted for a large fraction of harmful recommendations. Tier 3 biases such \nas confirmation bias, anchoring bias, and stark omission were consistently linked to the lowest \nimpact scores (Likert = 1). Thus, certain GPT-4 reasoning failures manifest as guideline-\ndiscordant recommendations that could adversely influence patient care if used. \n \n"}, {"page": 13, "text": " \n13 \n \nFigure 4. Clinical impact of reasoning errors in recommendations for prostate \ncancer patients. (A) Inter-rater agreement for two blinded clinical review tasks—NCCN \nguideline concordance and clinical impact scoring—was measured using Cohen’s kappa \nacross 487 GPT-4-generated recommendation responses. Concordance scoring \nassessed whether outputs aligned with NCCN guidelines; clinical impact was rated on a \n5-point scale reflecting the potential benefit or harm of each recommendation. (B) Clinical \nimpact scores were compared across all responses, responses with at least one reasoning \nerror, and those without any detected errors. Responses containing reasoning errors had \nsignificantly lower (worse) clinical impact scores compared to those without errors (p < \n0.0001, two-sided t-test). (C) NCCN guideline concordance was summarized as the \nproportion of responses in each group rated as concordant, alternative, or discordant with \n0.81\n0.85\n0.00\n0.25\n0.50\n0.75\n1.00\nConcordance\nClinical Impact\nKappa \nA\nB\nC\nD\nE\np < 0.0001\n1\n2\n3\n4\n5\nAll Responses\nReasoning \nErrors\nExcluding \nReasoning Errors\nClinical Impact\n(reviewer average)\nAll Responses\nReasoning \nErrors\nExcluding \nReasoning Errors\nalternative\nconcordant\nNCCN Concordance\n(reviewer pooled )\ndiscordant\nProportion of responses\n(by column) 0.25 0.50 0.75\nConfirmation Bias\nStark Omission\nAnchoring Bias\nFactual Error\nMisunderstanding \nthe Question\nPremature\nContext of Chart Error\nRedundancy\nTier 2\nTier 3\nTier 1\n1\n2\n3\n4\n5\nReasoning\nClinical Impact Score\nFalse Association\nContextual \nIncoherence\nMissing Step\nComprehension\nns\n***\nns*\nns\n**\nns\nns\n**\nns\n**\n***\nManagement\nEvaluation\n2\n4\nBiochemical\nRecurrent\nCSPC\nLOCAL\nM1 MCRPC\nBiochemical\nRecurrent\nCSPC\nLOCAL\nM1 MCRPC\nIMPACT\nClinical Recommendation Questions\n"}, {"page": 14, "text": " \n14 \nguideline-based care. Circle size and color indicate the frequency of each concordance \nlevel, showing increased discordance among error-containing outputs. (D) Clinical impact \nscores stratified by prostate cancer stage and clinical context. Within each context, clinical \nimpact scores were compared using Wilcoxon rank-sum tests to assess differences \nacross cancer types (ns = p ≥ 0.05, * = p < 0.05, ** = p < 0.01, *** = p < 0.001).  (E) Ridge \nplots of clinical impact scores are shown for responses containing each reasoning error \nsubtype, grouped by taxonomy tier. Each dot represents a GPT-4 recommendation \nlabeled with the specified error type.  \n \nAutomated Evaluation Attempts \nTo scale up our taxonomy for automated evaluation, we implemented a three-tier \nprompting framework in which models sequentially categorized errors from general (Tier 1) to \nspecific (Tier 3) types. Across Gemini-2.0-Flash, GPT-4o, and Claude-3.5-Sonnet, this approach \ndemonstrated reasonable sensitivity for detecting the presence of reasoning failures but \ninconsistent subtype classification. Claude achieved the highest Tier 1 accuracy (Figure 5A), yet \nprecise categorization was inconsistent (Figure 5A-B). Failure modes varied by model (Figure \n5C). GPT-4 and Gemini produced balanced false-positive and false-negative rates, while Claude \nexhibited especially high false-positive rates, overclassifying errors. \n \n"}, {"page": 15, "text": " \n15 \n \nFigure 5. Automated evaluation of errors using three-tiered prompt strategy. Models \nwere prompted three times across taxonomy tiers with instruction to classify each model-\ngenerated response into daughter error categories, and outputs were compared against \nhuman gold-standard annotations from the CORAL oncology dataset. (A) Stacked bar plots \nshow the proportion of responses classified as correct or incorrect. Results are shown for all \nresponses (“All Responses”) and for the subset of responses containing human-annotated \nreasoning errors (“True Mistakes”). (B) Heatmaps display the correspondence between \nhuman gold-standard annotations (x-axis: x = mistake vs. ✓ = no mistake) and auto-\nevaluator accuracy (y-axis: success vs. failure). Cell shading reflects row-normalized \nproportions, and each cell is annotated with the proportion (P) and count (N) of responses; \nchi-square test results are reported in panel titles. (C) Bar plots show the distribution of \nevaluator-provided reasons among misclassified responses within the “True Mistakes” \nsubset. Reasons were grouped by category (e.g., false negatives, false positives, \ndisagreement) and plotted as the proportion of all misclassifications. Each column represents \none model.  \n0%\n25%\n50%\n75% 100%\nProportion\nP=0.26\nN=31\nP=0.74\nN=89\nP=0.95\nN=457\nP=0.05\nN=26\nX\n✓\nSuccess\nFailure\nEvaluator Accuracy\nHuman Annotation\np = 4.49e−65\nP=0.25\nN=30\nP=0.75\nN=90\nP=0.89\nN=428\nP=0.11\nN=55\nX\n✓\nSuccess\nFailure\nEvaluator Accuracy\nHuman Annotation\np = 1.77e−47\nP=0.35\nN=42\nP=0.65\nN=78\nP=0.87\nN=421\nP=0.13\nN=62\nX\n✓\nSuccess\nFailure\nEvaluator Accuracy\nHuman Annotation\np = 3.92e−33\n0%\n10%\n20%\n30%\n40%\n50%\nProportion\nClaude-3.5-Sonnet\n0%\n25%\n50%\n75% 100%\nProportion\n0%\n10%\n20%\n30%\n40%\n50%\nDisagree\nFalse\nNegative\nFalse \nPositive\nDisagree\nFalse\nNegative\nFalse \nPositive\nDisagree\nFalse\nNegative\nFalse \nPositive\nProportion\nGPT-4o\n0%\n25%\n50%\n75% 100%\nTotal\nTrue\nMistakes\nProportion\n0%\n10%\n20%\n30%\n40%\n50%\nReason for \nEvaluator Failure\nProportion\nGemini-2.0-Flash\nEvaluator \nAccuracy\ncorrect\nincorrect\nTier-based triple prompt evaluator\nA\nB\nC\n"}, {"page": 16, "text": " \n16 \nWe next tested a decomposed prompting strategy in which each potential error subtype \nwas queried independently (Mistake 1à Mistake 2 …. à Mistake n), allowing the LLM to self-\narbitrate. Across models, overall accuracy in detecting and classifying errors remained similar \n(Figure 6A). Claude-3.5-Sonnet demonstrated a meaningful improvement compared to the \nprevious strategy, increasing agreement with gold-standard labels to 44% (versus 26% in Figure \n5) (Figure 6B). Gemini and GPT-4 had more false negatives (under-classification) and \npersistent false positives (over-classification) (Figure 6C). Claude’s distribution of failure modes \nwas relatively unchanged. Although decomposed prompting moderately improved \ncategorization in Claude, it did not uniformly enhance our preceding results. \n \n \nFigure 6. Decomposed prompting with self-arbitration for automated evaluation. \nModels were prompted to evaluate each possible error subtype sequentially (Mistake 1 → \nMistake 2 … → Mistake n), with outputs compared against human gold-standard annotations \nfrom the CORAL oncology dataset. (A) Stacked bar plots show the proportion of responses \nError type multi-prompt evaluator\nClaude-3.5-Sonnet\nGPT-4o\nGemini-2.0.Flash\nEvaluator \nAccuracy\ncorrect\nincorrect\n0% \n25% \n50% \n75% \n100%\nProportion\nP=0.44\nN=53\nP=0.56\nN=67\nP=0.92\nN=445\nP=0.08\nN=38\np = 1.38e−34\n0%\n20%\n40%\n60%\nProportion\n0% \n25% \n50% \n75% \n100%\nProportion\nP=0.28\nN=34\nP=0.72\nN=86\nP=0.91\nN=438\nP=0.09\nN=45\np = 6.46e−49\n0%\n20%\n40%\n60%\nProportion\nTotal\nTrue\nMistakes\n0% \n25% \n50% \n75% \n100%\nProportion\nP=0.29\nN=35\nA\nB\nC\nP=0.71\nN=85\nP=0.94\nN=454\nP=0.06\nN=29\np = 2.46e−58\n0%\n20%\n40%\n60%\nProportion\nSuccess\nFailure\nEvaluator Accuracy\nSuccess\nFailure\nEvaluator Accuracy\nSuccess\nFailure\nEvaluator Accuracy\nX\nHuman Annotation\nX\n✓\n✓\nHuman Annotation\nX\n✓\nHuman Annotation\nReason for \nEvaluator Failure\nDisagree\nFalse\nNegative\nFalse \nPositive\nDisagree\nDisagree\nFalse\nNegative\nFalse\nNegative\nFalse \nPositive\nDisagree\nDisagree\nFalse\nNegative\nFalse \nPositive\nFalse \nPositive\n"}, {"page": 17, "text": " \n17 \nclassified as correct or incorrect, stratified by “All Responses” versus the subset of \nresponses with human-annotated reasoning errors. (B) Heatmaps display the \ncorrespondence between human annotations (x-axis: x = mistake vs. ✓ = no mistake) and \nauto-evaluator accuracy to that gold-standard. Row-normalized proportions are indicated by \nshading, with each tile labeled by proportion (P) and count (N); chi-square test results are \nreported in panel titles. (C) Bar plots show the relative distribution of evaluator-provided \nreasons among misclassified responses within the “True Mistakes” subset, grouped by \nreason category. Each column represents one model.  \n \n \nDISCUSSION \nGPT-4 committed reasoning errors linked to cognitive biases in 19.7% of oncology note \ninterpretations. Among recommendations containing reasoning errors, clinical impact scores \nwere significantly lower, with confirmation bias, anchoring bias, and stark omission \ndisproportionately represented among unsafe outputs. This association between reasoning \nfidelity and clinical safety represents a fundamental gap in current LLM evaluation paradigms, \nwhich prioritize endpoint accuracy over logical validity of intermediate reasoning steps40,41. \nThe reproducible hierarchical taxonomy we developed reveals reasoning failures could \nexplain 85.4% of mistakes. This three-tier framework, applied across multiple cancer types, \nprovides a clinically conscious approach to classify LLM reasoning errors. By mapping \ncomputational failures onto established cognitive bias frameworks, our taxonomy bridges \ncomputer science error analysis and clinical reasoning literature42,43. Confirmation bias \ndominated extraction and analysis tasks (31.6% of errors), wherein GPT-4 selectively attended \nto data aligning with preliminary impressions while dismissing contradictory evidence. Anchoring \nbias was most prevalent in recommendation tasks, where the model over-weighted initial clinical \nfindings. This task-dependent error distribution implies different contexts elicit distinct failure \nmodes: extraction triggers pattern-matching errors, while recommendation tasks requiring \nweighing uncertainty expose insufficient deliberation44–46. Increased error rates in advanced \ndisease states amplify this risk; the model performs worst precisely when clinical stakes are \nhigh and complex. \nThese findings expose a limitation of accuracy-centric benchmarks, like USMLE and \nAIME47,48, where models can reach correct answers through faulty reasoning. In controlled \n"}, {"page": 18, "text": " \n18 \nsettings this distinction may appear academic49, yet in clinical practice it becomes critical. \nClinical notes are nonlinear documents where contextual cues, provisional impressions, and \nobjective data coexist in complex relationships50. Reasoning through these requires causal \ninference, temporal sequencing, and some skepticism, capabilities that endpoint accuracy alone \ncannot capture51,52. Our taxonomy-based approach exposes subtle reasoning failures missed by \naccuracy metrics and is scalable to other domains involving complex clinical reasoning. \nPerhaps our most actionable finding is the strong association between reasoning errors \nand guideline discordance. Prospective monitoring of LLM reasoning quality could serve as an \nearly warning system for unsafe recommendations. Error types most strongly associated with \ndiscordance—confirmation bias, anchoring bias, and stark omissions—represent targetable \nfailure modes that could be mitigated through adversarial training data53, debiasing prompts \nrequiring counter-evidence generation54–56, or risk-stratified oversight where management-phase \nqueries in advanced disease warrant mandatory human review57,58. \nWhile models like Claude, GPT-4o, and Gemini demonstrated reasonable sensitivity for \ndetecting error presence, they performed poorly at classifying subtypes. This distinction is \ncritical since not all reasoning failures carry equal clinical risk17,59, and suggests we cannot use \nisolated models for clinical deployment. Instead, a pragmatic framework resembles distributed \nquality assurance: automated evaluators perform initial screening while human experts conduct \ntargeted review of flagged cases60–63. Such \"reasoning audits\" embedded in tumor boards or \ndecision-support workflows would protect patients while improving future models. Regulatory \nframeworks should formalize this human-in-the-loop oversight as a deployment prerequisite. \nStudy limitations include focus on a single model (GPT-4-32k), though this allowed in-\ndepth characterization of one ubiquitous system. Assessment of clinical harm relied on expert \njudgment using NCCN guidelines because this was a retrospective, note-based simulation \nupstream of actual care. We applied zero-shot prompting reflecting realistic usage, though \nengineered prompts might reduce errors, itself a concern if safe deployment requires \nspecialized expertise. Finally, we did not compare LLM reasoning errors directly against human \nclinician errors on identical cases, limiting ability to contextualize errors. \nAs LLMs move from experimental tools to clinical decision-support systems, their value \nwill depend on reasoning soundness, not output fluency. We show GPT-4, despite high \nperformance on benchmarks, commits reasoning errors correlating with guideline-discordant \nand potentially harmful recommendations. The hierarchical taxonomy we developed offers a \ndiagnostic schema for identifying these failures and possibly transferable across clinical \n"}, {"page": 19, "text": " \n19 \ndomains. Ensuring clinical deployment requires models reach correct conclusions through \nlogically valid and clinically defensible reasoning. Anything less risks encoding the very \ncognitive biases causing human clinicians to err, transforming a tool meant to augment \njudgment into one that undermines it. \n \n \n \n \n"}, {"page": 20, "text": " \n20 \n \nSupplementary Figure 1. Framework for Annotating Errors in LLM Responses. \n \n \n1. Development Phase \n2. Validation Phase \nCohort:\n40 Patients (CORAL)\n• \n20 breast cancer cases\n• \n20 pancreatic adenocarcinoma cases\n• \nAll progress notes\nTasks: \n• \nExtraction (n=15) Questions focused \non extracting and analyzing clinical \ninformation found in most oncology \nnotes.  \n \n \n(Table S2A)\n600 GPT4 responses \ngenerated and \nmanually annotated by \nerror taxonomy\n+\n=\nPrompt development\n• Clinical zero-shot tasks for extraction, \nanalysis, and recommendation + chain of \nthough prompting\n4 iterations of prompt \nengineering\nReviewed 150 CoT responses from 15 prompts \nacross 5 breast and 5 pancreatic cancer notes\nAnnotation and post-hoc analysis:\n• Generate hierarchy of chain-of-thought \nreasoning with GPT4-32k\nTasks:\n• \nExtraction (n=15) Questions focused \non extracting and analyzing clinical \ninformation found in most oncology \nnotes. \n• \nPCa stage-specific: NCCN PCa \nguideline-based, clinical analysis and \nrecommendation tasks [localized (10), \nbiochemical recurrence (5), MCRPC \n(5), CSPC (5)] \n \n(Table S3A)\n822 GPT4 responses \ngenerated and \nmanually annotated by \nerror taxonomy, \nclinical impact, and \nguideline \nconcordance\n+\n=\n3. Testing Phase \nMisunderstandin\ng the Question\nRedundancy\nHallucination\nCohort:\n24 Patients (Mayo)\n• \n6 patients per prostate cancer stage \n(local, biochemical recurrence, MCRPC, \nCSPC)\n• \n1 note / patient (exception: additional \nRadOnc or Urology note included for 3/6 of \nlocalized PCa patients)\n• \nAssessment/Plan masked from input\n"}, {"page": 21, "text": " \n21 \nREFERENCES \n1.  \nSinghal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature \n2023;620(7972):172–80.  \n2.  \nVan Veen D, Van Uden C, Blankemeier L, et al. Adapted large language models can outperform \nmedical experts in clinical text summarization. Nat Med 2024;30(4):1134–42.  \n3.  \nZaretsky J, Kim JM, Baskharoun S, et al. Generative Artificial Intelligence to Transform Inpatient \nDischarge Summaries to Patient-Friendly Language and Format. JAMA Network Open \n2024;7(3):e240357.  \n4.  \nSoroush A, Glicksberg BS, Zimlichman E, et al. Large Language Models Are Poor Medical Coders \n— Benchmarking of Medical Code Querying. NEJM AI [Internet] 2024 [cited 2024 Dec 8];Available \nfrom: https://ai.nejm.org/doi/full/10.1056/AIdbp2300040 \n5.  \nHou Z, Liu H, Bian J, He X, Zhuang Y. Enhancing medical coding efficiency through domain-specific \nfine-tuned large language models. npj Health Syst 2025;2(1):14.  \n6.  \nSushil M, Kennedy VE, Mandair D, Miao BY, Zack T, Butte AJ. CORAL: Expert-Curated Oncology \nReports to Advance Language Model Inference. NEJM AI 2024;1(4):AIdbp2300110.  \n7.  \nArtsi Y, Sorin V, Konen E, Glicksberg BS, Nadkarni G, Klang E. Large language models in \nsimplifying radiological reports: systematic review [Internet]. 2024 [cited 2024 Dec \n8];2024.01.05.24300884. Available from: \nhttps://www.medrxiv.org/content/10.1101/2024.01.05.24300884v1 \n8.  \nSerapio A, Chaudhari G, Savage C, et al. An open-source fine-tuned large language model for \nradiological impression generation: a multi-reader performance study. BMC Medical Imaging \n2024;24(1):254.  \n9.  \nHe Z, Bhasuran B, Jin Q, et al. Quality of Answers of Generative Large Language Models Versus \nPeer Users for Interpreting Laboratory Test Results for Lay Patients: Evaluation Study. J Med \nInternet Res 2024;26:e56655.  \n10.  Savage T, Nayak A, Gallo R, Rangan E, Chen JH. Diagnostic reasoning prompts reveal the \npotential for large language model interpretability in medicine. npj Digit Med 2024;7(1):1–7.  \n11.  Goh E, Gallo R, Hom J, et al. Large Language Model Influence on Diagnostic Reasoning: A \nRandomized Clinical Trial. JAMA Network Open 2024;7(10):e2440969.  \n12.  McDuff D, Schaekermann M, Tu T, et al. Towards Accurate Differential Diagnosis with Large \nLanguage Models [Internet]. 2023 [cited 2024 Dec 8];Available from: http://arxiv.org/abs/2312.00164 \n13.  Liévin V, Hother CE, Motzfeldt AG, Winther O. Can large language models reason about medical \nquestions? Patterns 2024;5(3):100943.  \n14.  Aydin S, Karabacak M, Vlachos V, Margetis K. Large language models in patient education: a \nscoping review of applications in medicine. Front Med (Lausanne) 2024;11:1477898.  \n15.  Using large language model to guide patients to create efficient and comprehensive clinical care \nmessage | Journal of the American Medical Informatics Association | Oxford Academic [Internet]. \n[cited 2024 Dec 8];Available from: https://academic.oup.com/jamia/article/31/8/1665/7699038 \n"}, {"page": 22, "text": " \n22 \n16.  Pool J, Indulska M, Sadiq S. Large language models and generative AI in telehealth: a responsible \nuse lens. J Am Med Inform Assoc 2024;31(9):2125–36.  \n17.  Hao Y, Qiu Z, Holmes J, et al. Large language model integrations in cancer decision-making: a \nsystematic review and meta-analysis. npj Digit Med 2025;8(1):450.  \n18.  Carl N, Schramm F, Haggenmüller S, et al. Large language model use in clinical oncology. npj \nPrecis Onc 2024;8(1):1–17.  \n19.  Benary M, Wang XD, Schmidt M, et al. Leveraging Large Language Models for Decision Support in \nPersonalized Oncology. JAMA Network Open 2023;6(11):e2343689.  \n20.  Longwell JB, Hirsch I, Binder F, et al. Performance of Large Language Models on Medical Oncology \nExamination Questions. JAMA Network Open 2024;7(6):e2417641.  \n21.  Brin D, Sorin V, Vaid A, et al. Comparing ChatGPT and GPT-4 performance in USMLE soft skill \nassessments. Sci Rep 2023;13(1):16492.  \n22.  Ullah E, Parwani A, Baig MM, Singh R. Challenges and barriers of using large language models \n(LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology – a recent scoping \nreview. Diagnostic Pathology 2024;19(1):43.  \n23.  Bélisle-Pipon J-C. Why we need to be careful with LLMs in medicine. Front Med [Internet] 2024 \n[cited 2024 Dec 8];11. Available from: \nhttps://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1495582/full \n24.  Griot M, Hemptinne C, Vanderdonckt J, Yuksel D. Large Language Models lack essential \nmetacognition for reliable medical reasoning. Nat Commun 2025;16(1):642.  \n25.  Maitland A, Fowkes R, Maitland S. Can ChatGPT pass the MRCP (UK) written examinations? \nAnalysis of performance and errors using a clinical decision-reasoning framework. BMJ Open \n2024;14(3):e080558.  \n26.  Schmidgall S, Harris C, Essien I, et al. Evaluation and mitigation of cognitive biases in medical \nlanguage models. npj Digit Med 2024;7(1):1–9.  \n27.  Mahajan A, Obermeyer Z, Daneshjou R, Lester J, Powell D. Cognitive bias in clinical large language \nmodels. NPJ Digit Med 2025;8:428.  \n28.  Blagec K, Kraiger J, Frühwirt W, Samwald M. Benchmark datasets driving artificial intelligence \ndevelopment fail to capture the needs of medical professionals. Journal of Biomedical Informatics \n2023;137:104274.  \n29.  Tyagi N, Parmar M, Kulkarni M, et al. Step-by-Step Reasoning to Solve Grid Puzzles: Where do \nLLMs Falter? [Internet]. 2024 [cited 2025 Jan 7];Available from: http://arxiv.org/abs/2407.14790 \n30.  Gallifant J, Afshar M, Ameen S, et al. The TRIPOD-LLM Statement: A Targeted Guideline For \nReporting Large Language Models Use. medRxiv 2024;2024.07.24.24310930.  \n31.  PhysioNet [Internet]. [cited 2024 Dec 9];Available from: https://physionet.org/ \n32.  GPT-4 32k - API, Providers, Stats | OpenRouter [Internet]. [cited 2024 Dec 9];Available from: \nhttps://openrouter.ai/openai/gpt-4-32k \n33.  GPT-4 | OpenAI [Internet]. [cited 2024 Dec 9];Available from: https://openai.com/index/gpt-4/ \n"}, {"page": 23, "text": " \n23 \n34.  Croskerry P. The importance of cognitive errors in diagnosis and strategies to minimize them. Acad \nMed 2003;78(8):775–80.  \n35.  Croskerry P. Diagnostic Failure: A Cognitive and Affective Approach [Internet]. In: Henriksen K, \nBattles JB, Marks ES, Lewin DI, editors. Advances in Patient Safety: From Research to \nImplementation (Volume 2: Concepts and Methodology). Rockville (MD): Agency for Healthcare \nResearch and Quality (US); 2005 [cited 2025 Jun 29]. Available from: \nhttp://www.ncbi.nlm.nih.gov/books/NBK20487/ \n36.  Norman GR, Monteiro SD, Sherbino J, Ilgen JS, Schmidt HG, Mamede S. The Causes of Errors in \nClinical Reasoning: Cognitive Biases, Knowledge Deficits, and Dual Process Thinking. Acad Med \n2017;92(1):23–30.  \n37.  Dror IE. Cognitive and Human Factors in Expert Decision Making: Six Fallacies and the Eight \nSources of Bias. Anal Chem 2020;92(12):7998–8004.  \n38.  Patel N, Kulkarni M, Parmar M, et al. Multi-LogiEval: Towards Evaluating Multi-Step Logical \nReasoning Ability of Large Language Models [Internet]. 2024 [cited 2025 Jun 29];Available from: \nhttp://arxiv.org/abs/2406.17169 \n39.  National Comprehensive Cancer Network. NCCN Clinical Practice Guidelines in Oncology (NCCN \nGuidelines®): Prostate Cancer.  \n40.  Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on Medical Challenge \nProblems [Internet]. 2023 [cited 2025 Jul 21];Available from: http://arxiv.org/abs/2303.13375 \n41.  Agrawal M, Chen IY, Gulamali F, Joshi S. The evaluation illusion of large language models in \nmedicine. npj Digit Med 2025;8(1):600.  \n42.  Asgari E, Montaña-Brown N, Dubois M, et al. A framework to assess clinical safety and hallucination \nrates of LLMs for medical text summarisation. NPJ Digit Med 2025;8(1):274.  \n43.  Kim J, Podlasek A, Shidara K, Liu F, Alaa A, Bernardo D. Limitations of Large Language Models in \nClinical Problem-Solving Arising from Inflexible Reasoning [Internet]. 2025 [cited 2025 Jul \n21];Available from: http://arxiv.org/abs/2502.04381 \n44.  Nachane SS, Gramopadhye O, Chanda P, et al. Few shot chain-of-thought driven reasoning to \nprompt LLMs for open ended medical question answering [Internet]. 2024 [cited 2024 Dec \n8];Available from: http://arxiv.org/abs/2403.04890 \n45.  Wei J, Wang X, Schuurmans D, et al. Chain-of-Thought Prompting Elicits Reasoning in Large \nLanguage Models [Internet]. 2023 [cited 2025 Jul 21];Available from: \nhttp://arxiv.org/abs/2201.11903 \n46.  Shah K, Xu AY, Sharma Y, et al. Large Language Model Prompting Techniques for Advancement in \nClinical Medicine. J Clin Med 2024;13(17):5101.  \n47.  Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: Potential for AI-\nassisted medical education using large language models. PLOS Digit Health 2023;2(2):e0000198.  \n48.  Patel B, Chakraborty S, Suttle WA, Wang M, Bedi AS, Manocha D. AIME: AI System Optimization \nvia Multiple LLM Evaluators [Internet]. 2024 [cited 2025 Jul 21];Available from: \nhttp://arxiv.org/abs/2410.03131 \n"}, {"page": 24, "text": " \n24 \n49.  McCoy LG, Swamy R, Sagar N, et al. Assessment of Large Language Models in Clinical Reasoning: \nA Novel Benchmarking Study. NEJM AI 2025;2(10):AIdbp2500120.  \n50.  Lee SA, Jain S, Chen A, et al. Clinical decision support using pseudo-notes from multiple streams of \nEHR data. npj Digit Med 2025;8(1):394.  \n51.  Nguyen JK. Human bias in AI models? Anchoring effects and mitigation strategies in large language \nmodels. Journal of Behavioral and Experimental Finance 2024;43:100971.  \n52.  Moëll B, Sand Aronsson F, Akbar S. Medical reasoning in LLMs: an in-depth analysis of DeepSeek \nR1. Front Artif Intell [Internet] 2025 [cited 2025 Jun 29];8. Available from: \nhttps://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1616145/full \n53.  Han T, Nebelung S, Khader F, et al. Medical large language models are susceptible to targeted \nmisinformation attacks. npj Digit Med 2024;7(1):288.  \n54.  Wang L, Chen X, Deng X, et al. Prompt engineering in consistency and reliability with the evidence-\nbased guideline for LLMs. npj Digit Med 2024;7(1):1–9.  \n55.  Afshar M, Gao Y, Wills G, et al. Prompt engineering with a large language model to assist providers \nin responding to patient inquiries: a real-time implementation in the electronic health record. JAMIA \nOpen 2024;7(3):ooae080.  \n56.  Chen B, Zhang Z, Langrené N, Zhu S. Unleashing the potential of prompt engineering for large \nlanguage models. Patterns (N Y) 2025;6(6):101260.  \n57.  Gao Y, Myers S, Chen S, et al. Uncertainty estimation in diagnosis generation from large language \nmodels: next-word probability is not pre-test probability. JAMIA Open 2025;8(1):ooae154.  \n58.  Wu J, Yu Y, Zhou H-Y. Uncertainty Estimation of Large Language Models in Medical Question \nAnswering [Internet]. 2024 [cited 2025 Jul 21];Available from: http://arxiv.org/abs/2407.08662 \n59.  Al-Garadi M, Mungle T, Ahmed A, Sarker A, Miao Z, Matheny ME. Large Language Models in \nHealthcare [Internet]. 2025 [cited 2025 Aug 31];Available from: http://arxiv.org/abs/2503.04748 \n60.  Health C for D and R. Request For Public Comment: Measuring and Evaluating Artificial \nIntelligence-enabled Medical Device Performance in the Real-World. FDA [Internet] 2025 [cited \n2025 Oct 26];Available from: https://www.fda.gov/medical-devices/digital-health-center-\nexcellence/request-public-comment-measuring-and-evaluating-artificial-intelligence-enabled-\nmedical-device \n61.  How AI is reshaping EU medicines regulation: Key findings from the 2024 AI Observatory report | \nBecaris Publishing [Internet]. 2025 [cited 2025 Oct 26];Available from: \nhttps://becarispublishing.com/digital-content/blog-post/ai-reshaping-eu-medicines-regulation-key-\nfindings-2024-ai-observatory-report \n62.  Burns B, Nemelka B, Arora A. Practical implementation of generative artificial intelligence systems \nin healthcare: A United States perspective. Future Healthc J 2024;11(3):100166.  \n63.  Rosenthal JT, Beecy A, Sabuncu MR. Rethinking clinical trials for medical AI with dynamic \ndeployments of adaptive systems. NPJ Digit Med 2025;8:252.  \n \n"}]}