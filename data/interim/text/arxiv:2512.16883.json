{"doc_id": "arxiv:2512.16883", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2512.16883.pdf", "meta": {"doc_id": "arxiv:2512.16883", "source": "arxiv", "arxiv_id": "2512.16883", "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning", "authors": ["Tzu-Han Lin", "Wei-Lin Chen", "Chen-An Li", "Hung-yi Lee", "Yun-Nung Chen", "Yu Meng"], "published": "2025-12-18T18:50:01Z", "updated": "2025-12-18T18:50:01Z", "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2512.16883v1", "url_pdf": "https://arxiv.org/pdf/2512.16883.pdf", "meta_path": "data/raw/arxiv/meta/2512.16883.json", "sha256": "5faff28da1071ab6ee7271dc4e61e42be64bd0029f5a39ffd3a096c68377f11a", "status": "ok", "fetched_at": "2026-02-18T02:24:07.288108+00:00"}, "pages": [{"page": 1, "text": "Preprint.\nADASEARCH:\nBALANCING PARAMETRIC KNOWL-\nEDGE AND SEARCH IN LARGE LANGUAGE MODELS\nVIA REINFORCEMENT LEARNING\nTzu-Han Lin1, Wei-Lin Chen2, Chen-An Li1, Hung-yi Lee1, Yun-Nung Chen1, Yu Meng2\n1National Taiwan University\n2Department of Computer Science, University of Virginia\n{r12944034,r13942069,hungyilee}@ntu.edu.tw\ny.v.chen@ieee.org\n{wlchen,yumeng5}@virginia.edu\nABSTRACT\nEquipping large language models (LLMs) with search engines via reinforcement\nlearning (RL) has emerged as an effective approach for building search agents.\nHowever, overreliance on search introduces unnecessary cost and risks exposure\nto noisy or malicious content, while relying solely on parametric knowledge risks\nhallucination. The central challenge is to develop agents that adaptively balance\nparametric knowledge with external search, invoking search only when neces-\nsary. Prior work mitigates search overuse by shaping rewards around the number\nof tool calls. However, these penalties require substantial reward engineering,\nprovide ambiguous credit assignment, and can be exploited by agents that su-\nperficially reduce calls. Moreover, evaluating performance solely through call\ncounts conflates necessary and unnecessary search, obscuring the measurement of\ntrue adaptive behavior. To address these limitations, we first quantify the self-\nknowledge awareness of existing search agents via an F1-based decision met-\nric, revealing that methods such as Search-R1 often overlook readily available\nparametric knowledge. Motivated by these findings, we propose ADASEARCH, a\nsimple two-stage, outcome-driven RL framework that disentangles problem solv-\ning from the decision of whether to invoke search, and makes this decision pro-\ncess explicit and interpretable. This transparency is crucial for high-stakes do-\nmains such as finance and medical question answering, yet is largely neglected by\nprior approaches. Experiments across multiple model families and sizes demon-\nstrate that ADASEARCH substantially improves knowledge-boundary awareness,\nreduces unnecessary search calls, preserves strong task performance, and offers\nmore transparent, interpretable decision behaviors.1\n1\nINTRODUCTION\nLarge language models (LLMs) have achieved significant advances across various natural language\nprocessing tasks (Brown et al., 2020; Hendrycks et al., 2020; Team et al., 2023; Touvron et al.,\n2023; Wei et al., 2022; Guo et al., 2025). Yet they face inherent limitations: parametric knowledge\nalone cannot fully capture domain-specific (Lewis et al., 2020; Yang et al., 2024) or rapidly evolving\ninformation (Kasai et al., 2023; Vu et al., 2024), and models may hallucinate when queried beyond\ntheir knowledge boundary (Ji et al., 2023; Xu et al., 2024; Huang et al., 2025a; Kalai et al., 2025).\nTo overcome these challenges, integrating LLMs with external knowledge sources has become a\ncrucial direction (Karpukhin et al., 2020; Guu et al., 2020; Nakano et al., 2021; Lazaridou et al.,\n2022; Shi et al., 2024; Press et al., 2023; Nakano et al., 2021; Feng et al., 2024).\nEarly work has primarily focused on retrieval-augmented generation (RAG) (Lewis et al., 2020;\nBorgeaud et al., 2022), which searches for relevant passages and appends them to the context before\ngeneration (Shuster et al., 2021; Ram et al., 2023; Jiang et al., 2023; Asai et al., 2024; Wei et al.,\n2024). However, the relatively static pipeline of RAG overlooks the model’s reasoning process,\n1Code and artifacts are available at https://github.com/hank0316/AdaSearch\n1\narXiv:2512.16883v1  [cs.CL]  18 Dec 2025\n"}, {"page": 2, "text": "Preprint.\nStage 2: Problem Solving\nUser Question: Which designer is famous for his/her red-soled footwear?\n[Goal: Assess whether self-knowledge sufﬁces.]\nStep 1:  I need to think of a designer known for \nred-soled footwear. …\nBased on this information, I can answer the \nquestion.\n<assessment>yes</assessment>\nTo determine which designer …\n… the designer famous for his/her red-soled \nfootwear is Christian Louboutin.\n<answer>Christian Louboutin</answer>\nTo determine which designer …\n…\nOne of the most famous designers known for \nred-soled footwear is Alexander McQueen. \n…\n<answer>Alexander McQueen</answer>\nTo ﬁnd out which designer \nis famous for their red-soled footwear, I will need \nto search for information about notable footwear \ndesigners and their signature styles.\n<search>Which designer is famous for …</search>\n…\n<answer>Christian Louboutin</answer>\nExplicit Decision \nwith Rationale\nOver-Search\nUnder-Search \n& Hallucinate\nSolve w/o Search\nStage 1: Decision Making\nSearch-R1\nRL + # Search Penalty\nAdaSearch (Ours)\nFigure 1: Comparison of RL methods for search agents. Left: ADASEARCH provides transpar-\nent and interpretable decisions via explicit reasoning. Conversely, Search-R1 overuses search even\nwhen parametric knowledge suffices, while RL with search penalties results in underuse (leading\nto hallucinations) where the decision rationale remains implicit. Right: ADASEARCH achieves\nthe best overall self-knowledge awareness while preserving task performance. In contrast, Search-\nR1 achieves zero self-knowledge awareness due to its always-search behavior, and reward-shaping\nmethods fail to maintain QA performance.\nyielding suboptimal results (Trivedi et al., 2023). Moreover, enabling dynamic multi-turn search is\nchallenging due to the requirement of large-scale annotated trajectories and the non-differentiable\nnature of the search operation (Schick et al., 2023; Asai et al., 2024; Guan et al., 2025). To address\nthese limitations, treating search as a tool and training tool-integrated LLMs via reinforcement learn-\ning (RL) has emerged as a promising paradigm in recent works (Chen et al., 2025b; Jin et al., 2025a;\nWang et al., 2025; Huang et al., 2025b; Fan et al., 2025). In such settings, models are trained to act\nas search agents, interleaving reasoning with search in a multi-turn, interactive fashion. This allows\nLLMs to issue queries adaptively and integrate retrieved information into their reasoning process,\noutperforming prior prompting-based and supervised approaches.\nNevertheless, building search agents that are truly adaptive remains challenging (Jeong et al., 2024;\nQian et al., 2025; Huang et al., 2025b; Wang et al., 2025). Ideally, a search agent should balance\nparametric knowledge and invoke external search only when necessary. Excessive search calls might\nraise concerns about efficiency, safety, and privacy, potentially increasing unnecessary costs, expos-\ning the agent to noisy or malicious content, and unintentionally leaking sensitive information. Prior\nattempts (Huang et al., 2025b; Wang et al., 2025) encourage adaptivity and mitigate overuse by ty-\ning rewards and evaluation to the number of search invocations. However, reward shaping around\nsearch-call counts requires careful engineering (Table 3), often involving substantial human effort\nand trial-and-error to determine suitable penalties for redundant search. Furthermore, reward credit\nassignment (Pignatelli et al., 2023) may be ambiguous: agents may exploit these signals by crafting\nstronger queries to reduce calls or by superficially avoiding calls even when search is necessary.\nIn addition, naively employing search-call counts as an evaluation metric conflates necessary and\nunnecessary calls, obscuring the measurement of true adaptive behavior.\nBeyond efficiency, a key limitation of prior search-agent frameworks is the lack of transparency\nand interpretability in the decision to invoke search. Because existing methods provide no explicit\nsupervision for this decision, the reasoning process behind “Should I search?” remains implicit\nand difficult for users to interpret. This limitation becomes critical in real-world, high-stakes en-\nvironments such as finance, medical question answering, and legal compliance, where users must\nunderstand why an agent chooses to rely on parametric knowledge or to consult external informa-\ntion. Consider a case where the agent skips search but then produces an incorrect answer: without\nan explicit decision rationale, users cannot determine whether the model was overconfident in its\nknowledge, whether it misinterpreted the query, or whether the failure arose from other factors.\nSuch opacity hinders trust, auditing, and safe deployment of search agents.\nIn this work, we introduce ADASEARCH, a simple outcome-based RL framework that disentangles\nand jointly optimizes two abilities: problem solving and deciding whether to invoke search. Specif-\n2\n"}, {"page": 3, "text": "Preprint.\nStage 1: Decision Making\nStage 2: Problem Solving\n   Agent       \nMφ\nDecision-making \nprompt: Assess \nwhether parametric \nknowledge alone is sufficient\nUser Q:\nWhat year was the director of \nthe movie “Interstellar” born?\nSelf-knowledge\nAssessment\nAssessment:\nNo\nAssessment:\nYes\nParametric-knowledge \nprompt: Use only internal \nknowledge to recall relevant \ninformation\nSearch prompt: Call \nsearch tools to retrieve \nexternal knowledge\n   Agent        \nMφ\nUser Q:\nWhat year was the director \nof the movie “Interstellar” \nborn?\n   Agent        \nMφ\nOutput:\nThe director of the movie \n\"Interstellar\" is Christopher \nNolan, and he was born in 1970. \n<answer>1970</answer>\nOutput:\n1st turn search results indicates \nthe director is Christopher \nNolan.\n2nd turn results shows his birth \nyear is 1970.\nAs a result, the answer is:\n<answer>1970</answer>\nInterleave search\n& reasoning\nFigure 2: Overview of our proposed ADASEARCH framework. In stage 1, the agent explicitly\nreasons to decide whether the query can be solved using parametric knowledge. In stage 2, it follows\nthe parametric-knowledge prompt if the knowledge is sufficient; otherwise, it switches to the search\nprompt to interleave reasoning and search for the final answer.\nically, during training we adopt different prompts to guide LLMs in three modes: (i) solving the\nproblem with internal parametric knowledge, (ii) solving with the problem external search, and (iii)\nexplicitly deciding whether search invocation is needed. At inference time, the model first decides\nwhether the question can be solved using its own knowledge; if so, the model answers the question\ndirectly, otherwise search calls will be invoked as part of the model’s reasoning process (Figure 2).\nUnlike prior approaches that depend on intricate reward engineering and often suffer from ambigu-\nous credit assignment and implicit decision behavior, ADASEARCH simply leverages task outcomes\nas rewards with a training framework that provides clearer learning signals, avoids the pitfalls of\nambiguous tool-count penalties, and enhances the model’s self-knowledge awareness through ex-\nplicit reasoning (Figure 1 left). Experiments and evaluations with our proposed fine-grained self-\nknowledge awareness F1 metric demonstrate that ADASEARCH fosters better adaptivity, reducing\nunnecessary search calls while preserving strong task performance (Figure 1 right).\nIn summary, our contributions are three-fold:\n• We propose ADASEARCH, a simple yet effective multi-task RL framework that explicitly opti-\nmizes both problem solving and decision making to build adaptive search agents.\n• ADASEARCH improves interpretability by generating explicit reasoning during the decision\nstage, enhancing the trustworthiness of search agents in real-world scenarios.\n• ADASEARCH eliminates the need for complex reward engineering while outperforming exist-\ning methods in both problem-solving accuracy and self-knowledge awareness, and generalizes\nacross different model sizes and families.\n2\nEXAMINE SELF-KNOWLEDGE AWARENESS ON VANILLA SEARCH AGENTS\n2.1\nSELF-KNOWLEDGE AWARENESS F1 SCORE\nWe evaluate models’ awareness on parametric knowledge by computing the F1 score, where the\npositive class corresponds to the model deciding that its parametric knowledge is sufficient to answer\nthe input question (equivalently, not invoking search). For brevity, we use the notation F1aware in the\nfollowing sections. To determine whether a test instance is a true or false sample, we check whether\nthe model can in fact solve the instance using parametric knowledge alone.\nFormally, we define\nF1aware = 2 · Precision · Recall\nPrecision + Recall ,\n(1)\nwhere precision and recall are computed on the binary decision of whether to use search, com-\nparing the model’s choice with an oracle label that indicates whether the question is solvable with\nparametric knowledge. This metric reflects model’s awareness on its parametric knowledge.\n3\n"}, {"page": 4, "text": "Preprint.\nPrevious RL-based methods (Jin et al., 2025a; Wang et al., 2025; Huang et al., 2025b) adopt a single\nprompt that asks the agent to conduct the decision-making process and problem-solving simultane-\nously. To compute the score for these methods, two cases arise: (1) if the model does not call search,\nwe directly evaluate the EM of its final answer to determine whether the instance is a true or false\nsample; (2) if the model does call search, we enforce parametric-only reasoning by applying an-\nother system prompt sparam (Appendix E), and use the correctness of the resulting answer to decide\nwhether the instance is a true or false sample.\n2.2\nUNDERUSE OF PARAMETRIC KNOWLEDGE IN VANILLA SEARCH AGENTS\nIn our preliminary experiments, we evaluate prior outcome-based RL methods using Qwen2.5-7B-\nBase (Team, 2024). Specifically, we include (1) Search-R1 (Jin et al., 2025a), which equips LLMs\nwith search access and optimizes them with outcome rewards, and (2) RL w/o Search, which con-\nducts RL training without search access and fully relies on parametric knowledge. QA performance\nis measured by exact match (EM). We follow the testing setup of Jin et al. (2025a) and report macro-\naveraged EM, F1aware, and confusion matrices. We re-evaluate the official Search-R1 checkpoint.2\nFor RL w/o Search, we report the R1-base results from Jin et al. (2025a).\nTable 1: Macro-averaged QA performance (EM),\nself-knowledge awareness (F1aware), and confu-\nsion matrix results on Qwen2.5-7B-Base. TP: true\npositives; TN: true negatives; FP: false positives;\nFN: false negatives.\nMethod\nEM\nF1aware\nTP\nTN\nFP\nFN\nSearch-R1\n39.4\n0.0\n0.0\n72.4\n0.0\n27.6\nRL w/o Search\n27.6\n27.6\n27.6\n0.0\n72.4\n0.0\nThe results are summarized in Table 1.\nFor\nSearch-R1, although search access improves\nEM, its F1aware is 0 because the model invokes\nsearch for every query. This overreliance, re-\nflected in many false negatives, exemplifies tool\noveruse (Qian et al., 2025), where the model\nsearches even when parametric knowledge suf-\nfices.\nIn contrast, the model trained without\nsearch access achieves a higher F1aware but\nlower EM due to limited internal knowledge. These observations highlight the central challenge:\nenabling greater adaptivity by balancing parametric and external knowledge.\n3\nOUR METHOD: ADASEARCH\nWe introduce ADASEARCH, illustrated in Figure 2. The key idea is to achieve adaptivity without\ncomplex reward engineering by disentangling problem solving from the decision of whether to in-\nvoke search. Separating these sub-tasks and optimizing them independently enables training policy\nLLM with simple outcome-based rewards. In this section, we first present the problem formulation\n(Sec. 3.1), and then describe the ADASEARCH framework (Sec. 3.2). Our main approach is a two-\nstage training framework, where the first stage targets problem solving and the second stage focuses\non decision making. Finally, we conclude with the inference pipeline of ADASEARCH (Sec. 3.3).\n3.1\nPROBLEM FORMULATION\nThe problem setting is the same as Jin et al. (2025a). Given an input x, we let policy LLM πθ\ninteract with the search engine E to generate a problem solving trajectory τ. Formally,\nτ ∼πθ(· | x; E).\n(2)\nOur goal is to train the policy πθ to interact with E by maximizing the following RL objective:\nJ(θ) = max\nπθ\nE(x,y)∼D,τ∼πθ(·|x;E) [R(τ, y)] −β DKL (πθ(· | x; E)∥πref(· | x; E)) ,\n(3)\nwhere D = {(xi, yi)}D\ni=1 is the training set with size D, y is the golden answer for input x, R(τ, y)\nis the reward function, and πref is a reference policy. We use GRPO (Shao et al., 2024) as our base\nRL algorithm.\n3.2\nADASEARCH\nStage 1: Problem solving.\nWe focus on incentivizing problem-solving capabilities for the policy\nLLM. Concretely, we train the policy to utilize (1) parametric knowledge and (2) search call for\n2Huggingface: PeterJinGo/SearchR1-nq hotpotqa train-qwen2.5-7b-em-ppo\n4\n"}, {"page": 5, "text": "Preprint.\nAlgorithm 1 ADASEARCH Training\nRequire: base policy LLM πθb; search engine E; training set D = (xi, yi); prompts sparam, ssearch,\nsdecision; string concatenation [·, ·]; solve-rate threshold ρ.\n1: Initialize policy πθ ←πθb\n2: for iteration = 1 to T do\n▷Stage-1 Training Loop\n3:\nSample a batch Db ∼D\n4:\nR ←{}; A ←{}\n5:\nfor each (xi, yi) ∼Db do\n6:\nfor g ∈{param, search} do\n▷Rollout Phase\n7:\nif g is param then\n8:\nGenerate Ri\ng = {τ n\nparam}N\nn=1, where τ n\nparam ∼πθ(· | [sparam, xi]).\n9:\nelse\n10:\nGenerate Ri\ng = {τ m\nsearch}M\nm=1, where τ m\nsearch ∼πθ(· | [ssearch, xi]; E).\n11:\nCompute rewards for all τ ∈Ri\ng with Eq 4.\n12:\nCompute group-wise advantages Ai\ng for Ri\ng.\n13:\nR ←R ∪Ri\ng; A ←A ∪Ai\ng\n▷Aggregate Rollouts & Advantages\n14:\nUpdate πθ using GRPO with R and A.\n15: Generate dataset Ddecision with πθ, D, and sdecision\n▷Detailed in section 3.2\n16: Stage-2 Training: Update πθ with Ddeision using GRPO\n17: return πθ\nproblem solving. We design two different system prompts: (1) parametric-knowledge prompt sparam,\nwhich requires the policy to answer using only its internal knowledge, and (2) search prompt ssearch,\nwhich permits the use of search tools. Full prompt details are provided in Appendix E.\nThe training procedure is depicted in Algorithm 1. For each training instance (x, y) in the training\nbatch, we augment the problem x with both sparam and ssearch, and generate two groups of rollouts:\nRparam = {τ 1\nparam, τ 2\nparam, . . . , τ N\nparam} and Rsearch = {τ 1\nsearch, τ 2\nsearch, . . . , τ M\nsearch}.\nFor each trajectory τ ∈Rparam ∪Rsearch, we use regular expressions to extract the final answer ˆy\nfrom τ and apply exact match EM(ˆy, y) →{true, false}, which checks whether the extracted\nanswer is exactly the same as any of the gold answer after normalization, to check the correctness.\nIn the following sections, we use EM to denote the verification procedure above for simplicity.\nThe reward design in this stage is simply the binary correctness reward. Formally,\nR(τ, y) =\n\u001a1.0\nif EM = true,\n0\notherwise.\n(4)\nFinally, we compute group-wise advantages as in Shao et al. (2024) and update policy with Eq 3.\nStage 2: Decision making.\nThe goal of stage 2 is to incentivize the decision making of search\ninvocations. Before training, we use the stage 1 policy πθ1 and the parametric-knowledge prompt\nsparam to generate pseudo labels. Specifically, for each training instance (x, y), we sample K re-\nsponses from πθ1 and compute the empirical solve rate p using substring exact match (SubEM),\nwhich checks whether any gold answer is a substring of the extracted answer after normalization:\np = 1\nK\nK\nX\nk=1\n1 [SubEM(ˆyk, y) = true] ,\n(5)\nwhere ˆyk is the final answer extracted from the k-th response, and 1[·] is the indicator function. We\nuse SubEM instead of the stricter EM because the objective here is not to evaluate exact answer\nformatting, but to estimate whether the model can solve the problem using its parametric knowledge.\nSubEM provides a more tolerant measure that avoids penalizing semantically correct answers with\nminor formatting differences, making it better suited for generating decision-making supervision.\nThen, we define a threshold ρ and label instances with p ≥ρ as solvable with parametric knowledge.\nWe use the token yes as a label for such instances and the token no otherwise. With the pseudo\nlabels, we craft the dataset Ddecision = {(xi, ℓi)}D\ni=1, where ℓi ∈{yes, no}, for stage-2 training.\n5\n"}, {"page": 6, "text": "Preprint.\nWe design a decision-making system prompt sdecision that requires explicit reasoning before produc-\ning the final decision ℓ, as detailed in Appendix E. Such explicit reasoning increases transparency\nand interpretability of the decision-making process, making it clearer why the agent chooses to in-\nvoke search or rely solely on parametric knowledge. The reward function in this stage remains a\nsimple binary outcome reward, identical to that used in stage 1 (Equation 4).\n3.3\nADASEARCH INFERENCE\nWe adopt a two-stage inference pipeline (Figure 2). In the first stage, the model is prompted with the\nsystem instruction sdecision to decide whether it can answer the query using parametric knowledge\nthrough explicit reasoning. In the second stage, the model receives the corresponding problem-\nsolving instruction: if it decides it can answer with parametric knowledge, we use sparam; otherwise,\nwe use ssearch. Note that the stage-1 history is not prepended in stage 2.\n4\nEXPERIMENTS\n4.1\nEXPERIMENT SETUP\nModels.\nWe conduct experiments on Qwen2.5 (Team, 2024) and Llama-3.2 (Grattafiori et al.,\n2024) to ensure generalizability. Due to compute constraints, our main experiments use the 3B\nInstruct variants, with scaling results presented in Section 4.3.\nSearch environment.\nFollowing Jin et al. (2025a), we use E5 (Wang et al., 2022) as the retriever\nover a 2018 Wikipedia dump, with the implementation from Griggs et al. (2025). For each query,\nwe return the top-3 documents and limit the number of search calls to at most three.\nBaselines.\nWe consider three categories of baselines. For prompting baselines, we select Direct\ninference, Chain-of-Thought (CoT) Reasoning (Wei et al., 2022), and RAG (Lewis et al., 2020).\nFor outcome-RL baselines, we apply GRPO (Shao et al., 2024) to train models on the parametric\nprompt sparam (RL w/o search) and on the search prompt ssearch (akin to Search-R1). For reward-\nshaping baselines, we include IKEA (Huang et al. (2025b), Table 3) as a representative method,\nand also design two baselines. The first one is denoted as “Naive Shaping”. Its reward function is\ndefined as\nR(τ, y) =\n\u001a1.0 −λ · (# search calls)\nif EM = true,\n0\notherwise,\n(6)\nwhich is conceptually identical to OTC (Wang et al., 2025) but easier to implement, assigning higher\nrewards to correct rollouts with fewer search calls. We set λ = 0.05.\nIn addition, we introduce another reward-shaping baseline designed to enforce prompt-level search\ndecisions, which closely resembles the objective of ADASEARCH.\nWe refer to this variant as\n“Awareness Shaping”. Although it directly encourages knowledge-aware search behavior, it still\nunderperforms compared to ADASEARCH (Table 2). Its reward function is defined as\nR(τ, y) = 1 [EM = true] + α ·\n\u001a1 [τ has search]\nif p < ρ,\n1 [τ has no search]\notherwise,\n(7)\nwhere p is the empirical solve rate defined in Eq 5, ρ is the solve-rate threshold, and α a hyper-\nparameter controlling the bonus for correct self-knowledge awareness. This baseline encourages\nawareness by rewarding the model for making the appropriate search decision. In our experiments,\nwe set both α and ρ to 0.5. We estimate p for each sample using the base policy before RL train-\ning. Finally, the prompts used for each baseline and the hyperparameter analysis are detailed in\nAppendix E and Appendix C.2, respectively.\nTraining.\nTo ensure fair comparison across training-based baselines, we construct a difficulty-\nbalanced training set of 8,192 samples following the procedure of Huang et al. (2025b). We estimate\nthe solve rate of each problem using the base policy and split the data into easy and hard subsets\nusing a threshold ρ = 0.5, then sample equal-sized portions from each subset. A validation set\nis constructed using the same procedure and used for checkpoint selection. Full details for dataset\n6\n"}, {"page": 7, "text": "Preprint.\nTable 2: Results of different model across benchmarks. EM denotes exact match. F1aware denotes\nthe self-knowledge awareness score (see Section 2). Avg is computed by averaging EM and F1aware\nover all benchmarks. The best score is shown in bold and the second best in underline.\nMethod\nGeneral QA\nMulti-Hop QA\nAvg.\nNQ\nTQ\nPopQA\nHotpotQA\n2Wiki\nMuSiQue\nBamboogle\nEM\nF1aware\nEM\nF1aware\nEM\nF1aware\nEM\nF1aware\nEM\nF1aware\nEM\nF1aware\nEM\nF1aware\nEM\nF1aware\nQwen2.5-3B-Instruct\nPrompting Baselines\nDirect Answer\n8.1\n8.1\n24.1\n24.1\n7.8\n7.8\n13.2\n13.2\n19.5\n19.5\n1.4\n1.4\n5.6\n5.6\n11.4\n11.4\nCoT\n14.1\n14.1\n38.2\n38.2\n13.5\n13.5\n17.3\n17.3\n22.8\n22.8\n3.9\n3.9\n24.0\n24.0\n19.1\n19.1\nRAG\n31.7\n0.0\n52.7\n0.0\n38.1\n0.0\n24.6\n0.0\n20.1\n0.0\n4.4\n0.0\n8.0\n0.0\n25.7\n0.0\nOutcome RL Baselines\nRL w/o search\n21.7\n21.7\n44.9\n44.9\n17.3\n17.3\n20.5\n20.5\n28.0\n28.0\n5.9\n5.9\n23.2\n23.2\n23.1\n23.1\nSearch-R1\n42.7\n0.0\n60.1\n0.3\n43.9\n0.0\n36.9\n0.1\n37.3\n0.2\n16.1\n0.0\n29.6\n0.0\n38.1\n0.1\nReward-Shaping Baselines\nNaive Shaping\n37.8\n41.6\n54.8\n66.9\n42.2\n55.3\n29.8\n49.3\n33.1\n58.4\n9.9\n15.8\n24.8\n43.8\n33.2\n47.3\nAwareness Shaping 37.7\n45.9\n56.1\n70.3\n41.8\n61.3\n32.1\n57.5\n35.9\n61.1\n12.5\n21.7\n22.4\n41.1\n34.1\n51.3\nIKEA\n41.4\n37.7\n59.5\n62.4\n44.2\n55.2\n32.0\n58.1\n35.0\n60.2\n11.5\n14.2\n23.2\n46.0\n35.3\n47.7\nADASEARCH\n37.9\n49.0\n56.8\n71.7\n42.8\n58.8\n33.4\n57.5\n38.5\n65.5\n14.4\n25.2\n28.0\n50.0\n36.0\n54.0\nLlama-3.2-3B-Instruct\nPrompting Baselines\nDirect Answer\n18.8\n18.8\n42.3\n42.3\n16.4\n16.4\n12.5\n12.5\n16.2\n16.2\n3.0\n3.0\n8.8\n8.8\n16.9\n16.9\nCoT\n25.0\n25.0\n45.6\n45.6\n15.8\n15.8\n16.3\n16.3\n11.3\n11.3\n4.5\n4.5\n35.2\n35.2\n22.0\n22.0\nRAG\n29.4\n0.0\n53.0\n0.0\n35.7\n0.0\n20.6\n0.0\n8.3\n0.0\n4.1\n0.0\n12.8\n0.0\n23.4\n0.0\nOutcome RL Baselines\nRL w/o search\n38.2\n38.2\n54.6\n54.6\n23.1\n23.1\n23.7\n23.7\n26.7\n26.7\n6.7\n6.7\n34.4\n34.4\n29.6\n29.6\nSearch-R1\n46.1\n1.7\n64.4\n2.5\n44.8\n1.5\n38.2\n6.6\n34.8\n1.0\n14.6\n1.3\n42.4\n0.0\n40.7\n2.1\nReward-Shaping Baselines\nNaive Shaping\n41.2\n57.6\n59.7\n75.3\n39.3\n56.3\n31.2\n52.6\n33.5\n54.8\n11.5\n21.1\n36.8\n55.9\n36.2\n53.4\nAwareness Shaping\n41.4\n58.8\n61.5\n75.4\n41.2\n61.4\n33.7\n57.5\n33.7\n48.1\n13.0\n31.4\n32.0\n58.1\n36.6\n55.8\nIKEA\n42.2\n58.6\n62.0\n76.2\n41.1\n59.9\n32.3\n58.2\n32.9\n51.3\n11.2\n31.9\n40.8\n63.1\n37.5\n57.0\nADASEARCH\n43.5\n62.7\n62.9\n79.2\n40.4\n62.3\n34.4\n59.2\n36.2\n60.1\n13.2\n36.5\n42.4\n64.2\n39.0\n60.6\nconstruction are presented in Appendix B.1. For ADASEARCH stage 2, we use the same threshold\nρ = 0.5. Full training details are provided in Appendix B.2.\nEvaluation.\nFollowing Jin et al. (2025a), we use EM as the task metric, apply greedy de-\ncoding, and evaluate on standard QA benchmarks.\nFor single-hop QA, we use Natural Ques-\ntions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2023).\nFor multi-hop QA, we use HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020),\nMuSiQue (Trivedi et al., 2022), and Bamboogle (Press et al., 2023). We also report F1aware (Sec-\ntion 2) to measure self-knowledge awareness.\n4.2\nMAIN RESULTS\nComparison against prompting and outcome-RL baselines.\nTable 2 summarizes the results\nof prompting and outcome-RL baselines. In terms of task performance EM, Search-R1 achieves\nthe best overall results across benchmarks, and the RAG baseline consistently outperforms all\nprompting-based methods. This pattern reflects the characteristics of these benchmarks, where re-\ntrieval generally offers an advantage over purely parametric reasoning; the performance gap between\nRL w/o Search and Search-R1 further supports this observation. Regarding self-knowledge aware-\nness F1aware, however, both the RAG baseline and Search-R1 obtain scores close to zero across\nbenchmarks due to their always-search behavior. The results of Search-R1 highlight the deficiency\nof naive QA correctness rewards in balancing parametric and external knowledge. Non-search base-\nlines achieve higher F1aware, but because they never explicitly assess their own knowledge bound-\naries, their awareness remains suboptimal. In comparison, our ADASEARCH maintains competi-\ntive EM while substantially improving self-knowledge awareness across different model families,\nachieving 54% to 60% relative gain in F1aware over Search-R1. These results highlight that our\nmethod can effectively elicit self-knowledge awareness without compromising task performance.\nComparison against reward-shaping baselines.\nAs shown in Table 2, our ADASEARCH consis-\ntently outperforms intricate reward-shaping baselines in both EM and F1aware. First, Naive Shaping\ngenerally underperforms other baselines in EM, suggesting that naively penalizing tool usage might\nhurt performance. On the other hand, although Awareness Shaping generally shows improvements\n7\n"}, {"page": 8, "text": "Preprint.\nin both EM and F1aware over Naive Shaping, the results are still suboptimal. We suspect that\nthis may be due to distribution shifts: we use the base policy before training to compute the em-\npirical solve rate p, but as training proceeds, the solve rate may change as well. Training on such\noffline-generated labels may therefore introduce noise. Lastly, IKEA (Huang et al., 2025b) generally\noutperforms the other reward-shaping baselines in both EM and F1aware (except on Qwen2.5-3B-\nInstruct). Its reward design can be viewed as combining both Naive Shaping (e.g., penalizing tool\nusage on correct trajectories) and Awareness Shaping (e.g., adding bonuses on incorrect trajectories\nthat involve search calls). However, ADASEARCH still consistently outperforms all reward-shaping\nbaselines across model families, especially on challenging multi-hop QA where search is more gen-\nuinely required. This suggests that reward shaping often over-optimizes for reducing tool calls and\nfails to adapt to query difficulty. In contrast, ADASEARCH strengthens self-knowledge awareness\nwhile achieving better task performance through explicit binary supervision.\n4.3\nANALYSIS\n(a) Confusion matrix.\nMethod\nEM\nF1aware\nPrec.\nRec.\nQwen2.5-3B-Instruct\nSearch-R1\n38.1\n0.1\n39.0\n0.0\nNaive Shaping\n33.2\n47.3\n36.1\n76.8\nAwareness Shaping\n34.1\n51.3\n42.0\n70.3\nIKEA\n35.3\n47.6\n45.0\n53.4\nADASEARCH\n36.0\n54.0\n45.1\n68.8\nADASEARCH-E2E\n35.2\n51.3\n44.1\n63.9\nADASEARCH-SFT\n37.0\n51.1\n45.9\n59.5\nLlama-3.2-3B-Instruct\nSearch-R1\n40.7\n2.1\n47.3\n1.1\nNaive Shaping\n36.2\n53.4\n41.4\n79.0\nAwareness Shaping\n36.6\n55.8\n52.1\n60.5\nIKEA\n37.5\n57.0\n51.4\n64.5\nADASEARCH\n39.0\n60.6\n53.0\n71.4\nADASEARCH-E2E\n37.7\n58.6\n48.1\n75.9\nADASEARCH-SFT\n36.6\n55.6\n46.6\n69.4\n(b) EM, F1aware, precision (Prec.),\nand recall (Rec.).\nFigure 3: Analysis on self-knowledge awareness. (a) Confusion matrix of different RL methods. (b)\nAveraged EM, F1aware, precision (Prec.), and recall (Rec.) across benchmarks.\nADASEARCH achieves better self-knowledge awareness.\nFigure 3 presents the confusion matri-\nces and precision–recall–F1 scores across different methods and model families. For Naive Shaping,\nwe observe the lowest false negative rates but substantially higher false positive rates for both Qwen\nand Llama (Figure 3a). This pattern suggests that naively penalizing search usage causes the model\nto underuse search: it spuriously avoids invoking search even when its parametric knowledge is\ninsufficient, thereby drifting away from adaptive behavior. In contrast, ADASEARCH achieves com-\nparable true positive and false negative rates while significantly reducing the false positive rate.\nFor the other reward-shaping baselines, although they show more balanced false positive and false\nnegative rates, our ADASEARCH still outperforms them in terms of both F1aware and true positive\nrate. A closer examination shows that ADASEARCH attains precision on par with Awareness Shap-\ning and IKEA, while achieving noticeably higher recall (Figure 3b).\nTwo-stage training vs joint optimization.\nTo validate the benefit of two-stage training over joint\noptimization, we implement an end-to-end variant, denoted as ADASEARCH-E2E, which jointly\noptimizes problem solving and self-knowledge awareness. This design enables on-the-fly label gen-\neration for self-awareness by using the empirical solve rate of parametric knowledge at each step to\nproduce pseudo labels. Further details are provided in Appendix D.\nThe results are shown in Figure 3b. ADASEARCH-E2E consistently underperforms ADASEARCH on\nboth EM and F1aware, suggesting that the two-stage approach is more effective than joint training.\nRL vs SFT in stage-2 training.\nA common approach for teaching models to express uncer-\ntainty is supervised fine-tuning (SFT) (Lin et al., 2022; Zhang et al., 2024). In stage 2, we im-\nplement an SFT variant, ADASEARCH-SFT. After computing solve rates as described in Sec-\n8\n"}, {"page": 9, "text": "Preprint.\ntion 3.2, we directly construct target responses: for problems with solve rate p ≥ρ, we assign\n“<assessment>yes</assessment>,” and “<assessment>no</assessment>” other-\nwise, with ρ = 0.5 in our experiments. Additional details are provided in Appendix B.\nThe results are shown in Figure 3b. While ADASEARCH-SFT improves F1aware over Search-R1 and\nachieves slightly higher EM on Qwen, it still underperforms ADASEARCH on Llama across both\nmetrics. A closer look reveals that the F1aware gap is especially large on challenging benchmarks\nsuch as MuSiQue (Trivedi et al., 2022) and Bamboogle (Press et al., 2023), echoing findings from\nChu et al. (2025) that RL generalizes better than SFT to out-of-distribution tasks.\n(a) Problem-solving EM. (b) Avg. EM and F1aware.\n(c) Stage-1 rewards.\n(d) Stage-2 rewards.\nFigure 4: Averaged performance and training rewards across stages. (a) Stage 1 substantially im-\nproves problem solving, and Stage 2 does not degrade it. (b) Both test EM and self-knowledge\nawareness (F1aware) improve throughout training. (c) (d) Stage 1 and Stage 2 respectively incen-\ntivize problem-solving and self-knowledge awareness on the training set.\nLearning curves.\nThe learning curves are shown in Figure 4. In stage 1, the training reward\nincreases steadily (Figure 4c), and the model’s performance on both search-based and parametric\nproblem solving matches the single-task baselines, Search-R1 and RL w/o Search (Figure 4a). In\nstage 2, the training reward also increases smoothly (Figure 4d), while self-knowledge awareness\nimproves substantially and task performance continues to rise (Figure 4b). Moreover, Stage 2 does\nnot degrade problem-solving ability under either prompt type (Figure 4a), consistent with recent\nfindings that on-policy RL mitigates forgetting (Shenfeld et al., 2025; Chen et al., 2025a).\nStage-2 labeling hyperparameters.\nTo understand the effect of labeling hyperparameters for\nstage-2 training, we conduct ablation studies on (1) the number of responses K used to estimate\nthe empirical solve rate in stage 2 and (2) the solve-rate threshold ρ for stage-2 labeling. Additional\nanalysis is provided in Appendix C.1.\nThe results are shown in Figure 5. For (1) (Figure 5a), increasing K improves both F1aware and\nEM by providing a more reliable estimate of the solve rate. Notably, even K = 1 performs well\n(35.1% EM and 52.1% F1aware), likely because RL sharpens output distributions(Yue et al., 2025;\nHe et al., 2025), allowing a single sample to produce effective signals. For (2) (Figure 5b), ρ has\nlimited effect when ρ ≤0.5, aside from a drop in EM at ρ = 0.3. As ρ increases further, F1aware\ndeclines while EM rises, since higher ρ encourages more no assessments, increasing search usage.\n(a) Number of samples K. (b) Solve rate threshold ρ.\nMethod\nEM\nF1aware\nQwen2.5-3B-Instruct\nE5-base\n36.0\n54.0\nBM25\n30.9\n52.4\nQwen2.5-7B-Instruct\nSearch-R1\n43.4\n0.3\nNaive Shaping\n38.1\n55.4\nADASEARCH\n40.2\n59.6\n(c) Retrievers & model size.\nMethod\nQwen\nLlama\nSearch-R1\n1.64\n1.75\nNaive\n0.77\n0.69\nAwareness\n1.00\n1.11\nIKEA\n0.88\n0.79\nADASEARCH\n1.08\n1.10\n(d) Avg. number of search.\nFigure 5: Ablations of ADASEARCH.\n9\n"}, {"page": 10, "text": "Preprint.\nRetriever choices.\nTo test the generalizability of ADASEARCH across different retrievers,\nwe compare ADASEARCH when train and inference with E5-base (Wang et al., 2022) and\nBM25 (Robertson & Walker, 1994). As shown in Figure 5c, ADASEARCH continues to improve\nself-knowledge awareness even with the weaker BM25. While EM drops due to poorer retrieval,\nthe self-awareness signal remains stable, indicating that ADASEARCH is robust to retriever choice.\nModel sizes.\nTo assess the robustness of our method across model scales, we train Qwen2.5-\n7B-Instruct and compare ADASEARCH with Search-R1 and Naive Shaping. As shown in Figure 5c,\nADASEARCH boosts self-knowledge awareness on Qwen2.5-7B-Instruct by roughly 60% in F1aware\ncompared to Search-R1. Relative to Naive Shaping, ADASEARCH improves both EM (+2.1%) and\nF1aware (+4.2%). These results indicate that ADASEARCH generalizes well to larger model scales.\nAverage number of searches.\nWe report the average search calls across all benchmarks.\nADASEARCH reduces unnecessary search usage by 34–38% compared to Search-R1, and its fre-\nquency is similar to Awareness Shaping since both rely on self-knowledge for prompt-level decision\nmaking. ADASEARCH uses slightly more searches than Naive Shaping and IKEA, which is ex-\npected because it does not directly optimize for minimizing tool calls. Further analysis of efficiency\nis provided in Appendix C.3.\n5\nRELATED WORK\nRetrieval-augmented generation.\nRetrieval-augmented generation (RAG) has become a widely\nadopted paradigm for equipping LLMs with external knowledge (Shuster et al., 2021; Ram et al.,\n2023; Jiang et al., 2023; Asai et al., 2024; Wei et al., 2024). By integrating retrieval into the genera-\ntion process, RAG has demonstrated strong potential to mitigate hallucinations and improve output\naccuracy across a range of real-world applications (Jin et al., 2025b; Lu et al., 2022; Tan et al., 2024;\nXiong et al., 2025). Early approaches follow a static retrieve-and-read pipeline (Lewis et al., 2020;\nGuu et al., 2020; Izacard et al., 2023), which is effective for factoid queries but relatively limited\nfor multi-step reasoning. Architectural variants such as RETRO (Borgeaud et al., 2022) deliver\nsubstantial gains but require model modifications and retraining, whereas in-context RAG (Ram\net al., 2023) simply prepends retrieved passages to the prompt, enabling practical off-the-shelf use.\nMore recent works emphasize reasoning-aware retrieval: IRCoT (Trivedi et al., 2023) interleaves\nChain-of-Thought reasoning (Wei et al., 2022) with retrieval to improve multi-hop question answer-\ning, Adaptive-RAG (Jeong et al., 2024) adapts retrieval strategies to query complexity, and Deep-\nRAG (Guan et al., 2025) formulates retrieval as a step-wise decision process to balance parametric\nknowledge with external evidence. Our work shares a similar spirit but focuses on developing search\nagents via RL to adaptively decide whether to search through explicit reasoning.\nReinforcement learning for search agents.\nReinforcement Learning (RL) has emerged as a pow-\nerful paradigm for augmenting LLMs with the ability to invoke external tools during reasoning, ad-\ndressing tasks that require information beyond the model’s internal knowledge (Guo et al., 2025;\nJaech et al., 2024; Li et al., 2025). By treating search call as tools, LLMs are trained to act as\nsearch agents, interleaving reasoning with search in a multi-turn, interactive fashion. Representative\nmethods include Search-R1 (Jin et al., 2025a) and ReSearch (Chen et al., 2025b). Following this\nline of work, recent studies have focused on explicitly optimizing for search adaptivity and effi-\nciency. Huang et al. (2025b) train agents to delineate knowledge boundaries and synergize internal\nand external knowledge; Wang et al. (2025) seek to enhance tool productivity by reducing redundant\ncalls through optimal tool call-controlled policy. However, these approaches rely on intricate reward\nshaping around search-call counts, requiring substantial manual tuning and trial-and-error. They\nalso face ambiguous credit assignment, where agents exploit reward signals by superficially lower-\ning call frequency instead of genuinely improving search behavior. Moreover, whether to invoke\nsearch remains an implicit decision, limiting transparency and hindering deployment in real-world,\nhigh-stakes settings. In contrast, we adopt a simple outcome-based RL framework that separates\nsearch decision making from problem solving, avoiding complex reward engineering while provid-\ning clearer learning signals. At inference time, ADASEARCH offers explicit decision rationales,\nimproving transparency and making the agent’s behavior easier to inspect.\n10\n"}, {"page": 11, "text": "Preprint.\n6\nCONCLUSION\nIn this work, we first conduct analysis and reveal that existing search agents often struggle to recog-\nnize the limits of their parametric knowledge, leading to leading to unnecessary or excessive search.\nMotivated by this observation, we propose ADASEARCH, a simple two-stage, outcome-driven RL\nframework that avoids complex reward engineering while improving both decision quality and task\nperformance. ADASEARCH achieves the strongest self-knowledge awareness on both Qwen and\nLlama models, reduces unnecessary search calls, preserves task accuracy, and offers more transpar-\nent and interpretable decisions about when to search. These results demonstrate that ADASEARCH\nis an effective and generalizable approach for building adaptive, trustworthy search agents.\nREFERENCES\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection. 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In International conference on\nmachine learning, pp. 2206–2240. PMLR, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nHoward Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of\non-policy data in mitigating forgetting. arXiv preprint arXiv:2510.18874, 2025a.\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan,\nWen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforce-\nment learning. arXiv preprint arXiv:2503.19470, 2025b.\nTianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V\nLe, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: A comparative study of founda-\ntion model post-training. In Forty-second International Conference on Machine Learning, 2025.\nURL https://openreview.net/forum?id=dYur3yabMj.\nYuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai\nZhu, Che Jiang, Yuchen Zhang, et al. Ssrl: Self-search reinforcement learning. arXiv preprint\narXiv:2508.10874, 2025.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.\nKnowledge card: Filling llms’ knowledge gaps with plug-in specialized language models. In\nICLR, 2024.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\nTyler Griggs, Sumanth Hegde, Eric Tang, Shu Liu, Shiyi Cao, Dacheng Li, Charlie Ruan, Philipp\nMoritz, Kourosh Hakhamaneshi, Richard Liaw, Akshay Malik, Matei Zaharia, Joseph E. Gonza-\nlez, and Ion Stoica. Evolving skyrl into a highly-modular rl framework, 2025. Notion Blog.\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han,\nLe Sun, and Jie Zhou. Deeprag: Thinking to retrieve step by step for large language models.\narXiv preprint arXiv:2502.01142, 2025.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n11\n"}, {"page": 12, "text": "Preprint.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International conference on machine learning, pp. 3929–3938.\nPMLR, 2020.\nAndre Wang He, Daniel Fried, and Sean Welleck. Rewarding the unlikely: Lifting GRPO beyond\ndistribution sharpening. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and\nViolet Peng (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 25559–25571, Suzhou, China, November 2025. Association for Compu-\ntational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.1298. URL\nhttps://aclanthology.org/2025.emnlp-main.1298/.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt.\nMeasuring massive multitask language understanding.\narXiv preprint\narXiv:2009.03300, 2020.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-\nhop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel,\nand Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational\nLinguistics, pp. 6609–6625, Barcelona, Spain (Online), December 2020. International Com-\nmittee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https:\n//aclanthology.org/2020.coling-main.580/.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong\nChen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language\nmodels: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information\nSystems, 43(2):1–55, 2025a.\nZiyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, and Kang Liu.\nReinforced internal-\nexternal knowledge synergistic reasoning for efficient adaptive search agent.\narXiv preprint\narXiv:2505.07596, 2025b.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning\nwith retrieval augmented language models. Journal of Machine Learning Research, 24(251):\n1–43, 2023.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec\nHelyar, Aleksander Madry, Alex Beutel, Alex Carney, et al.\nOpenai o1 system card.\narXiv\npreprint arXiv:2412.16720, 2024.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. Adaptive-rag:\nLearning to adapt retrieval-augmented large language models through question complexity. In\nProceedings of the 2024 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 7029–7043,\n2024.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM\ncomputing surveys, 55(12):1–38, 2023.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Processing, pp. 7969–7992, 2023.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan O Arik, Dong Wang, Hamed Za-\nmani, and Jiawei Han.\nSearch-r1: Training LLMs to reason and leverage search engines\nwith reinforcement learning.\nIn Second Conference on Language Modeling, 2025a.\nURL\nhttps://openreview.net/forum?id=Rwhi91ideu.\nJiajie Jin, Yutao Zhu, Zhicheng Dou, Guanting Dong, Xinyu Yang, Chenghao Zhang, Tong Zhao,\nZhao Yang, and Ji-Rong Wen. Flashrag: A modular toolkit for efficient retrieval-augmented\ngeneration research. In Companion Proceedings of the ACM on Web Conference 2025, pp. 737–\n740, 2025b.\n12\n"}, {"page": 13, "text": "Preprint.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan\n(eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/\nP17-1147/.\nAdam Tauman Kalai, Ofir Nachum, Santosh S Vempala, and Edwin Zhang. Why language models\nhallucinate. arXiv preprint arXiv:2509.04664, 2025.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP\n(1), pp. 6769–6781, 2020.\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A\nSmith, Yejin Choi, Kentaro Inui, et al. Realtime qa: What’s the answer right now? Advances in\nneural information processing systems, 36:49025–49043, 2023.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N.\nToutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural questions: a benchmark for question answering research. Transactions of the\nAssociation of Computational Linguistics, 2019.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev.\nInternet-\naugmented language models through few-shot prompting for open-domain question answering.\narXiv preprint arXiv:2203.05115, 2022.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented gener-\nation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:\n9459–9474, 2020.\nXuefeng Li, Haoyang Zou, and Pengfei Liu.\nTorl: Scaling tool-integrated rl.\narXiv preprint\narXiv:2503.23383, 2025.\nStephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in\nwords. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https:\n//openreview.net/forum?id=8s8K2UZGTZ.\nShuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. Reacc:\nA retrieval-augmented code completion framework. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6227–6240, 2022.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric\nmemories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802–9822, Toronto, Canada, July 2023. Association for Computational Linguis-\ntics.\ndoi: 10.18653/v1/2023.acl-long.546.\nURL https://aclanthology.org/2023.\nacl-long.546/.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nEduardo Pignatelli, Johan Ferret, Matthieu Geist, Thomas Mesnard, Hado van Hasselt, Olivier\nPietquin, and Laura Toni. A survey of temporal credit assignment in deep reinforcement learning.\narXiv preprint arXiv:2312.01072, 2023.\n13\n"}, {"page": 14, "text": "Preprint.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 5687–5711, 2023.\nCheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-T¨ur,\nGokhan Tur, and Heng Ji. Smart: Self-aware agent for tool overuse mitigation. arXiv preprint\narXiv:2502.11435, 2025.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. Transactions of the Association\nfor Computational Linguistics, 11:1316–1331, 2023.\nStephen E Robertson and Steve Walker. Some simple effective approximations to the 2-poisson\nmodel for probabilistic weighted retrieval. In SIGIR’94: Proceedings of the Seventeenth Annual\nInternational ACM-SIGIR Conference on Research and Development in Information Retrieval,\norganised by Dublin City University, pp. 232–241. Springer, 1994.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can\nteach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539–\n68551, 2023.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathemati-\ncal reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nIdan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rl’s razor: Why online reinforcement learning\nforgets less. arXiv preprint arXiv:2509.04259, 2025.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke\nZettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. In\nProceedings of the 2024 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 8364–8377,\n2024.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation\nreduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.\nHanzhuo Tan, Qi Luo, Ling Jiang, Zizheng Zhan, Jing Li, Haotian Zhang, and Yuqun Zhang.\nPrompt-based code completion via multi-retrieval augmented generation. ACM Transactions on\nSoftware Engineering and Methodology, 2024.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.\ngithub.io/blog/qwen2.5/.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Mul-\ntihop questions via single-hop question composition.\nTransactions of the Association for\nComputational Linguistics, 10:539–554, 2022.\ndoi: 10.1162/tacl a 00475.\nURL https:\n//aclanthology.org/2022.tacl-1.31/.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving re-\ntrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Pro-\nceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 10014–10037, 2023.\n14\n"}, {"page": 15, "text": "Preprint.\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan\nSung, Denny Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search\nengine augmentation. In Findings of the Association for Computational Linguistics ACL 2024,\npp. 13697–13720, 2024.\nHongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin,\nMengdi Wang, Kam-Fai Wong, and Heng Ji. Acting less is reasoning more! teaching model to\nact efficiently. arXiv preprint arXiv:2504.14870, 2025.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-\njumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv\npreprint arXiv:2212.03533, 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837, 2022.\nZhepei Wei, Wei-Lin Chen, and Yu Meng. Instructrag: Instructing retrieval-augmented generation\nvia self-synthesized rationales. arXiv preprint arXiv:2406.13629, 2024.\nGuangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing\nSong, Dengyu Wang, Minjia Zhang, et al. Rag-gym: Optimizing reasoning and search agents\nwith process supervision. arXiv preprint arXiv:2502.13957, 2025.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of\nlarge language models. arXiv preprint arXiv:2401.11817, 2024.\nXiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze\nGui, Ziran Jiang, Ziyu Jiang, et al. Crag-comprehensive rag benchmark. Advances in Neural\nInformation Processing Systems, 37:10470–10490, 2024.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceed-\nings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–\n2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/.\nYang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao\nHuang.\nDoes reinforcement learning really incentivize reasoning capacity in LLMs beyond\nthe base model?\nIn 2nd AI for Math Workshop @ ICML 2025, 2025.\nURL https:\n//openreview.net/forum?id=upehLVgq1b.\nHanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng\nJi, and Tong Zhang. R-tuning: Instructing large language models to say ‘I don’t know’. In\nKevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pp. 7113–7139, Mexico City, Mexico, June\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.394. URL\nhttps://aclanthology.org/2024.naacl-long.394/.\nYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and\nYongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Pro-\nceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\n3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguis-\ntics. URL http://arxiv.org/abs/2403.13372.\n15\n"}, {"page": 16, "text": "Preprint.\nTable 3: Comparison of different reward designs. Unlike prior methods that rely on complex reward\nengineering, such as OTC (Wang et al., 2025) and IKEA (Huang et al., 2025b), ADASEARCH uses\na simple binary outcome reward to explicitly optimize both problem solving and decision making.\nSearch-R1 (Jin et al., 2025a) & ADASEARCH (Ours)\nR(τ, y) =\n\u001a1.0\nif EM = true,\n0\notherwise.\nOTC-GRPO (Wang et al., 2025)\nR(τ, y) = α · Rtool · 1 [EM = true] , where Rtool =\n\n\n\n\n\n\n\n1.0\nif f(m, n) = n = 0,\ncos\n\u0010\nmπ\n2m+c\n\u0011\nif n = 0,\nsin\n\u0010\nf(m,n)π\n2n\n\u0011\notherwise.\nf(m, n) =\n\n\n\n0\nif m = n = 0,\nm\nif n = 0,\n2nm\nm+n\notherwise.\nα: hyperparameter, m: # search calls in τ, n: min # search calls for input x during training.\nIKEA (Huang et al., 2025b)\nR(τ, y) =\n\u001a−1\nif incorrect format,\n1 [EM = true] + Rkb\notherwise.\nRkb =\n\n\n\n\n\nrkb+\n\u0010\n1 −\nRT\nRTmax\n\u0011\nif EM = true,\n0\nif EM = false & RT = 0,\nrkb−\notherwise.\nRT: # search calls; RTmax: max # search allowed; rkb+, rkb−: hyperparameters.\nNaive Shaping\nR(τ, y) =\n\u001a1.0 −λ · (# search calls)\nif EM = true,\n0\notherwise,\nwhere λ is the hyperparameter controlling penalties for search calls.\nAwareness Shaping\nR(τ, y) = 1 [EM = true] + α ·\n\u001a1 [τ has search]\nif p < ρ,\n1 [τ has no search]\notherwise,\nwhere α: self-awareness bonus, p: solve rate for the input problem, and ρ: threshold for solve rate p.\nA\nCOMPARISON ON REWARD FUNCTIONS\nWe summarize the reward designs of different methods in Table 3. OTC (Wang et al., 2025) is\ndesigned to minimize search calls and maximize task performance. IKEA (Huang et al., 2025b)\nshares a similar spirit, but focus more on adaptivity by adding a knowledge-boundary bonus (r−\nkb)\nto encourage agents using search on difficult problems. Naive Shaping is conceptually similar to\nOTC but much easier to implement, aiming to encourage agents to use fewer search calls while\nobtaining the correct answer. Awareness Shaping aims to sharply penalize inconsistent transitions\nbetween no-search and search behaviors. In contrast, our ADASEARCH use a simple binary rewards\nto optimize both problem solving and decision making on whether to search.\nB\nIMPLEMENTATION DETAILS\nB.1\nTRAINING DATASET CONSTRUCTION\nTo ensure a fair comparison across training-based baselines, we follow the procedure of Huang et al.\n(2025b) to construct a difficulty-balanced training set. Specifically, we use the base policy with the\nsystem prompt sprobe (Table 11), which encourages the model to answer directly without accessing\nadditional information, to generate K = 10 responses for each problem in the training set of Jin\net al. (2025a), drawn from Natural Questions (Kwiatkowski et al., 2019) and HotpotQA (Yang et al.,\n2018). We then compute solve rates using substring exact matching (SubEM) for verification.\nUsing a threshold ρ = 0.5, we split the dataset into easy and hard subsets and sample 4,096 examples\nfrom each to form the final training set. The same procedure is used to construct a validation set of\n2,048 examples, which is used for checkpoint selection during evaluation.\nWe create datasets for each model independently, and all methods evaluated on the same model use\nthe same dataset to ensure fair comparison.\nB.2\nTRAINING DETAILS\nMost RL training experiments are conducted on a compute node equipped with 4 NVIDIA A100-\n80GB GPUs. The only exception is the outcome-based RL baselines for the 3B models, which are\n16\n"}, {"page": 17, "text": "Preprint.\ntrained on a machine with 2 NVIDIA H100-94GB GPUs, as these baselines require relatively fewer\ncomputational resources. We use full parameter fine-tuning for all experiments. To optimize training\nefficiency, we adopted DeepSpeed with Zero3 offload, gradient checkpointing, FlashAttention-2,\nand bfloat16 mixed precision training.\nWe adopt LlamaFactory (Zheng et al., 2024) for SFT. We conduct hyperparameter search on batch\nsize ∈{16, 32} and learning rate ∈{1e-6, 5e-6}. We only train for one epoch as the training loss\nquickly converged to nearly 0. We select the checkpoint that maximizes the product of validation\nEM and F1aware.\nFor RL training, we use SkyRL (Griggs et al., 2025) as our base framework. Hyperparameters for\nRL baselines and for ADASEARCH are shown in Table 4 and Table 5, respectively. Most hyperpa-\nrameters follow Jin et al. (2025a) and Wang et al. (2025). For reward-shaping baselines, we increase\nthe group size to 15 to match the compute used in problem-solving training (ADASEARCH stage 1).\nFor the reward-function hyperparameters, we set λ in Naive Shaping (Equation (6)) to 0.05; both α\nand ρ in Awareness Shaping to 0.5; and r+\nkb and r−\nkb in IKEA (Table 3) to 0.2 and 0.05, following\nthe original IKEA paper (Huang et al., 2025b). In addition, we analyze the reward-function hyper-\nparameters for Naive Shaping and compare our training hyperparameters with those in the original\nIKEA implementation in Appendix C.2.\nFor ADASEARCH stage 2, we search over the combinations of (batch size, mini-batch size) in (128,\n64), (256, 128). For Llama-3.2-3B-Instruct, we use the (128, 64) setting, while for the Qwen2.5\nseries models, we use (256, 128).\nFor efficient rollouts, we use vLLM (Kwon et al., 2023) and set the temperature to 1.0, top-p to 1.0,\nand top-k to –1, following Jin et al. (2025a).\nFor baselines that involve non-search generation (e.g., RL w/o Search, rollouts for sparam in\nADASEARCH stage 1, and ADASEARCH stage 2), we set the maximum number of turns to 2, with\na maximum generation length of 500 tokens in the second turn. In most cases, the full trajectory of\nthese baselines contains only one turn unless the model fails to follow the required output format\n(e.g., the answer is not wrapped in <answer>...</answer> or the assessment is not wrapped in\n<assessment>...</assessment>). Similar to Search-R1 (Jin et al., 2025a), which appends\na nudge prompt (Table 20) when the model violates the required format, we design nudge prompts\n(Table 21 and 22) to enforce correct formatting. This formatting-correction procedure is applied to\nall reward-shaping baselines as well to ensure fairness.\nWe save checkpoints at the end of each epoch. For RL baselines, we select the checkpoint that\nachieves the highest validation reward. For ADASEARCH stage 1, we use the final checkpoint be-\nfore collapse. For ADASEARCH stage 2, we select the checkpoint that maximizes the product of\nvalidation EM and F1aware.\nTable 4: Hyperparameters for RL baselines. ∗: in the second turn, the maximum generation length\nis set to 500 (See Appendix B.2).\nHyperparameter\nSearch-R1\nRL w/o Search\nReward-Shaping Baselines\n(batch size, mini-batch size)\n(512, 256)\n(512, 256)\n(512, 256)\nlearning rate\n1e-6\n1e-6\n1e-6\nepoch\n6\n6\n6\ngroup size\n5\n10\n15\nKL loss coefficient β\n0.001\n0.001\n0.001\nPPO clip ratio\n0.2\n0.2\n0.2\nwarm-up step ratio\n0.285\n0.285\n0.285\nmax generation length\n500\n2048∗\n500\nmax number of turns\n4\n2\n4\nmax input length\n4096\n3072\n4096\n17\n"}, {"page": 18, "text": "Preprint.\nTable 5: Hyperparameters for ADASEARCH. ∗: in the second turn, the maximum generation length\nis set to 500 (See Appendix B.2).\nHyperparameter\nADASEARCH stage 1\nADASEARCH stage 2\n(batch size, mini-batch size)\n(512, 256)\n{(128, 64), (256, 218)}\nlearning rate\n1e-6\n1e-6\nepoch\n6\n6\ngroup size\n(ssearch, sparam) = (5, 10)\n5\nKL loss coefficient β\n0.001\n0.001\nPPO clip ratio\n0.2\n0.2\nwarm-up step ratio\n0.285\n0.285\nmax generation length\n(ssearch, sparam) = (500, 2048∗)\n2048∗\nmax number of turns\n(ssearch, sparam) = (4, 2)\n2\nmax input length\n(ssearch, sparam) = (4096, 3072)\n3072\nB.3\nINFERENCE DETAILS\nWe perform inference with vLLM (Kwon et al., 2023) and bfloat16 precision on the same compute\nnodes used for training. For each method, the maximum generation length, maximum number of\nturns, and maximum input length are set to match the rollout configuration used during training for\nconsistency. Following Jin et al. (2025a), we use greedy decoding for evaluation.\nC\nADDITIONAL ANALYSIS\nIn this section, we analyze the effects of hyperparameters across different methods and examine\nefficiency in terms of the number of search calls and the latency. All analyses are conducted on\nQwen2.5-3B-Instruct.\nC.1\nADASEARCH STAGE 2: THRESHOLD ρ VS NUMBER OF SEARCH CALLS\nWe analyze the effect of the solve-rate threshold ρ on the number of search calls, with results shown\nin Table 6. As ρ increases, the number of search calls rises, which is expected since a higher\nthreshold encourages the model to invoke search more often. However, when ρ becomes too large\n(e.g., 0.9), F1aware drops substantially. This suggests overuse of search, where the model chooses\nto search even when its parametric knowledge is sufficient, as reflected in the decrease in recall and\nthe increase in precision.\nTable 6: Analysis on the effect of ρ for ADASEARCH stage 2 (Section 3.2).\nρ\nEM\nF1aware\nPrecision\nRecall\nAvg. Search\n0.1\n35.8\n53.8\n44.5\n69.3\n0.99\n0.3\n34.1\n53.8\n43.4\n72.5\n1.00\n0.5\n36.0\n54.0\n45.1\n68.8\n1.08\n0.7\n36.2\n53.2\n48.9\n59.6\n1.12\n0.9\n36.8\n46.2\n55.9\n41.0\n1.21\nC.2\nHYPERPARAMETERS OF REWARD-SHAPING BASELINES\nWe analyze the reward-function hyperparameter of Naive Shaping and Awareness Shaping, and the\ntraining hyperparameters of IKEA (Huang et al., 2025b) on Qwen2.5-3B-Instruct.\nNaive shaping.\nWe vary the value of λ in Equation 6 over 0.05, 0.1, 0.25, and report EM, F1aware,\nprecision, recall, and the micro-averaged number of search calls. The results are shown in Table 7.\nAs expected, the average number of search calls decreases as λ increases, due to the stronger penalty\n18\n"}, {"page": 19, "text": "Preprint.\non invoking search. F1aware increases, which can be attributed to the increase in recall, while pre-\ncision remains relatively stable. Lastly, EM decreases with larger λ, possibly because the model\nunderuses search, as reflected by the lower recall and reduced number of search calls.\nTable 7: Analysis on the effect of λ for Naive Shaping baseline (Eq 6).\nλ\nEM\nF1aware\nPrecision\nRecall\nAvg. Search\n0.05\n33.2\n47.3\n36.1\n76.8\n0.77\n0.1\n33.0\n47.8\n35.7\n81.3\n0.74\n0.25\n32.1\n48.7\n35.9\n85.6\n0.67\nAwareness shaping.\nWe vary the value of α in Equation 7 over 0.1, 0.5, 1.0 and report EM,\nF1aware, precision, recall, and the micro-averaged number of search calls. The results are shown\nin Table 8. Increasing α improves F1aware and recall, as expected from assigning a larger bonus to\ncorrect decisions. In contrast, precision, average search-call counts, and task EM decrease. These\ntrends suggest that the model may underuse search, reflected in higher recall but lower precision and\nfewer search calls.\nTable 8: Analysis on the effect of α for Awareness Shaping baseline (Eq 7).\nα\nEM\nF1aware\nPrecision\nRecall\nAvg. Search\n0.1\n36.2\n49.5\n44.7\n57.5\n1.12\n0.5\n34.1\n51.3\n42.0\n70.3\n1.00\n1.0\n33.8\n52.5\n41.6\n76.4\n0.98\nIKEA.\nWe compare our training hyperparameters for IKEA (Huang et al., 2025b) (Table 4) with\nthose used in the original IKEA paper. In the original work, the authors train with both batch size\nand mini-batch size set to 256, a smaller learning rate of 5e-7, a much larger warm-up ratio of\n0.75, a slightly larger group size of 16, and a total of 120 training steps (approximately 4 epochs).\nThe performance comparison is shown in Table 9. We find that using the original hyperparameters\nresults in IKEA being undertrained. Therefore, we adopt the hyperparameters in Table 4 for our\nexperiments.\nTable 9: Comparison of Hyperparameters for IKEA Huang et al. (2025b)\nHyperparameter Setting\nEM\nF1aware\nPrecision\nRecall\nAvg. Search\nOriginal (Huang et al., 2025b)\n32.3\n12.4\n42.0\n8.0\n1.06\nOurs (Table 4)\n35.3\n47.6\n45.0\n59.5\n0.88\nC.3\nCOMPARISON ON EFFICIENCY\nIn this section, we compare the efficiency of each method in terms of the average number of search\ncalls and the average latency, as shown in Table 10. All averages are computed over the full set of\ntesting samples. In addition, we estimate the total search cost on the evaluation benchmarks using\nthe pricing of the Google Search API (5 USD per 1,000 queries)3.\nRegarding the average number of search calls, ADASEARCH substantially reduces unnecessary\nsearches and costs compared to Search-R1, achieving a 34% reduction. ADASEARCH also incurs\na similar number of search calls as Awareness Shaping, as both methods decide whether to invoke\nsearch on prompt-level based on self-knowledge. Compared to methods that explicitly minimize\nsearch calls (Naive Shaping and IKEA), ADASEARCH uses slightly more searches. This behav-\nior is expected, since ADASEARCH does not directly optimize for minimizing tool calls. Instead,\n3https://developers.google.com/custom-search/v1/overview#pricing\n19\n"}, {"page": 20, "text": "Preprint.\nAlgorithm 2 ADASEARCH-E2E Training\nRequire: base policy LLM πθb; search engine E; training set D = (xi, yi); prompts sparam, ssearch,\nsdecision; string concatenation [·, ·]; solve-rate threshold ρ.\n1: Initialize policy πθ ←πθb\n/* End-to-End Training */\n2: for iteration = 1 to T do\n3:\nSample a batch Db ∼D\n4:\nR ←{}; A ←{}\n5:\nfor each (xi, yi) ∼Db do\n6:\nfor g ∈{param, search, decision} do\n▷Rollout Phase\n7:\nif g is param then\n8:\nGenerate Ri\ng = {τ n\nparam}N\nn=1, where τ n\nparam ∼πθ(· | [sparam, xi]).\n9:\nelse if g is decision then\n10:\nGenerate Ri\ng = {τ k\ndecision}K\nk=1, where τ k\ndecision ∼πθ(· | [sdecision, xi]).\n11:\nelse\n12:\nGenerate Ri\ng = {τ m\nsearch}M\nm=1, where τ m\nsearch ∼πθ(· | [ssearch, xi]; E).\n13:\nCompute rewards for all τ ∈Ri\ng with Eq 4.\n14:\nCompute group-wise advantages Ai\ng for Ri\ng.\n15:\nR ←R ∪Ri\ng; A ←A ∪Ai\ng\n▷Aggregate rollouts & advantages\n16:\nUpdate πθ using GRPO with R and A.\nreturn πθ\nit focuses on avoiding unnecessary searches when parametric knowledge is sufficient, while still\nachieving a significant reduction relative to Search-R1.\nIn terms of average latency, although ADASEARCH introduces an additional inference stage that ex-\nplicitly decides whether to invoke search, its latency remains lower than that of Search-R1 by 20%.\nThese results demonstrate that ADASEARCH not only reduces the average number of searches (and\nthus API cost) but also decreases inference latency, while providing transparent and interpretable\ndecision rationales that are valuable in real-world applications. Compared to reward-shaping base-\nlines, ADASEARCH incurs a slightly higher latency, but it achieves higher task performance (EM)\nand demonstrates better discrimination between necessary and unnecessary search calls, as reflected\nby F1aware (Table 2).\nFinally, our framework is orthogonal to reward-shaping methods. Additional reward terms can be\nincorporated to penalize undesirable search behaviors, such as issuing duplicate queries. We leave a\ndetailed exploration of this direction to future work.\nTable 10: Average number of searches, their estimated costs, and the average latency.\nMethod\nAvg. Search\nEst. Costs\nAvg. Latency\nQwen2.5-3B-Instruct\nSearch-R1\n1.64\n424.3\n0.111\nNaive Shaping\n0.77\n198.4\n0.059\nAwareness Shaping\n1.00\n259.0\n0.074\nIKEA\n0.88\n228.6\n0.066\nADASEARCH\n1.08\n279.2\n0.089\nD\nADASEARCH-E2E\nWe unify the two stages in ADASEARCH to obtain an end-to-end variant, ADASEARCH-E2E. The\noverall training pipeline is shown in Algorithm 2. For each training instance (x, y), we construct\nthree variants of the input by applying the parametric knowledge prompt sparam, the search prompt\nssearch, and the decision-making prompt sdecision.\n20\n"}, {"page": 21, "text": "Preprint.\nThe pseudo labels for decision making are generated on the fly: for each training instance (x, y), we\nestimate the empirical solve rate p from Rparam using SubEM, and assign pseudo labels according\nto whether p exceeds a predefined threshold ρ. The reward function is simply the binary outcome\nreward, and the remaining optimization follows GRPO (Shao et al., 2024).\nThe hyperparameters largely follow those of our two-stage method (Table 5), except for the batch\nand mini-batch sizes. We search over {(256, 128), (512, 256)} and use (256, 128) for Qwen and\n(512, 256) for Llama.\nE\nPROMPT TEMPLATES\nTable 11: Knowledge-probing system prompt for dataset construction.\nKnowledge-Probing System Prompt sprobing\nYou are a helpful assistant. Your goal is to answer questions using step-by-step reasoning. Use\nonly your internal knowledge to recall and connect relevant information. Do NOT assume access\nto external tools or documents. Provide the final answer only after completing your reasoning,\nfollowing the output instructions below.\n## Output Instructions\n1.\nWrap your final answer within <answer> and </answer> tags.\nFor example:\n<answer>Wilhelm Conrad R¨ontgen</answer>.\n2. Only include the direct final answer within these tags; place all explanations and reasoning\noutside.\nTable 12: System prompt for search, adapt from Jin et al. (2025a)\n.\nSearch System Prompt ssearch\nAnswer the given question.\nYou must conduct reasoning every time you get new informa-\ntion. After reasoning, if you find you lack some knowledge, you can call a search engine by\n<search>query</search>, and it will return the top searched results between <output>\nand </output>. You can search as many times as you want. If you find no further external\nknowledge needed, you can directly provide the answer inside <answer> and </answer>\nwithout detailed illustrations. For example, <answer>xxx</answer>.\nTable 13: Parametric-knowledge system prompt. This prompt is used for CoT and RL w/o Search\nbaselines as well.\nParametric-knowledge System Prompt sparam\nAnswer the given question. Always conduct and show your step-by-step reasoning first, then\ngive the final answer. When providing the answer, wrap it inside <answer> and </answer>\nwithout detailed illustrations. For example, <answer>xxx</answer>.\n21\n"}, {"page": 22, "text": "Preprint.\nTable 14: Decision-making system prompt.\nDecision-Making System Prompt sdecision\nYour goal is to assess whether your knowledge is sufficient to answer the given question. Think\nstep-by-step and then output the final assessment by following the instructions below:\n## Output Instructions\n1. Always output your step-by-step reasoning first.\n2. After your reasoning, output a single assessment token wrapped within <assessment> ...\n</assessment> tags.\n– If\nyou\nare\nconfident\nyou\ncan\nanswer\nthe\nquestion\ndirectly,\noutput:\n<assessment>yes</assessment>\n– Otherwise, output: <assessment>no</assessment>\n3. Only include the final assessment token within these tags; place all reasoning outside the tags.\nTable 15: System prompt for reward-shaping baselines (Naive Shaping, Awareness Shaping, and\nIKEA), adapt from Wang et al. (2025)\nSystem Prompt for Reward-shaping Baselines\nAnswer the given question.\nYou must conduct reasoning every time you get new informa-\ntion. After reasoning, if you find you lack some knowledge, you can call a search engine by\n<search>query</search>, and it will return the top searched results between <output>\nand </output>. You need to make every search call count and gain helpful results. If you find\nno further external knowledge needed, you can directly provide the answer inside <answer>\nand </answer> without detailed illustrations. For example, <answer>xxx</answer>.\nTable 16: System prompt for direct answer baseline.\nSystem Prompt for Direct Answer Baseline\nAnswer the given question.\nWhen providing the answer, wrap it inside <answer> and\n</answer> without detailed illustrations. For example, <answer>xxx</answer>.\nTable 17: System prompt for RAG baseline.\nSystem Prompt for RAG Baseline\nAnswer the given question based on the provided documents.\nWhen providing the answer,\nwrap it inside <answer> and </answer> without detailed illustrations.\nFor example,\n<answer>xxx</answer>.\nTable 18: Default user prompt template, where question will be replaced with the actual input\nquestion.\nDefault User Prompt Template\nQuestion: {question}\n22\n"}, {"page": 23, "text": "Preprint.\nTable 19: RAG User prompt template, where document i and question will be replaced with\nthe retrieved document and the actual input question, respectively.\nRAG User Prompt Template\nDoc 1: {document 1}\nDoc 2: {document 2}\nDoc 3: {document 3}\nQuestion: {question}\nTable 20: Retry prompt used when the model produces an invalid action under the search prompt\nssearch.\nRetry Prompt (Search)\nMy previous action is invalid. If I want to search, I should put the query between <search> and\n</search>. If I want to give the final answer, I should put the answer between <answer>\nand </answer>. Let me try again.\nTable\n21:\nRetry\nprompt\nused\nwhen\nthe\nmodel\nfails\nto\nwrap\nthe\nanswer\nin\n<answer>...</answer> under the parametric-knowledge prompt sparam.\nRetry Prompt (Parametric-knowledge)\nI should put the answer between <answer> and </answer>.\nTable 22: Retry prompt used when the model produces a search call under the parametric-knowledge\nprompt sparam.\nRetry Prompt (Parametric-knowledge; When search is invoked)\nMy previous action is invalid. I should rely on my own step-by-step reasoning to solve the\nproblem and then put the answer between <answer> and </answer>. Let me try again.\n23\n"}, {"page": 24, "text": "Preprint.\nF\nCASE STUDIES\nF.1\nCASE 1: PARAMETRIC KNOWLEDGE IS SUFFICIENT\nTable 23: Input question. The answer is “Christian Louboutin”.\nInput Question (Answer: Christian Louboutin)\nWhich designer is famous for his/her red-soled footwear?\nTable 24: ADASEARCH stage-1 output. The model explicitly evaluates its own knowledge and\nconcludes that it is sufficient.\nADASEARCH Stage-1 Output (Answer: Christian Louboutin)\nStep 1: I need to think of a designer known for red-soled footwear.\nStep 2: One famous designer known for red-soled footwear is Christian Louboutin.\nStep 3: Christian Louboutin is a French fashion designer who is renowned for his distinctive\nred-soled high-heeled shoes.\nStep 4: Based on this information, I can answer the question.\n<assessment>yes</assessment>\nTable 25: ADASEARCH stage-2 output. The model correctly answer the question using parametric\nknowledge.\nADASEARCH Stage-2 Output (Answer: Christian Louboutin)\nTo determine which designer is famous for his/her red-soled footwear, we need to consider some\nof the most well-known designers and their iconic designs.\n1.\nChristian Louboutin is a renowned French fashion designer who is famous for creating\nred-soled high-heeled shoes.\n2. Other designers like Jimmy Choo and Alexander McQueen have also designed red-soled\nshoes, but Christian Louboutin is particularly known for this specific style.\nGiven this information, the designer famous for his/her red-soled footwear is Christian\nLouboutin.\n<answer>Christian Louboutin</answer>\n24\n"}, {"page": 25, "text": "Preprint.\nTable 26: Search-R1 output. The agent overuses search on problems where its parametric knowledge\nis sufficient.\nSearch-R1 Output (Answer: Christian Louboutin)\nTo find out which designer is famous for their red-soled footwear, I will need to search for\ninformation about notable footwear designers and their signature styles.\n<search>Which designer is famous for red-soled footwear</search>\n<output>\nDoc 1: ”Christian Louboutin”\nChristian Louboutin Christian Louboutin (; born 7 January 1964) is a French fashion designer\nwhose high-end stiletto footwear incorporates shiny, red-lacquered soles that have become his\nsignature. Initially a freelance designer for fashion houses, he started his own shoe salon in Paris,\nwith his shoes finding favor with celebrity clientele. He has partnered with other organizations\nfor creative projects including limited edition pieces, gallery exhibits, and even a custom bar.\nHis company has since branched out into men’s footwear, handbags, fragrances and makeup.\nLouboutin was born and raised in Paris’s 12th arrondissement. He was the only son of Roger, a\nDoc 2: ”Christian Louboutin”\ntempered to 28,000 Mexican pesos (1471.89 dollars). Christian Louboutin Christian Louboutin (;\nborn 7 January 1964) is a French fashion designer whose high-end stiletto footwear incorporates\nshiny, red-lacquered soles that have become his signature. Initially a freelance designer for\nfashion houses, he started his own shoe salon in Paris, with his shoes finding favor with celebrity\nclientele.\nHe has partnered with other organizations for creative projects including limited\nedition pieces, gallery exhibits, and even a custom bar. His company has since branched out into\nmen’s footwear, handbags, fragrances and makeup. Louboutin was born and raised in Paris’s\n12th arrondissement. He\nDoc 3: ”Christian Louboutin”\non his high heel shoes, commonly referred to as ””sammy red soled shoes””. His signature red\nsoles were first created in 1993. ””My sketches were not reproduced exactly as I had designed\nthem and I couldn’t figure out why. The two dimensional sketch was so powerful on paper, but\nwhen turned into a three-dimensional object, it was somehow lacking energy. Frustrated after\nhaving tried different things to liven up the design, I spontaneously grabbed my assistant’s red\nnail polish and painted the sole. I instantly knew that this would be a success!””. Christian\nLouboutin’s red-bottom colour code is Pantone 18-1663\n</output>\nFrom the search results, it is clear that Christian Louboutin is famous for his red-soled\nfootwear.\n<answer>Christian Louboutin</answer>\nTable 27: Naive Shaping output. The model produces an incorrect answer without explicit self-\nknowledge assessment.\nNaive Shaping Output (Answer: Christian Louboutin)\nTo determine which designer is famous for their red-soled footwear, let’s consider some\nprominent designers and their notable styles.\nOne of the most famous designers known for red-soled footwear is Alexander McQueen.\nHe was known for his bold and innovative designs, including red-soled shoes that became iconic\nin his collections.\nGiven this information, the designer famous for his/her red-soled footwear is Alexander\nMcQueen.\n<answer>Alexander McQueen</answer>\n25\n"}, {"page": 26, "text": "Preprint.\nF.2\nCASE 2: PARAMETRIC KNOWLEDGE IS INSUFFICIENT\nTable 28: Input question. The answer is “Bardney”.\nInput Question (Answer: Bardney)\nSouthrey is a village in the civil parish of which village in the West Lindsey district of Lin-\ncolnshire, England?\nTable 29: ADASEARCH stage-1 output. After self-assessment, the model is not confident in answer-\ning using parametric knowledge.\nADASEARCH Stage-1 Output (Answer: Bardney)\nStep 1: Identify the key information in the question. The question asks about a civil parish that\ncontains the village of Southrey.\nStep 2: Recall that a civil parish is a territorial unit of local government in England, and it is often\npart of a larger administrative division such as a district.\nStep 3: Think about the West Lindsey district in Lincolnshire, England. I need to find which\nvillage in this district contains Southrey.\nStep 4: Without specific knowledge of the exact location, I would need to search for the civil\nparish that includes Southrey. However, I don’t have immediate access to this information.\nStep 5: Given the lack of specific knowledge about the exact civil parish containing Southrey, I\ncannot answer the question directly.\n<assessment>no</assessment>\n26\n"}, {"page": 27, "text": "Preprint.\nTable 30: ADASEARCH stage-2 output. The model invokes search and produces a correct answer.\nADASEARCH Stage-2 Output (Answer: Bardney)\nTo determine the village in the West Lindsey district of Lincolnshire, England, that is the civil\nparish of Southrey, I will need to search for this information.\n<search>Village in the West Lindsey district of Lincolnshire that is the civil parish of\nSouthrey</search>\n<output>\nDoc 1: Bardney\nconsists of crafts, trade and community stands, funfair rides, bar and BBQ, sports, children’s\nraces, car boot sales, classic vehicles, and an Exemption Dog Show. Originally part of the\nWraggoe Wapentake, and of the Lincoln poor law union in the Parts of Lindsey, the parish is now\npart of West Lindsey District, and Lincolnshire County Council. The civil parish has become\npart of a group that consists of Bardney, Apley and Stainfield parishes. The villages of Southrey\nand Kingthorpe are also included. The 2011 electoral arrangements are: Bardney lies between 7\nand 17 metres above sea level, on the edge\nDoc 2: Snelland\nSnelland Snelland is a village and civil parish in the West Lindsey district of Lincolnshire,\nEngland It is situated approximately north-east from the city and county town of Lincoln and\nabout from the town of Market Rasen. The civil parish includes the hamlet of Swinthorpe.\nThe population at the 2011 census was included in the civil parish of Friesthorpe. Snelland is\nlisted in ””Domesday Book”” of 1086 as ””Sneleslunt””, with seven households. The nearby\ndeserted medieval village of Swinthorpe is listed as ””Sonetorp””, with three households. There\nis evidence that a Cistercian grange existed at Swinthorpe, attached to Kirkstead Abbey.\nDoc 3: ”South Thoresby”\nSouth Thoresby South Thoresby is a village and civil parish in the East Lindsey district of\nLincolnshire, England. It is situated north-east from Horncastle and south-east from Louth.\nSouth Thoresby is about east from the A16 in the Lincolnshire Wolds. The civil parish extends\nmuch further to the south, over the A16 and to include the hamlets of Calceby (a former medieval\nvillage) and Driby, extending southwards to border with Langton by Spilsby. To the south-west\nis Haugh. It lies close to the northern boundary with Aby with Greenfield. The parish council\nadministers Swaby, South Thoresby and Haugh, although these\n</output>\nFrom the search results, it is clear that Southrey is a village in the civil parish of Bardney.\nTherefore, the village in the West Lindsey district of Lincolnshire, England, that is the civil\nparish of Southrey is Bardney.\n<answer>Bardney</answer>\nTable 31: Naive Shaping output. The model produces an incorrect answer without explicit self-\nknowledge assessment.\nNaive Shaping Output (Answer: Bardney)\nTo determine the village in the West Lindsey district of Lincolnshire, England, that is the civil\nparish of Southrey, we need to find information about the civil parishes and their boundaries.\nSouthrey is a village in the civil parish of Little Sleaford.\nLittle Sleaford is a village lo-\ncated in the West Lindsey district of Lincolnshire, England.\nTherefore, the civil parish of Southrey is Little Sleaford.\n<answer>Little Sleaford</answer>\n27\n"}, {"page": 28, "text": "Preprint.\nF.3\nCASE 3: ADASEARCH UNDERUSE SEARCH\nTable 32: Input question. The answer is “Mary Elizabeth Braddon”.\nInput Question (Answer: Mary Elizabeth Braddon)\nWho is the author of Lady Audley’s Secret?\nTable 33: ADASEARCH stage-1 output. The model holds a false belief, which leads to an incorrect\nself-assessment.\nADASEARCH Stage-1 Output (Answer: Mary Elizabeth Braddon)\nStep 1: I need to recall the author of the novel ”Lady Audley’s Secret.”\nStep 2: I know that ”Lady Audley’s Secret” is a novel.\nStep 3: I remember that the author of ”Lady Audley’s Secret” is Mrs. Wilkie Collins.\nStep 4: I can confirm that Mrs. Wilkie Collins is indeed the author of this novel.\nStep 5: Based on my knowledge, I can answer the question directly.\n<assessment>yes</assessment>\nTable 34: ADASEARCH stage-2 output. The model maintains its false belief, resulting in insufficient\nsearch.\nADASEARCH Stage-2 Output (Answer: Mary Elizabeth Braddon)\nTo determine the author of ”Lady Audley’s Secret,” we need to consider the historical context\nand the well-known works of that era.\n”Lady Audley’s Secret” is a novel published in 1862.\nThe author of this novel is Mrs.\nWilkie Collins, a prominent English novelist of the Victorian era.\nTherefore, the author of ”Lady Audley’s Secret” is <answer>Mrs. Wilkie Collins</answer>\nF.4\nCASE 4: ADASEARCH OVERUSE SEARCH\nTable 35: Input question. The answer is “Politician”.\nInput Question (Answer: Politician)\nWhat is Javier Alva Orlandini’s occupation?\n28\n"}, {"page": 29, "text": "Preprint.\nTable 36: ADASEARCH stage-1 output. The model is not confident in its self-knowledge and there-\nfore abstains.\nADASEARCH Stage-1 Output (Answer: Politician)\nStep 1: I need to determine Javier Alva Orlandini’s occupation.\nStep 2: I do not have any information about Javier Alva Orlandini in my knowledge base.\nStep 3: Without specific information about this person, I cannot answer the question directly.\n<assessment>no</assessment>\nTable 37: ADASEARCH stage-2 output. The model produces a correct answer after search.\nADASEARCH Stage-2 Output (Search) (Answer: Politician)\nTo determine Javier Alva Orlandini’s occupation, I will need to search for information about\nhim. Let me start with a search.\n<search>Javier Alva Orlandini occupation</search>\n<output>\nDoc 1: ”Javier Alva Orlandini”\nJavier Alva Orlandini Javier Alva Orlandini (born December 11, 1927) is a lawyer and a\nPeruvian politician. A prominent member of the political party Popular Action, he serves as\nthe President of the party. He served as 2nd Vice President of Peru during the government of\nFernando Bela´unde from 1980 to 1985.. Throughout his life he became a Deputy (1963-1968),\nMinister of Government and Police (1965-1966), Senator (1980-1985 and 1990-1992), President\nof the Senate (1981-1982) and Congressman (1995-2000). In 2002, he became a Magistrate of\nthe Constitutional Court, at the same time being elected by Congress as the President of\nDoc 2: ”Luis Orlandini”\nLuis Orlandini Luis Orlandini (born 1964) is a Chilean guitarist and professor at the University\nof Chile and the Pontifical Catholic University of Chile. Orlandini studied guitar with Ernesto\nQuezada at the University of Chile and then with Eliot Fisk at the School of Music of Cologne.\nIn 1989, he obtained First Place at the ARD International Music Competition in Munich, thus\ninitiating an international career that has led him to perform in various countries and record with\ndifferent European labels. He performs mainly as a soloist, but has also played with groups of\ndifferent natures of chamber music, one\nDoc 3: ”Javier Alva Orlandini”\nthe Court for a 3-year term. He was a candidate for the Presidency of the Republic in 1985,\nbut got 4% of the popular vote, being defeated by Alan Garc´ıa. He later served as President\nof Peru’s constitutional tribunal. He has written several books, including ””Responding to the\nDictatorship””, ””Yesterday, Today and Tomorrow””, the ””Vicious Circle””, ””I Minister””,\n””Word of Honor”” and ””Yes I swear””, and numerous legal articles. The audits, representative\nlegislative, legal, academic and professional are of extensive quality and are recognized by the\nPeruvian people and their various institutions that have given many honorary degrees. Javier\nAlva\n</output>\nBased on the search results, Javier Alva Orlandini is a lawyer and a Peruvian politician.\nHe served as the President of the party Popular Action and was the 2nd Vice President of Peru\nduring the government of Fernando Bela´unde from 1980 to 1985.\n<answer>Politician</answer>\n29\n"}, {"page": 30, "text": "Preprint.\nTable 38: Model output under the parametric-knowledge system prompt sparam. The model is uncer-\ntain about its answer, even though the answer is correct.\nADASEARCH Stage-2 Output (Parametric Knowledge) (Answer: Politician)\nTo determine Javier Alva Orlandini’s occupation, we need to look up information about him.\nGiven that the question does not provide specific details, we can infer based on common\nknowledge or a search.\nJavier Alva Orlandini is a Mexican politician who has held various positions in the Mexi-\ncan government. Given his background, it is most likely that he is a politician.\nHowever, without a specific source, we can’t be certain.\nFor the purpose of this question,\nwe will assume his occupation is based on his known roles in Mexican politics.\n<answer>Politician</answer>\nF.5\nCASE 5: MULTI-HOP FAILURE OF ADASEARCH\nTable 39: Input question. The answer is “1978”.\nInput Question (Answer: 1978)\nThe director of Panic 5 Bravo was born in what year?\nTable 40: ADASEARCH stage-1 output. The model abstains because it lacks sufficient information.\nADASEARCH Stage-1 Output (Answer: 1978)\nStep 1: To answer this question, I need to know the director of the movie ”Panic 5 Bravo”.\nHowever, there is no widely known movie or film titled ”Panic 5 Bravo” that I am aware of. This\nsuggests it might be a fictional or obscure title.\nStep 2: Without knowing the director of this specific movie, I cannot determine the year of birth\nof the director.\nStep 3: Since I do not have the necessary information to answer the question, I cannot provide a\nyear of birth for the director.\n<assessment>no</assessment>\n30\n"}, {"page": 31, "text": "Preprint.\nTable 41: ADASEARCH stage-2 output (part 1). The model invokes search but makes a faulty\ninference (highlighted in red).\nADASEARCH Stage-2 Output (Answer: 1978)\nTo find out the year the director of ”Panic 5 Bravo” was born, I will need to gather information\nabout this film and its director. Let’s start by searching for more details about the film ”Panic 5\nBravo”.\n<search>The director of Panic 5 Bravo and the year of birth</search>\n<output>\nDoc 1: ”Panic (2000 film)”\nPanic (2000 film) Panic is a 2000 American crime film directed by Henry Bromell and starring\nWilliam H. Macy, Neve Campbell, Tracey Ullman, John Ritter and Donald Sutherland. Despite\nnot being a box office success, ””Panic”” received universal critical acclaim from critics, some\nof which called it one of the best movies of 2000. The performances of William H. Macy, John\nRitter, and Donald Sutherland were hailed by critics. Alex (Macy), a sad-eyed mournful man,\ngoes into psychotherapy disclosing that he is a hit man and also tells the doctor (Ritter) after a\nfew sessions that he is attracted to\nDoc 2: ”Panic (2000 film)”\noriginality from the tone: quirky yet convincing, irreverent yet moral.”” Panic (2000 film)\nPanic is a 2000 American crime film directed by Henry Bromell and starring William H. Macy,\nNeve Campbell, Tracey Ullman, John Ritter and Donald Sutherland. Despite not being a box\noffice success, ””Panic”” received universal critical acclaim from critics, some of which called\nit one of the best movies of 2000. The performances of William H. Macy, John Ritter, and\nDonald Sutherland were hailed by critics. Alex (Macy), a sad-eyed mournful man, goes into\npsychotherapy disclosing that he is a hit man and also tells the doctor\nDoc 3: ”A Town Called Panic (film)”\nA Town Called Panic (film) A Town Called Panic () is a 2009 internationally co-produced\nstop-motion animated adventure fantasy comedy family film directed by St´ephane Aubier and\nVincent Patar and co-written by Aubier, Guillaume Malandrin, Patar, and Vincent Tavier. The\nfilm is based on the TV series of the same name and stars Aubier, Jeanne Balibar, Nicolas\nBuysse, V´eronique Dumont, Bruce Ellison, Fr´ed´eric Jannin, Bouli Lanners, and Patar, among\nothers. It premiered at the 2009 Cannes Film Festival and was the first stop-motion film to be\nscreened at the festival. The film was released theatrically in Belgium on 17 June\n</output>\nFrom the search results, it seems ”Panic 5 Bravo” is not a widely known film.\nHowever,\nthe first result mentions a film called ”Panic (2000 film)” directed by Henry Bromell. Given this\ninformation, I will now search for Henry Bromell’s birth year.\n31\n"}, {"page": 32, "text": "Preprint.\nTable 42: ADASEARCH stage-2 output. Continuing from part 1, the model uses the wrong director\nas the search query, which ultimately leads to an incorrect answer.\nADASEARCH Stage-2 Output (Continued) (Answer: 1978)\n<search>Henry Bromell birth year</search>\n<output>\nDoc 1: ”Henry Bromell”\nHenry Bromell Alfred Henry Bromell (September 19, 1947 – March 18, 2013) was an American\nnovelist, screenwriter, and director. Bromell joined the crew of NBC police drama ”””” in 1994.\nHe served as a writer and co-executive producer for the show’s third season. He contributed to\nwriting seven episodes for the season. He was promoted to executive producer for the fourth\nseason and wrote a further 17 episodes. He scaled back his involvement with the fifth season and\nbecame a consulting producer. He wrote a further two episodes before leaving the crew at the\nend of the season in 1997.\nDoc 2: ”Henry Bromell”\nscreenwriter and director Caroline Thompson. He then married writer Trish Soodik, who died of\ncancer in January 2009; they had a son, William. His third wife was Sarah. Bromell died March\n18, 2013 of a heart attack, at UCLA Santa Monica hospital at age 65. Henry Bromell Alfred\nHenry Bromell (September 19, 1947 – March 18, 2013) was an American novelist, screenwriter,\nand director. Bromell joined the crew of NBC police drama ”””” in 1994. He served as a writer\nand co-executive producer for the show’s third season. He contributed to writing seven episodes\nfor the season. He was promoted\nDoc 3: ”Henry Bromell”\nHe contributed to a total of 26 episodes as a writer over three seasons with the series.\nHe\nreturned as a co-writer and co-executive producer for the feature-length follow-up ””Homicide:\nThe Movie”” in 2000. He wrote and produced for many television series, including ””Chicago\nHope””, ””Northern Exposure””, ””Homicide: Life on the Street””, ””Brotherhood””, ””Car-\nniv`ale””, and ””Rubicon””. He was a consulting producer, and later Executive Producer on the\nShowtime series ””Homeland”” at the time of his death and wrote four episodes: ””The Good\nSoldier””, ””Representative Brody””, ””Q&A””, and ””Broken Hearts””. He was awarded a for\n””The Good Soldier””, and he was\n</output>\nFrom the search results, Henry Bromell was born in 1947.\nTherefore, the director of\n”Panic (2000 film)” was born in 1947.\n<answer>1947</answer>\n32\n"}]}