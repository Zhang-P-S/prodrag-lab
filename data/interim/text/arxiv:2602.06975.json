{"doc_id": "arxiv:2602.06975", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.06975.pdf", "meta": {"doc_id": "arxiv:2602.06975", "source": "arxiv", "arxiv_id": "2602.06975", "title": "BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents", "authors": ["R. James Cotton", "Thomas Leonard"], "published": "2026-01-16T04:30:04Z", "updated": "2026-01-16T04:30:04Z", "summary": "Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.06975v1", "url_pdf": "https://arxiv.org/pdf/2602.06975.pdf", "meta_path": "data/raw/arxiv/meta/2602.06975.json", "sha256": "5f04771f50bd161784793c6e0a352602a395a081cb075b1251fce57eba3294f9", "status": "ok", "fetched_at": "2026-02-18T02:21:27.940154+00:00"}, "pages": [{"page": 1, "text": "BiomechAgent: AI-Assisted Biomechanical Analysis Through\nCode-Generating Agents\n1st R. James Cotton\nShirley Ryan AbilityLab\nNorthwestern University\nrcotton@sralab.org\n2nd Thomas Leonard\nStritch School of Medicine\nLoyola University Chicago\ntleonard1@luc.edu\nAbstract—Markerless motion capture is making quantitative\nmovement analysis increasingly accessible, yet analyzing the\nresulting data remains a barrier for clinicians without program-\nming expertise. We present BiomechAgent, a code-generating\nAI agent that enables biomechanical analysis through natural\nlanguage and allows users to querying databases, generating\nvisualizations, and even interpret data without requiring users\nto write code. To evaluate BiomechAgent’s capabilities, we de-\nveloped a systematic benchmark spanning data retrieval, visual-\nization, activity classification, temporal segmentation, and clini-\ncal reasoning. BiomechAgent achieved robust accuracy on data\nretrieval and visualization tasks and demonstrated emerging\nclinical reasoning capabilities. We used our dataset to systemati-\ncally evaluate several of our design decisions. Biomechanically-\ninformed, domain-specific instructions significantly improved\nperformance over generic prompts, and integrating validated\nspecialized tools for gait event detection substantially boosted\naccuracy on challenging spatiotemporal analysis where the base\nagent struggled. We also tested BiomechAgent using a local\nopen-weight model instead of a frontier cloud based LLM\nand found that perform was substantially diminished in most\ndomains other than database retrieval. In short, BiomechAgent\nmakes the data from accessible motion capture and much more\nuseful and accessible to end users.\nI. INTRODUCTION\nProgress in markerless motion capture is rapidly making\nquantitative motion analysis much cheaper and easier to\nobtain [1, 2, 3]. It can now even be integrated into clinical\nsettings and detect meaningful differences in gait [4, 5].\nThis is essential for overcoming a critical barrier in clinical\nmovement rehabilitation: we typically cannot measure what\nwe treat. While walking speed has been proposed as the sixth\nvital sign and is commonly measured with a stop watch [6],\nthe details of how people move carries substantial additional\ninformation; information that can now be quantitatively cap-\ntured.\nHowever, scalable access to motion analysis creates a new\nchallenge: analyzing all this data. For commonly performed\nactivities, such as gait analysis, high-quality, activity-specific\nalgorithms are available. For example, we developed the\nGaitTransformer [4, 7], which can accurately detect gait\nevents from video alone. However, there are many other\ncases where novel questions require novel analysis. Even\nnavigating a database to identify specific trials and writing\ncode to visualize kinematics during these trials is a barrier\nto wider use, and formal biomechanical analysis is outside\nthe training scope of most rehabilitation clinicians.\nOur recent work BiomechGPT [8] partially addresses this\nchallenge by developing a multimodal large language model\n(MLLM) that is fluent in both biomechanics and language.\nIt is trained to answer clinically relevant tasks in several\ndomains. For example, it shows high performance at classi-\nfying activities or detecting impaired gait. However, it can\nonly answer the classes of questions for which it is trained.\nFurthermore, like many language models, it can make errors\n(hallucinations), and the interpretability challenge means that\nit cannot explain the rationale for its answers.\nAs an alternative approach, LLMs are rapidly improving\ntheir performance on coding challenges [9, 10, 11], including\npotentially biomechanical analysis and interpretation. Fur-\nthermore, LLMs can be equipped with tools that enable them\nto interact with external data sources or users, giving rise\nto agentic systems [12, 13, 14]. Vision language models\n(VLMs) can also view and help interpret figures they produce\n[15, 16, 17, 18]. BiomechAgent is such an agent, with tools\nfor retrieving biomechanics data from a database, writing\ncode to analyze it, generating plots, visualizing these plots,\nand packaging it into a familiar and intuitive chat interface.\nEssential to this ability is the fact that all of our data is\nprocessed in PosePipe [19], which leverages DataJoint [20]\nto track all of our data collection and processed results in a\ndatabase. In this work, we focused on analyzing biomechan-\nics obtained with our multiview markerless motion capture\nsystem [3, 21, 22], but this is naturally extensible to biome-\nchanics obtained from our monocular Portable Biomechanics\nLaboratory [5].\nWe see BiomechAgent and BiomechGPT as synergistic\nin the same way as the cognitive science theory of Sys-\ntem 1 and System 2 thinking [23]. Specifically, System 1\nthinking is an automatic, nearly effortless form of percep-\ntion (e.g., BiomechGPT directly outputs an answer upon\nobserving biomechanical sequences). In contrast, System 2 is\neffortful and involves logical reasoning (e.g., BiomechAgent\nwrites code and explicitly manipulates data). Furthermore,\nBiomechAgent can be provided with access to other biome-\nchanical analysis tools, such as BiomechGPT.\nOur goal in this work is to systematically benchmark\nBiomechAgent and measure its performance across multiple\narXiv:2602.06975v1  [cs.CL]  16 Jan 2026\n"}, {"page": 2, "text": "domains to understand its performance limits better. This\ncan guide future improvements and inform end users of the\ntasks for which it can be trusted. However, a challenge for\nbenchmarking agentic systems is that while the potential\nuses of BiomechAgent are open-ended, quantitative and\ninterpretable benchmarking requires defining a discrete set\nof tasks. To address this, we started with open-ended user\ntesting to identify some example use cases. From these,\nwe established several domains of tasks, including targeted\ndata retrieval, data visualization, activity classification and\nsegmentation, and even clinical reasoning. For many of these,\nwe had additional ground-truth data that we used to develop\nour benchmarking dataset, including clinical reasoning and\nactivity classification, tested in a two-alternative forced-\nchoice (2AFC) paradigm. In some cases where it is harder\nto define an exact answer, we used an LLM-as-a-Judge\nframework to evaluate the performance of BiomechAgent\n[24].\nWe also tested the importance of several of our design\ndecisions, including custom instructions for our agent that\nprovide context for biomechanical analysis and the use of\nLLMs that can be run locally (Gemma models) versus\nfrontier, cloud-based models (Gemini 2.5 Flash Lite). In\naddition, we tested whether providing the agent with spe-\ncialized analysis tools, such as gait event detection using\nour GaitTransformer [4, 7], improved performance on tasks\non which it otherwise struggled.\nOverall, we found that BiomechAgent is a powerful tool\nthat provides a user-friendly, convenient chat interface for\nboth data retrieval and plotting. It also demonstrated im-\npressive examples of reasoning about clinically meaningful\nfeatures of movement with only modest prompting. In short,\nour contributions are:\n• We developed BiomechAgent, an agentic system for\naccessing and analyzing biomechanical data collected\nwith markerless motion capture to facilitate this analysis\nby clinicians.\n• We developed an evaluation dataset that was motivated\nby tasks we wanted BiomechAgent to solve, where we\ncould validate the answer either directly against data\nunavailable to BiomechAgent or using an LLM-as-a-\nJudge.\n• We demonstrated that BiomechAgent shows strong\nperformance on many of these tasks, and validated\nsome of our design decisions, including using a cloud-\nbased frontier model (Gemini 2.5 Flash Lite), custom\ninstructions to the agent about biomechanical analysis,\nand providing additional tools to the agent for detecting\ngait event timing.\nII. METHODS\nA. BiomechAgent System\n1) Agent\nArchitecture:\nBiomechAgent\nis\na\ncode-\ngenerating\nagent\narchitecture\nbuilt\non\nthe\nsmolagents\nframework [25]. Unlike function-calling approaches, in\nwhich language models output structured JSON to invoke\npredefined functions, smolagents generates and executes\nPython code directly, which is effective for expressing\nagentic plans [24]. The agent receives a natural-language\nquery, reasons about which computational steps are required,\nwrites executable code, observes the results in a sandboxed\nenvironment, and iterates until it reaches a final answer.\nThe agent accesses specialized capabilities through special\nPython functions called tools. These tools provide database\naccess for querying participant trials and fetching kinematic\ndata [20], gait analysis for detecting foot contact events from\nkinematics using GaitTransformer [7], and visualization for\ngenerating static and interactive plots. Tool documentation\nembedded in function definitions informs the agent how to\nuse each tool, including the expected inputs and outputs,\nand the agent then writes code to flexibly use these tools to\nretrieve and analyze data.\nDuring execution of BiomechAgent, the agent receives a\nuser prompt. It then begins emitting text, including flags\nindicating that sections are code. The smolagent framework\nextracts those from the LLM as they are produced, and when\na complete block of code is received, it is then executed.\nThe agent then receives that as the new input during a\nchat sequence. Once the agent determines it can answer the\nquestion, it responds with a ‘final answer’ tool call which\nstops the sequence.\n2) Language Model Backends: We evaluated BiomechA-\ngent with two language-model backends, representing cloud\nand local deployment scenarios. Gemini 2.5 Flash Lite [26],\na frontier cloud-based model, and MedGemma 27B [27], an\nopen-weights medical foundation model, both of which can\nprocess images and text. Whenever the agent generated an\nimage and used the ‘save plot’ tool, the image was presented\nto the end user and provided to the visual language model\n(VLM), along with the text, for subsequent chat iterations.\n3) Custom Instructions: Domain-specific guidance is em-\nbedded in the agent’s system prompt to prevent common\nerrors discovered during early testing. These instructions\ninclude biomechanical formulas (e.g., the calculation of the\nasymmetry index) and expected parameter ranges. It also\nprovides guidance on standards, such as using consistent\ncolors to indicate left and right, using real units of time\nand degrees on the axes, and avoiding common errors (e.g.,\nforgetting to import mathematical libraries or plotting tools).\nThey also encourage BiomechAgent to reason about clinical\nquestions and to formulate hypotheses to explore in the data,\nas often observed in the traces provided in the supplementary\ndocument.\n4) Data Sources: Kinematics were obtained from multi-\nview markerless motion capture [3]. Data were managed via\nDataJoint [20] and the PosePipe processing framework [19].\nThe Northwestern University Institutional Review Board\napproved all procedures, and participants provided informed\nconsent.\n5) Chat Interface: Our end-user chat interface was im-\nplemented in streamlit [28]. The user inputs are provided\nto BiomechAgent and then the resulting code and plots are\nshown to the user as they are generated until the agent emits\na ‘final answer’ in response to the input. At that point, the\n"}, {"page": 3, "text": "user can either start a new chat or ask a follow-up question\nto continue the existing chat interface.\nB. Dataset Generation\nWe developed test cases across seven categories. These\nwere developed by first mining the authors’ transcripts using\nthe system in an open-ended manner and then identifying\ncategories of use. We distilled these into multiple topics\nwithin each category, where the correct answers could be\nchecked against other available data. These additional data\nsources were not made available to BiomechAgent. We\ngenerated templates for each of these topics to allow a dataset\nto be generated for subsequent testing.\n1) Privileged Data Sources: A critical design principle is\nthe separation between data sources available to the agent\nand those used solely for ground-truthing during dataset\ngeneration. During evaluation, the agent has access only\nto kinematic reconstructions and activity annotations de-\nrived from video. The following privileged sources generate\nground truth answers but are unavailable to the agent\nduring evaluation:\nInstrumented walkway: Per-step spatiotemporal measure-\nments (velocity, step length, stride time) serving as ground\ntruth for gait parameter estimation. Clinical records: Diag-\nnosis labels (stroke, prosthesis user, control), affected side,\nand level of amputation. Berg Balance Scale assessments:\nStandardized fall risk scores obtained within a week of a gait\ntrial for inpatients. Activity segmentation annotation: Manual\nlabels of activity segmentation, such as rising from a chair,\nturning, or the push phase of wheelchair propulsion.\n2) Database Tasks: Database tasks test the agent’s ability\nto query and aggregate information from the motion capture\ndatabase. Questions span trial counts (“How many trials does\nparticipant X have?”), data shapes (“What are the dimensions\nof the kinematics array?”), activity queries (“List all TUG\ntrials for participant X”), session information, and date spans.\nGround truth is computed via direct queries to the database\nusing validated calls. These tasks are deterministic and verify\nthat the agent correctly interfaces with the database.\n3) Activity Classification Tasks: Activity classification\ntasks present two-alternative forced choice (2AFC) questions\nrequiring the agent to identify which of two trials con-\ntains a specific activity—such as backward walking versus\nforward walking or Timed Up and Go versus overground\nwalking—by analyzing kinematics alone. Success requires\nidentifying discriminative kinematic features: reversed ankle\nplantarflexion-dorsiflexion sequences for backward walking,\ncharacteristic turning patterns for TUG, or altered coordi-\nnation patterns for different gait conditions. The agent was\nspecifically instructed not to use activity annotations, and the\ntool providing activity labels was blocked via a forbidden\ntools constraint to enforce this.\n4) Spatiotemporal Tasks: Spatiotemporal tasks evaluate\nestimation of gait parameters from kinematics without ac-\ncess to instrumented walkway measurements. Topics include\nvelocity and spatial parameters (walking speed, step length,\nstride length), temporal parameters (stride time, stance and\nswing duration, stance/swing ratios), gait event detection\n(identifying stance and swing phase timing), event counting\n(number of left and right foot contacts), and asymmetry\nmetrics (left-right differences in step length and timing).\n5) Clinical Tasks: Clinical tasks require inferring clinical\nstatus from kinematic patterns. In a 2AFC format, the agent\nmust determine whether one of two participants has a stroke\n(versus an able-bodied control) or uses a lower-limb prosthe-\nsis, identify the affected side, or compare fall risk. Ground\ntruth comes from clinical records and Berg Balance Scale\nassessments. Success requires recognizing clinically relevant\npatterns, such as an asymmetric range of motion.\n6) Segmentation Tasks: Segmentation tasks test the identi-\nfication of temporal phases within complex activities. Ques-\ntions address phase duration (“How long was the turning\nphase?”), timing (“When did the sit-to-stand transition be-\ngin?”), and counting (“How many walking segments oc-\ncurred?”). Ground truth comes from manual video anno-\ntations. These tasks evaluate whether the agent can parse\ncontinuous motion into clinically meaningful segments.\n7) Visualization Tasks:\nVisualization tasks assess the\nagent’s ability to generate appropriate plots for different\nanalytical needs: single joint trajectories, bilateral compar-\nisons, multi-joint summaries, pelvis trajectories, and range\nof motion summaries. Evaluation uses an LLM-as-judge\napproach with domain-specific rubrics to assess whether the\nvisualization addresses the request and displays appropriate\ndata.\n8) Perception Tasks: Perception tasks evaluate the agent’s\nability to interpret time series plots and extract meaningful\nobservations. Unlike visualization tasks which test plot gen-\neration, perception tasks require the agent to first create a\nvisualization and then accurately describe what it observes.\nTopics include clinical pattern recognition (detecting ROM\nasymmetry indicative of stroke or prosthesis use), activity\npattern recognition (identifying turning, backwards walking,\nor head nodding from kinematic traces), movement phase\nidentification (recognizing sit-to-stand transitions, walking\nonset, or L-shaped trajectories), and gait cycle analysis\n(estimating cadence, assessing stride symmetry). The key\nevaluation criterion is whether the agent’s description is\nconcordant with the expected pattern given ground truth\nclinical or activity information that the agent cannot access\ndirectly. LLM-as-judge scoring emphasizes specific value\ncitations over generic descriptions.\nC. Evaluation\n1) Agent Configurations: We tested five agent configura-\ntions to understand the contributions of different components:\nConfiguration\nDescription\nBaseline\nGemini 2.5 Flash Lite with custom biomechanics instruc-\ntions\n+GaitTransformer\nBaseline with access to learned gait event detection tool\nNo Instructions\nDefault prompts only, removing domain-specific guidance\n(lesion study)\nNo Vision\nBaseline without feeding generated plots back to the model\nMedGemma\nLocal 27B model for on-premises deployment\n"}, {"page": 4, "text": "The GaitTransformer variant was the only one with ac-\ncess to this tool; it otherwise matched the Baseline variant\nand was tested only on the spatiotemporal dataset. The\n“No Instruction” variant lacked our custom biomechanically\ngrounded instructions. The MedGemma variant used a local\n27B model running on an A100 with 4-bit quantization. The\n“No Vision” did not provide images to the LLM when the\nagent generated them and removed the instructions about\nlooking at them.\n2) Scoring Methods: Deterministic scoring was applied\nwhere ground truth permits objective evaluation. Categorical\nresponses (activity names, laterality, participant IDs) require\nexact string matches. Numerical responses are specified with\ntolerances appropriate to measurement uncertainty: ±10%\nrelative tolerance for most spatiotemporal parameters (veloc-\nity, step length, temporal measures), ±5 percentage points for\nasymmetry metrics, ±1 for footfall counts, and exact matches\nfor database counts and data shapes.\nLLM-as-judge [24] was used for tasks requiring semantic\nevaluation, particularly visualization quality and clinical rea-\nsoning. Domain-specific rubrics score multiple dimensions\non a 0-10 scale. Importantly, the rubrics evaluate reasoning\nquality alongside correctness: well-reasoned answers that\nreach incorrect conclusions score higher than lucky guesses\nwithout justification, encouraging genuine analytical capabil-\nity over pattern matching.\n3) Evaluation Visualization: We also developed a Stream-\nlit GUI to visualize agent behavior from the evaluation\ndataset. This shows a chat interface and the responses from\nBiomechAgent, including the reasoning traces, the code that\nwas executed, and the plots that were generated.\n4) Additional Metrics: Beyond accuracy, we tracked to-\nken usage and execution time to characterize computational\ncosts across configurations. We also tracked metrics such as\nthe number of coding errors produced.\nIII. RESULTS\nA. End user testing\nInitial testing of BiomechAgent was informal, conducted\nthrough ad hoc use by the authors, who both found it\neasy and intuitive to use. It consistently identified trials,\nfiltered them by activity to select the desired ones, and\ngenerated plots aligned with our instructions. Figure 1 shows\nan example dialog in which the user asks the system to\nanalyze hip and knee kinematics and determine which side\nexhibits hemiparesis. Despite never being instructed in this\ntask, it generated a plot, analyzed it using the VLM, and\nidentified the side with hemiparesis. Logs from the initial\nuse of BiomechAgent were then used to develop some of the\nquestion classes employed in our systematic benchmarking.\nB. Overall Performance between Models\nFigure 2 shows performance across different domains for\neach of our agent variations. Our baseline model performed\nas well as or better than a model without custom prompt\ninstructions (i.e., only the default instructions from smola-\ngents [25]). Using a local model, MedGemma 27B, solved\nTABLE I\nDETAILED PERFORMANCE BY TOPIC GROUP (BASELINE)\nCategory\nTopic\nBaseline\nDatabase\nDatabase Access\n4/4 (100.0%)\nData Shape\n7/8 (87.5%)\nActivity Context\n8/8 (100.0%)\nTrial Duration\n4/4 (100.0%)\nSession Count\n4/4 (100.0%)\nJoint Index\n4/4 (100.0%)\nDate Span\n4/4 (100.0%)\nActivity Diversity\n8/8 (100.0%)\nVisualization\n(all topics)\n48/50 (96.0%)\nPerception\n(all topics)\n33/50 (66.0%)\nClinical\nDiagnosis 2AFC\n9/12 (75.0%)\nStroke Laterality\n2/9 (22.2%)\nLLPU 2AFC\n7/8 (87.5%)\nLLPU Laterality\n4/6 (66.7%)\nAmputation Level\n5/6 (83.3%)\nBalance\n9/20 (45.0%)\nActivity\nActivity 2AFC\n24/31 (77.4%)\nCross-Activity\n2/2 (100.0%)\nWalking Type\n11/14 (78.6%)\nSegmentation\nPhase Duration\n12/15 (80.0%)\nPhase Timing\n15/20 (75.0%)\nPhase Count\n2/10 (20.0%)\nTABLE II\nGAITTRANSFORMER ABLATION BY SPATIOTEMPORAL TOPIC\nTopic\nBaseline\n+GaitTransformer\n∆\nEvent Counting\n0/8 (0.0%)\n6/8 (75.0%)\n+75%\nTemporal\n0/12 (0.0%)\n10/12 (83.3%)\n+83%\nGait Events\n1/8 (12.5%)\n4/8 (50.0%)\n+38%\nVelocity & Length\n3/12 (25.0%)\n9/12 (75.0%)\n+50%\nAsymmetry\n1/10 (10.0%)\n3/10 (30.0%)\n+20%\nmany tasks, but not as well as using Gemini 2.5 Flash Lite.\nAll models performed poorly on spatiotemporal tasks, except\nwhen provided access to special gait analysis tools described\nbelow. We also discuss the ‘No Vision’ tests in more depth\nbelow.\nC. Detailed Performance Analysis of Baseline Model\nTable I provides a detailed breakdown of performance by\ntopic group within each category. Overall this shows that our\nbaseline model performed well across many different tasks.\nFor example, performance was near-perfect for database\naccess tasks (98%), with only the Data Shape topic showing\nany errors. The visualization tasks showed consistent per-\nformance generating plots aligned to requests (96%), while\nperception tasks—which require interpreting the generated\nplots—showed more variable performance (66%) reflecting\nthe added difficulty of visual interpretation. Both task types\nused an LLM-as-a-Judge to determine if the response was\nconsistent with the instructions, with the judge able to see\nthe generated images.\nD. Spatiotemporal Performance\nWe found that one domain in which BiomechAgent\nperformed poorly was the detailed identification of gait\nevent timing and the computation of spatiotemporal gait\nmetrics. This is somewhat unsurprising, as performing this\nwith only kinematic data would typically require designing\nand optimizing numerous heuristics to produce a reliable\nfunction. However, by giving BiomechAgent access to our\nGaitTransformer as an additional tool to analyze kinematic\n"}, {"page": 5, "text": "Fig. 1. BiomechAgent chat interface screenshots from a sequence of reasoning about the side with hemiparesis, including plotting and feeding the result\ninto the VLM.\nFig. 2. Performance comparison across task categories for different model\nconfigurations. Baseline+GT replaces the spatiotemporal results from the\nbaseline evaluation with those when the GaitTransformer tool was also\nincluded.\ndata and produce validated gait event timing, performance\non these tasks improved markedly.\nTable II presents a detailed breakdown of various topics,\nincluding gait analysis, compared with ground-truth data\nfrom our instrumented walkway, which the agent could not\naccess. Measuring step length asymmetry was the topic that\nperformed most poorly. Sources of performance limitations\nin these topics included differences between the GaitTrans-\nformer timing and the instrumented walkway, or the agent\nnot using the exact same definitions of terms, such as step\nlength, as the instrumented walkway. This requires privileged\naccess to the mat’s coordinate frame, although our prompts\nencourage analysis of heel positions relative to the direction\nof pelvis movement to approximate this.\nE. No Vision Performance\nWe anticipated that using the vision capabilities of Gemini\n2.5 Flash Lite to inspect plots would improve the perfor-\nmance and our custom instructions encouraged the model\nto perform this as a typical part of analyzing data when\nappropriate. This was motivated by how impressed we were\nat the ability for it to use this to visually estimate things like\nTABLE III\nEFFICIENCY METRICS BY MODEL\nModel\nAvg Time (s)\nAvg Tokens\nAvg Errors\nBaseline\n12.4\n61,365\n0.58\nNo Instructions\n10.0\n39,575\n0.27\nNo Vision\n12.0\n55,216\n0.52\nMedGemma\n496.3\n52,006\n1.06\nrange of motion and gait event timing, as well as papers\nshowing that visualizing graphs is a better representation\nfor VLMs than just directly feeding time series data in as\ntext [18]. To test this, we conducted a lesion experiment\nin which we disabled the transfer of generated plots to\nGemini’s visual inputs and removed instructions that would\notherwise prompt it to attend to plots. As shown in Figure 2,\nperformance on most tasks was comparable or even better\nwithout visualization. This included tasks that we anticipated\nwould benefit from graphing, such as clinical reasoning.\nTo understand when vision genuinely helps, we compared\nperformance on two visual task categories: Perception tasks\nthat require interpreting generated plots (e.g., “describe the\nmovement pattern you observe”, “estimate cadence from\nhip flexion peaks”) and Visualization tasks that only require\ncreating a correct plot (e.g., “plot the knee angle during\nwalking”). As shown in Figure 2, vision capabilities helped\nwith perception tasks (66% with vision vs 44% without),\nwhile visualization performance remained high regardless of\nvision (96% vs 94%). However, the model without vision\nwas still able to write code that could solve many perception\ntasks, such as computing statistics or detecting peak locations\nin waveforms.\nF. Token Usage and Coding Error Rate\nIn addition to accuracy, other important metrics for captur-\ning our agent’s behavior include the time to complete each\ntask, the number of tokens used, and the number of coding\nerrors produced. Table III shows that all of our models used\na consistent number of tokens, other than the model with no\ninstructions, which used fewer. This model also completed\n"}, {"page": 6, "text": "the tasks more quickly, likely because our instructions en-\ncouraged additional steps, such as articulating hypotheses\nand reviewing results. The local model (MedGemma 27B)\nrequired two orders of magnitude more time per task and\nproduced more than twice as many errors.\nG. Better Results with Improved Prompts\nThe quality of the instructions, documentation of the\ntools, and the questions played an important role in the\nperformance of BiomechAgent. With zero documentation\nof the tools, the agent would have no concept of how to\nuse them, and performance would be at zero. The ablation\nresults above show that our custom instructions contributed\npositively to performance. Our goal in developing our dataset\nwas to create questions that were sufficiently detailed and\nclear to provide a reasonable test for agentic analysis of\nbiomechanical data, without being overly detailed to the\npoint of providing the answer in the question. For tests on\nwhich the agent failed, we often found that a more detailed or\nprescriptive question, which a human user might try, could\nyield a correct result. At one level, this reflects a realistic\ntype of iterative chat an end user might have with the agent.\nAt another extreme, it is moving all of the ‘intelligence’ from\nthe LLM to the questioner.\n1) Example 1: Hurdle Detection: We asked the agent to\nidentify which overground walking trial for a participant\ncontained a hurdle step-over event. The original prompt\nprovided no guidance on which kinematic features to ex-\namine: “Through kinematic analysis of ALL the overground\nwalking trials, please determine which trial [contained a\nhurdle step].” The agent searched for explicit annotations\nlabeled “hurdle” or “obstacle,” found none, and did not\nattempt kinematic analysis. After adding “stepping over a\nhurdle requires greater hip flexion than normal walking,”\nthe agent computed maximum hip flexion across all trials,\nidentified a clear outlier, and correctly identified the hurdle\ntrial.\n2) Example 2: Stroke Detection: We asked the agent to\nidentify which of the two participants had a stroke based\non gait kinematics. The original prompt asked to “analyze\njoint angle patterns” without specifying methodology. The\nagent focused on walking speed and provided the wrong\nanswer. With methodological guidance specifying (1) com-\npare only overground walking trials, (2) calculate left-right\nROM asymmetry, and (3) use a >15% asymmetry threshold,\nthe agent computed hip asymmetry of 17.15% and knee\nasymmetry of 52.83%, and correctly identified the participant\nwith a stroke.\nIV. DISCUSSION\nIn general, we found BiomechAgent to be a very power-\nful and convenient tool for accessing and visualizing data.\nBiomechAgent has become a regular first-line tool for us\naccessing and analyzing data, allowing us to ‘dogfood’ this\ntool [29] and convinced us of its robustness and utility. At\nthe same time, the variety of uses presented a challenge\nfor systematic benchmarking. Consequently, our quantitative\nbenchmark covers only a small fraction of the task space\nthat BiomechAgent might be able to accomplish. In our\nsupplemental files, we included an HTML file that allows\nbrowsing transcripts of BiomechAgent interactions, which\ndemonstrates the range of reasoning capabilities that emerge\neven on this finite dataset.\nThe consistent and robust performance of BiomechAgent\nin both data retrieval and plotting addresses a common\nrequest from clinicians using our markerless motion capture\nsystem. The ability to ask in natural language, ‘please pull up\nthe knee kinematics during a single leg squat for participant\nABC and compare the first session to the last session,’\nlowers the barrier to working with biomechanical data. This\ncomplements the benefit of markerless motion capture by\nlowering the barrier to data collection. While collecting and\nprocessing data to obtain kinematics is fairly straightfor-\nward, further analysis to generate clinical insights becomes\nthe rate-limiting factor. This motivated the development of\nBiomechAgent, and the current capabilities already address\nmany use cases. However, many important uses are not\nwell addressed by BiomechAgent. For example, developing\ndashboards that longitudinally trend a standard set of metrics\n- such as gait parameters - in an intuitive and standardized\nmanner is an everyday use case that is a poor fit for\nBiomechAgent. While it may be able to write an analysis,\napply it to several trials, and extract results, it will perform\na different analysis the next time the question is asked.\nOur results on the spatiotemporal tasks are particularly\nnoteworthy as they show an explicit limitation of frontier\nmodels on solving certain biomechanical tasks without ac-\ncess to despoke tools – in this case, our GaitTransformer\nmodel that is itself another deep learning model trained\nto identify gait events [4, 7]. This also raises the question\nof when additional bespoke analysis code should be hand-\ncrafted and validated, and then provided to BiomechAgent.\nIn preliminary experiments, we found that, with guidance,\nBiomechAgent can produce reusable functions for tasks such\nas activity segmentation. We anticipate it will not be a\nbig leap to extend BiomechAgent to develop its own tools\nand save them for future use, particularly when there is\nadditional data available to validate this process. This is a\nnatural extension of the growing use of coding agents in\nother aspects of programming. We have also demonstrated\nthat BiomechAgent can easily and naturally use other tools\nsuch as BiomechGPT [8], and when this tool in the agent\nprompt, it will naturally use this to solve tasks like activity\nand diagnosis classification, similarly, when provided access\nto KinTwin [30], it will use this to compute torques and\nground reaction forces and to detect gait events when the\nGaitTransformer is unavailable.\nWe also envision that BiomechAgent will be just one\nagent in a zoo of agentic systems for analyzing rehabilitation\ndata to deliver better care. For example, we have piloted\nagents that can extract structured rehabilitation information\nfrom therapy notes. These can be coordinated by supervising\nagents, who then run many such analyses across different\nindividuals and time points, and identify and test novel\n"}, {"page": 7, "text": "hypotheses about rehabilitation, such as within our Causal\nFramework for Precision Rehabilitation [31]. Such a system\nhas been termed an AI Co-Scientist [32, 33], and we hope\nBiomechAgent will be a powerful tool to accelerate quanti-\ntative movement analysis, providing actionable information\nfor treatment optimization.\nIn addition to performing less accurately overall, running\nour benchmark with MedGemma 3 is relatively slow, even on\nan A100. Furthermore, a high-capacity (80GB) card is neces-\nsary to accommodate the long conversation lengths required\nfor some analyses. Even then, some evaluations failed due\nto finite memory and exceeding the 128k maximum context\nlength. If performing large-scale analysis rather than running\na chat interface, substantial speedups can be achieved by\nrunning multiple agents in parallel using tools such as vLLM\n[34], as LLM inference scales efficiently with parallelization.\nThis could be useful in situations where an organization is\nunable to use cloud-based language models, particularly if\nit is also analyzing relationships between other protected\nhealth information and biomechanics. With more transcripts\nof successful BiomechAgent usage, a smaller model could\nlikely be fine-tuned for this range of biomechanical analysis\ntasks to achieve equivalent or better performance. An even\nmore promising opportunity of using local models is training\na combined BiomechGPT+Agent that can ‘perceive’ biome-\nchanics with System 1 reasoning, while then also writing\nanalytic code to verify and refine those perceptions to provide\nrobustness, grounding, and interpretability. Our benchmarks\nprovide a large quantity of examples with known ground\ntruth that can be used to evaluate the quality of the reasoning\nand open the opportunity for reinforcement learning from\nverifiable feedback [35, 36]. This can also be combined with\nmore recent methods from Agent0 [37], which use another\nagent to develop a curriculum of more difficult challenges to\nadvance the next-generation BiomechGPT+Agent.\nThe substantially better performance with Gemini 2.5\nFlash Lite over MedGemma shows the importance of the\n‘intelligence’ in the LLM for solving these tasks. It serves\nto indicate that we are in a transitional period in which\nthese models move from frequently failing on many tasks\nto reliably solving them. We attempted to evaluate the\nGemini 3.0 Flash Preview but found the preview model’s API\nunstable. It is also essential to consider security, regulatory\nrequirements, and privacy when using a cloud-based LLM\nwith clinical data. In our case, several features make this\nsecure and compliant. Firstly, the database access tools\nprovided to BiomechAgent grant access only to anonymous\nsubject IDs, trial information, and kinematics (joint angles\nand locations). It is not able to access any of the videos used\nin the markerless motion capture analysis to produce these\nkinematics. Such kinematics are not considered identifiable\ninformation, and the NIH encourages and even mandates\npublic sharing of them for federally funded research. Second,\nthe kinematics do not actually leave the on-premises server.\nInstead, the LLM writes code to analyze the data and execute\nit on the server. However, in some cases, it may ‘print’\nkinematic data as text and is often encouraged to visualize\nplots of these data (i.e., Figure 1); such data is sent to the\ncloud. Thirdly, we have a Business Association Agreement\n(BAA) with our cloud LLM provider, which further governs\nthe location of this data. However, we strongly encourage all\nreaders to review their own applicable regulations and to use\nlocal models only when there is any uncertainty.\nBiomechAgent is not without its limitations. It uses a\nfair number of tokens to answer questions, and our entire\ndataset of 310 questions consumed approximately 15-20M\ntokens. Unsurprisingly, it still has limits; for vaguely phrased\nqueries or questions that are not readily answered by visual-\nizations, it is less likely to produce accurate responses. More\nspecific questions, along with instructions encouraging it to\ngenerate hypotheses and reason them through by analyzing\nthe data, helped improve performance. We anticipate there\nremain many opportunities to improve the performance and\nrobustness to question phrasing through further refinement\nof the instructions, tool documentation, including the use\nof dedicated tools for this (e.g., DSPy [38, 39]), and of\ncourse, the opportunity to fine-tune custom models for this.\nHowever, we defer this to further systematic end-user testing\nto delineate better real-world uses and failure modes. Finally,\nour analysis of spatiotemporal gait parameters shows that it\nis sometimes better to provide additional well-validated tools\nfor a more detailed analysis\nV. CONCLUSION\nBiomechAgent\ndemonstrates\nthat\ncode-generating\nAI\nagents can provide clinicians with accessible, natural lan-\nguage interfaces for biomechanical analysis, achieving ro-\nbust accuracy across multiple tasks, including data retrieval,\nvisualization, and clinical reasoning. By combining fron-\ntier language models with domain-specific instructions and\nspecialized analysis tools, such systems can bridge the gap\nbetween the increasing availability of motion capture data\nand the need for clinician-friendly analysis.\nVI. ACKNOWLEDGMENTS\nWe used Claude Code (Anthropic) during the development\nof BiomechAgent for software implementation, to assist in\ndeveloping the evaluation dataset and topics from transcripts\nof chat usage, to help format the results into the manuscript,\nto ensure the code and the methods description were accu-\nrately aligned, and to support writing this manuscript. The\nactual core agent code was written only with the use of\nGithub Copilot. Grammarly was used for editing.\nThis work was supported by R01HD114776 to RJC from\nNational Institute of Child Health and Human Development\nand the National Center for Medical Rehabilitation Research.\nREFERENCES\n[1] R. M. Kanko, E. K. Laende, E. M. Davis, W. S. Selbie, and K. J.\nDeluzio, “Concurrent assessment of gait kinematics using marker-\nbased and markerless motion capture,” Journal of Biomechanics,\nvol. 127, p. 110 665, Oct. 2021.\n[2] S. D. Uhlrich et al., “Opencap: Human movement dynamics from\nsmartphone videos,” PLOS Computational Biology, vol. 19, no. 10,\ne1011462, Oct. 2023.\n"}, {"page": 8, "text": "[3] R. J. Cotton, “Differentiable Biomechanics Unlocks Opportunities for\nMarkerless Motion Capture,” in 2025 International Conference On\nRehabilitation Robotics (ICORR), May 2025, pp. 44–51.\n[4] A. Cimorelli, A. Patel, T. Karakostas, and R. J. Cotton, “Validation\nof portable in-clinic video-based gait analysis for prosthesis users,”\nScientific Reports, vol. 14, no. 1, pp. 1–12, Feb. 2024.\n[5] J. D. Peiffer et al., Portable biomechanics laboratory: Clini-\ncally accessible movement analysis from a handheld smartphone,\nhttps://arxiv.org/abs/2507.08268, 2025.\n[6] S. Fritz and M. Lusardi, “White Paper: ”Walking Speed: The Sixth\nVital Sign”,” Journal of Geriatric Physical Therapy, vol. 32, no. 2,\npp. 2–5, 2009.\n[7] R. J. Cotton, E. McClerklin, A. Cimorelli, A. Patel, and T. Karakostas,\n“Transforming Gait: Video-Based Spatiotemporal Gait Analysis,” in\n2022 44th Annual International Conference of the IEEE Engineering\nin Medicine & Biology Society (EMBC), Jul. 2022, pp. 115–120.\n[8] R. Yang, A. Kennedy, and R. J. Cotton, Biomechgpt: Towards a\nBiomechanically Fluent Multimodal Foundation Model for Clinically\nRelevant Motion Tasks, May 2025.\n[9] J. Austin et al., Program Synthesis with Large Language Models, Aug.\n2021.\n[10] C. E. Jimenez et al., “Swe-bench: Can Language Models Resolve\nReal-world Github Issues?,” 2024.\n[11] J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim, “A Survey on Large\nLanguage Models for Code Generation,” ACM Trans. Softw. Eng.\nMethodol., Jul. 2025.\n[12] S. Yao et al., “React: Synergizing Reasoning and Acting in Language\nModels,” in International Conference on Learning Representations\n(ICLR), 2023.\n[13] T. Schick et al., “Toolformer: Language Models Can Teach Them-\nselves to Use Tools,” in Advances in Neural Information Processing\nSystems, Curran Associates Inc., Dec. 2023.\n[14] H. Wang, J. Gong, H. Zhang, J. Xu, and Z. Wang, Ai Agentic\nProgramming: A Survey of Techniques, Challenges, and Opportunities,\nSep. 2025.\n[15] J.-B. Alayrac et al., “Flamingo: A Visual Language Model for Few-\nShot Learning,” in Advances in Neural Information Processing Sys-\ntems, vol. 35, Dec. 2022, pp. 23 716–23 736.\n[16] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual Instruction Tuning,”\nin Advances in Neural Information Processing Systems, vol. 36, Dec.\n2023, pp. 34 892–34 916.\n[17] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque, “Chartqa:\nA Benchmark for Question Answering about Charts with Visual and\nLogical Reasoning,” in Findings of the Association for Computational\nLinguistics: ACL 2022, Association for Computational Linguistics,\nDublin, Ireland, May 2022, pp. 2263–2279.\n[18] H. Liu, C. Liu, and B. A. Prakash, “A Picture is Worth A Thousand\nNumbers: Enabling LLMs Reason about Time Series via Visualiza-\ntion,” in Proceedings of the 2025 Conference of the Nations of the\nAmericas Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), Association\nfor Computational Linguistics, Apr. 2025, pp. 7486–7518.\n[19] R. J. Cotton, Posepipe: Open-Source Human Pose Estimation Pipeline\nfor Clinical Research, Mar. 2022.\n[20] D. Yatsenko et al., “Datajoint: Managing big scientific data using\nMATLAB or Python,” bioRxiv, 2015.\n[21] R. J. Cotton et al., “Optimizing Trajectories and Inverse Kinematics\nfor Biomechanical Analysis of Markerless Motion Capture Data,” in\nIEEE International Consortium for Rehabilitation Robotics, arXiv,\nMar. 2023.\n[22] R. J. Cotton, A. Cimorelli, K. Shah, S. Anarwala, S. Uhlrich, and\nT. Karakostas, “Improved Trajectory Reconstruction for Markerless\nPose Estimation,” in 45th Annual International Conference of the IEEE\nEngineering in Medicine and Biology Society, Sydney, Mar. 2023.\n[23] D. Kahneman, Thinking, fast and slow. New York, NY: Farrar, Straus,\n2011.\n[24] L. Zheng et al., “Judging LLM-as-a-Judge with MT-Bench and Chat-\nbot Arena,” in Advances in Neural Information Processing Systems,\nvol. 36, Dec. 2023, pp. 46 595–46 623.\n[25] A. Roucher, A. V. del Moral, T. Wolf, L. von Werra, and E. Kau-\nnism¨aki, Smolagents: A smol library to build great agentic systems,\nhttps://github.com/huggingface/smolagents, 2025.\n[26] G. Comanici, E. Bieber, M. Schaekermann, et al., Gemini 2.5: Pushing\nthe Frontier with Advanced Reasoning, Multimodality, Long Context,\nand Next Generation Agentic Capabilities, Jul. 2025.\n[27] A. Sellergren et al., Medgemma Technical Report, Jul. 2025.\n[28] Streamlit, Inc., Streamlit: A faster way to build and share data apps,\nhttps://github.com/streamlit/streamlit, 2024.\n[29] W. Harrison, “Eating Your Own Dog Food,” IEEE Software, vol. 23,\nno. 3, pp. 5–7, May 2006.\n[30] R. J. Cotton, Kintwin: Imitation learning with torque and muscle\ndriven biomechanical models enables precise replication of able-\nbodied and impaired movement from markerless motion capture,\nhttps://arxiv.org/abs/2505.13436, 2025.\n[31] R. J. Cotton et al., A Causal Framework for Precision Rehabilitation,\nNov. 2024.\n[32] C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha, The AI\nScientist: Towards Fully Automated Open-Ended Scientific Discovery,\nSep. 2024.\n[33] J. Gottweis et al., Towards an AI co-scientist, Feb. 2025.\n[34] W. Kwon et al., “Efficient Memory Management for Large Language\nModel Serving with PagedAttention,” in Proceedings of the 29th\nSymposium on Operating Systems Principles, ser. SOSP ’23, Asso-\nciation for Computing Machinery, New York, NY, USA, Oct. 2023,\npp. 611–626.\n[35] D. Guo et al., “Deepseek-R1 Incentivizes Reasoning in LLMs Through\nReinforcement Learning,” Nature, vol. 645, no. 8081, pp. 633–638,\nSep. 2025.\n[36] A. Li, Z. Yuan, Y. Zhang, S. Liu, and Y. Wang, Know When to\nExplore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement\nLearning, Aug. 2025.\n[37] P. Xia et al., Agent0: Unleashing Self-Evolving Agents from Zero Data\nvia Tool-Integrated Reasoning, Nov. 2025.\n[38] A. Singhvi et al., Dspy Assertions: Computational Constraints for Self-\nRefining Language Model Pipelines, Dec. 2023.\n[39] O. Khattab et al., “Dspy: Compiling Declarative Language Model\nCalls into State-of-the-Art Pipelines,” in The Twelfth International\nConference on Learning Representations, 2024.\n"}]}