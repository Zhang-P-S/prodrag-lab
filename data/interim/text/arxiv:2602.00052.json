{"doc_id": "arxiv:2602.00052", "source": "arxiv", "lang": "en", "pdf_path": "data/raw/arxiv/pdf/2602.00052.pdf", "meta": {"doc_id": "arxiv:2602.00052", "source": "arxiv", "arxiv_id": "2602.00052", "title": "AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows", "authors": ["Ramtin Babaeipour", "François Charest", "Madison Wright"], "published": "2026-01-19T18:38:36Z", "updated": "2026-01-19T18:38:36Z", "summary": "Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.", "query": "(cat:cs.CL OR cat:cs.LG) AND (all:\"large language model\" OR all:LLM) AND (all:medical OR all:clinical OR all:healthcare)", "url_abs": "http://arxiv.org/abs/2602.00052v1", "url_pdf": "https://arxiv.org/pdf/2602.00052.pdf", "meta_path": "data/raw/arxiv/meta/2602.00052.json", "sha256": "f2652cdb49f71e8cd795ac67feb3f5ac5c8d4fdc5692ca0d14e939464be7e3c8", "status": "ok", "fetched_at": "2026-02-18T02:21:07.559833+00:00"}, "pages": [{"page": 1, "text": "AI-assisted Protocol Information Extraction For\nImproved Accuracy and Efficiency in Clinical\nTrial Workflows\nRamtin Babaeipoura, François Charesta, Madison Wrighta\naBanting Health AI, bantinghealth.ai\n357 Bay St., Toronto, ON, M5H 4A6, Canada\nFebruary 3, 2026\nAbstract\nIncreasing clinical trial protocol complexity, amendments, and chal-\nlenges around knowledge management create significant burden for trial\nteams. Structuring protocol content into standard formats has the poten-\ntial to improve efficiency, support documentation quality, and strengthen\ncompliance. We evaluate an Artificial Intelligence (AI) system using gener-\native LLMs with Retrieval-Augmented Generation (RAG) for automated\nclinical trial protocol information extraction. We compare the extraction\naccuracy of our clinical-trial-specific RAG process against that of publicly\navailable (standalone) LLMs. We also assess the operational impact of\nAI-assistance on simulated extraction CRC workflows.\nOur RAG process was measured as more accurate (87.8%) than stan-\ndalone LLMs with fine-tuned prompts (62.6%) against expert-supported\nreference annotations. In the simulated extraction workflows, AI-assisted\ntasks were completed ≥40% faster, rated as less cognitively demanding\nand strongly preferred by users. While expert oversight remains essential,\nthis suggests that AI-assisted extraction can enable protocol intelligence\nat scale, motivating the integration of similar methodologies into real-\nworld clinical workflows to further validate its impact on feasibility, study\nstart-up, and post-activation monitoring.\nKeywords: Clinical trials, CRC workflows, Protocols, Information extraction,\nSchedule of Events, RAG, LLM\n1\nIntroduction and related work\nWhen properly planned and executed, clinical trials are known to be the best\nexperimental method to evaluate the effectiveness and safety of a medical in-\ntervention. A clinical trial protocol constitutes a written agreement between\n1\narXiv:2602.00052v1  [cs.IR]  19 Jan 2026\n"}, {"page": 2, "text": "investigators, research teams, participants, and the scientific community that\nassists communication by providing the trial’s background, objectives and details\nabout its design and organization [1]. It therefore contains foundational infor-\nmation that teams must extract and interpret in order to ensure a consistent\nand compliant execution. However, as protocol complexity increases [2, 3, 4],\ncompleteness and adherence to quality guidelines of protocol documents varies\n[5] and time-consuming, avoidable amendments became more frequent [6]. In\nthis context, structured data extraction from protocol document and referencing\ncan be time-intensive and prone to inconsistencies [7, 8] even though it has the\npotential to improve downstream efficiency, support documentation quality, and\nstrengthen compliance [9, 10, 11]\nAutomation of routine structuring and integration of protocol content can free\nresearch teams from low-leverage administrative tasks, redirecting focus towards\npatient-centric care and scientific decision-making. Ultimately, efficiency and\nquality gains would reduce delays, accelerate evidence generation and improve\npatient outcomes by expanding access to high-quality, compliant research.\nRecent advances in AI offer promising solutions. LLMs are AI systems trained\non massive textual datasets, capable of distilling complex, unstructured informa-\ntion into key data elements and summaries. Beyond base information processing,\nthey provide automated reasoning and content generation capabilities. While\nthe latter have the potential to accelerate protocol document standardization\nthrough automated document authoring [12], their more immediate application\nlies in extracting structured information from existing unstructured protocol\ndocuments and providing initial automated analyses. This first paper focuses on\ninformation extraction, while the automated analysis is addressed in [13].\nThe current literature shows LLMs being used to extract from study docu-\nments certain study design features such as eligibility criteria [7, 14] and study\nschedule of events [8, 15]. Recent examples feature the extraction of more general\nsemi-structured information from unstructured oncology medical records [16].\nThis paper presents a more comprehensive extraction approach.\nWhen extracting information from unstructured documents, a direct ap-\nproach involves prompting a LLM with detailed instructions and providing entire\ndocuments as part of its context (standalone LLM). While recent LLMs allow for\nvery large context windows, this approach has theoretical limitations. Among\nthem:\n• Context window limits: These LLMs still can only work with limited\ndocument lengths, and protocol documents may exceed their input token\nlimits [17],\n• Context window spread: they may not consistently identify and ex-\ntract all relevant information, especially when details are scattered across\ndifferent sections of lengthy protocols [18],\n• Query number tradeoff: on one hand, using a small number of prompts\nto extract hundreds of independent, individual data elements may lead to\n2\n"}, {"page": 3, "text": "suboptimal performance, while on the other hand, using many prompts\nwith such large contexts increases cost and time to completion [19],\n• Lack of element-specific context and referencing: each data element\nextracted may require specialized context and prompting, knowledge of\nclinical research terminology and output requirements [20]. Practitioners\nmay also need to reference specific protocol sections for auditability and\ntraceability, which standalone LLMs may not provide in a natural way.\nRetrieval-Augmented Generation (RAG) [19] addresses these limitations by\ncombining the general knowledge encapsulated in LLMs with element-specific\ninformation retrieval queries, context and generation prompts. Our clinical-trial-\nspecific RAG process works in three key steps, illustrated in Figure 1:\n1. Document processing: Protocol documents are subdivided into manage-\nable chunks of information, vectorized using a distinct embedding model,\nand stored in a searchable database preserving context and relationships\nbetween sections.\n2. Custom retrieval queries: When seeking specific information (e.g.\nprimary endpoints, inclusion criteria), the system uses tailored queries to\nidentify and retrieve the most relevant chunks from the protocol document.\nReferencing those chunks also allows for direct auditability and traceability.\n3. Structured information generation: A generation LLM is presented\nwith the retrieved information as context along with custom instructions\n(prompts) designed specifically for clinical research data extraction, and\nrequired to structure the information according to the required formats\n(json schemas).\nThis approach allows the system to work with protocol documents of any\nlength, focusing on relevant sections for each element-specific query, to leverage\nclinical research expertise embedded in the retrieval and prompting strategies, and\nto provide transparent referencing for human approval and process auditability\nsupporting regulatory electronic records compliance. Moreover, since data privacy\nand security are paramount in clinical research, our RAG system is designed to\noperate within secure, compliant environments, ensuring that all information\ninvolved remains private throughout data processing (e.g. it cannot be used by\nthird-parties for LLM training).\nSince the Schedule of Events (SoE) defines study timing and procedures\non which numerous downstream processes depend, it is crucial for operational\nexecution [21]. Clinical trial protocols very frequently use table formatting to\nrepresent information, most notably the SoE. Furthermore, SoE often involves\nmulti-page spans, intricate cell merging, and hierarchical visit structures encoded\nthrough visual layout.\nCurrent PDF extraction methods often struggle with those particular chal-\nlenges, from markup conversion [22] (cumbersome and labor-intensive) and\nimage-based recognition [23] (dominant approach, but heavily dependent on\n3\n"}, {"page": 4, "text": "Figure 1: RAG Process for Clinical Protocol Information Extraction.\nThe\nRAG system first processes protocol Portable Document Format (PDF) files\nsubdividing them into meaningful chunks and storing them in a vector database\nwith semantic embeddings. When users query for specific protocol information\n(e.g., inclusion/exclusion criteria), the system retrieves the most relevant chunks\nand provides them as context to a LLM along with specialized instructions.\nThe LLM then generates a (semi-)structured, standardized output that research\nteams can readily use for protocol review and study operations.\ntrained models) to text and metadata direct extraction (hard to generalize).\nBecause SoE formatting varies wildly across protocols, traditional rule-based or\nmetadata-dependent approaches are ruled out, failing to reliably capture the\nhierarchical relationships. Instead, we address SoE extraction through a special-\nized two-stage approach involving table detection on protocol pages followed by\nvision-based multimodal generation on those pages for information extraction.\nThis approach seems sufficient to achieve similar performance levels on SoE than\non other information categories.\nAnother challenge requiring specific considerations is the quality evaluation\nof semi-structured data outputs. Recent work has demonstrated the effectiveness\nof using LLMs as evaluators, often termed \"LLM-as-a-judge\", for assessing\nthe quality of AI-generated content. A comprehensive survey shows that in\nmany settings LLM-based evaluation correlates well with human judgments\n[24]. In healthcare specifically, it was demonstrated that gpt-4o can effectively\nautomate the evaluation of AI-generated clinical text, achieving strong agreement\nwith expert clinicians while significantly reducing evaluation time and cost [25].\nThis approach is particularly valuable in clinical research where human expert\nevaluation is resource-intensive, yet maintaining quality standards is critical.\nBuilding on this foundation, we employed an LLM-based evaluation framework\nto assess the accuracy and completeness of protocol abstractions in our study.\nIn addition to evaluation being subject to variations in output phrasing,\nthere is also a subjective nature to what consititutes the ground truth, as\n4\n"}, {"page": 5, "text": "experts sometimes differ on what constitutes necessary and sufficient extracted\ninformation. Moreover, establishing a complete ground truth for hundreds of data\nelements coming from dozens of protocol documents spanning over sometimes\nmore than 100 pages is especially challenging, as even expert reviewers are prone\nto incomplete data capture given the document density and breadth of the task.\nThis means that a small group of expert annotators cannot realistically guarantee\na truly exhaustive ground truth. While increasing the number of human reviewers\ncould improve completeness, such a redundant process is prohibitively labor-\nintensive and fundamentally unscalable.\nIn many recent comparable works\n([26], [27], [28]), researchers introduce LLMs in their annotation process, both\non generating final adjudication candidates and on adjudicating those outputs.\nSimilarly, we designed a LLM-assisted annotation process that presents a hybrid\nhuman-AI collaboration to an independent LLM-based adjudication layer whose\noutput is human-reviewed on low-confidence cases and quality-controlled on a\nrandomly sampled subset.\nIn summary, this work implements and evaluates a novel clinical-trial-specific\nRAG system with a SoE-specific methodology for extensive automated protocol\ninformation extraction. It combines domain-specific RAG for text-based repre-\nsentations with specialized vision-based methods for tabular SoE data, enabling\ncomprehensive extraction across diverse protocol structures. It is addressing the\nfoundational limitations of both standalone LLMs and general-purpose extraction\napproaches as well as extending the reach and scale of extraction procedures\nfound in the current literature.\nWe empirically compare our RAG approach against standalone LLMs across\n23 publicly available protocols spanning multiple therapeutic areas, demonstrat-\ning improved accuracy particularly for complex, scattered information. Through\na controlled experiment with 13 CRCs, we assess information extraction accuracy\nas well as real-world operational impact, measuring time savings, cognitive load,\nand user preferences. In order to achieve a robust evaluation methodology, we\ndeveloped an LLM-assisted evaluation and annotation adjudication framework\nthat enables scalable, consistent assessment across hundreds of semi-structured\ndata fields.\n2\nMethods\n2.1\nProtocol documents selection\nWe start from all the studies listed in clinicaltrials.gov [29] at the end of March\n2025 having an identifiable protocol document (using the \"Study Documents\"\nfield).\nUsing a python script, we then filter them using the logic shown in Listing\n1, to select interventional drug studies with treatment as primary purpose,\nconducted in Canada or the United States, and extract their therapeutic area\n(see Listing 1).\nThen, we classify them into three therapeutic areas (oncology, cardiovascu-\n5\n"}, {"page": 6, "text": "Table 1: Statement of significance\nProblem or Issue\nClinical trial protocols are long, heterogeneous PDFs;\nextracting accurate, auditable, structured operational\ndata (e.g., endpoints, interventions, SoE) is time-\nconsuming, inconsistent, and difficult to scale.\nWhat\nis\nAlready\nKnown\nLLMs were used to extract eligibility and schedules,\nRAG improves precision by retrieving relevant context\nand vision enhanced LLMs can interpret tables.\nWhat this Paper\nAdds\nA clinical-trial-specific RAG pipeline plus a SoE ta-\nble extraction approach that produces traceable, semi-\nstructured outputs, empirical comparison vs standalone\nLLMs, a CRC controlled experiment showing faster,\nmore accurate extraction, and a LLM-based evaluation\nframework for scalable, consistent performance assess-\nment.\nWho would benefit\nSite feasibility, start-up, operations and monitoring,\nsite IT (CTMS/EDC, downstream automation), CROs\nand sponsors.\nListing 1: Filtering logic for studies.\nstudies_filtered = studies[\n(studies[\"Interventions\"].str.startswith(\"intervention_type:Drug\"))\n& (studies[\"Study_Design_Primary_Purpose\"] == \"Treatment\")\n& (studies[\"Location_Countries\"].str.contains(\"Canada|United\nStates\"))\n]\nlar, and others) based on the \"Primary_Disease_or_Condition\" field, using\nkeyword matching (for the lack of a more reliable approach). We (uniformly)\nrandomly sample studies, 9 from oncology, 7 from cardiovascular, and 7 from\nother therapeutic areas. For those studies, a human data annotation expert\nmanually created a semi-structured dataset from the data elements described in\nsection 2.2.\n2.2\nExtracted semi-structured data models\nIn order to structure and evaluate the information extraction accuracy of different\nmethodologies, we define a set of semi-structured data models representing key\ndata elements to be extracted from clinical trial protocols. Those data models\nare designed to capture essential protocol information in a standardized, json-\nrepresentable format that simplifies data handling, comparisons and evaluation.\n6\n"}, {"page": 7, "text": "We classify the information to be extracted into six broad categories: general\ninformation, inclusion/exclusion criteria, adverse event definitions, intervention,\nsite requirements and schedule of events. Each category is subdivided into smaller\ndata elements representing the relevant (semi-)structured output to be extracted\nby the RAG invocations, and also serve as a basis for performance evaluation.\nThe list of those data elements is provided in the following subsections and Table\n7.\n2.2.1\nGeneral information\nElements from this category include general study information (e.g.\nNCT\nID, protocol version, title, sponsor, phase, therapeutic area, disease/condition,\nallocation, masking, estimated duration) as well as primary and secondary\nobjectives and endpoints (see Listing 2 for an example).\nListing 2: Example json output for general information and endpoints.\n{\n\"general_information\": {\n\"study_nct_id\": \"NCT03360071\",\n\"protocol_id\": \"PMA 001\",\n\"protocol_version\": \"6.9.1\",\n\"protocol_date\": \"2018-08-07\",\n\"study_title\": \"A double-blind, prospective, parallel group\nevaluation of a novel biologic therapy for perennial allergic\nrhinitis\",\n\"sponsor_name\": \"Relez Therapeutics, LLC\",\n\"phase\": \"Phase 2\",\n\"therapeutic_area\": \"allergology and immunology\",\n\"disease_or_condition\": \"perennial allergic rhinitis\",\n\"allocation\": \"Randomized\",\n\"masking\": \"Double Masking\",\n\"number_of_sites\": 1,\n\"estimated_duration\": \"14 weeks\"\n},\n\"objectives_and_endpoints\": {\n\"primary_objectives\": [\"To evaluate whether there is a difference in\nthe signs and symptoms of moderate to severe allergic rhinitis when\ntreated with a proprietary mixed preparation of FDA approved\nallergens [...]\"],\n\"secondary_objectives\": [\n\"To quantify allergy medication use and adverse events.\",\n\"Serum will also be stored to evaluate changes in IgE and IgG\nsubclasses.\",\n...\n],\n\"primary_endpoints\": [\n\"Quantitative comparison of clinical response of active PMA\nimmunotherapy compared to placebo\",\n\"Total combined symptom and medication score (DCS)\",\n7\n"}, {"page": 8, "text": "\"Total Nasal Symptoms Score (TNSS)\"\n],\n\"secondary_endpoints\": [\n...,\n],\n\"measurement_method\": [\"Validated instruments that assess study\nsubject clinical response, physician global assessment, and\nmedication use.\"],\n\"endpoint_timepoint\": \"8 weeks\"\n}\n}\n2.2.2\nInterventions\nElements from this category include trial arms (name, description) and interven-\ntion details (type, name, dosage, schedule) as well as treatment-level information\n(product name, dose, administration, restrictions, modifications) (see Listing 3\nfor an example).\nListing 3: Example json output for intervention/treatment information.\n{\n\"arms\": [\n{\n\"arm_name\": \"Active PMA\",\n\"interventions\": [{\"type\": \"Immunotherapy\", \"name\":\n\"Proprietary Mixture of FDA Approved Allergens (PMA)\", \"dosage\":\n\"0.15-0.60 ml\", \"schedule\": \"Twice-weekly clinic visits for 8\nweeks\"}]\n},\n{\n\"arm_name\": \"Placebo\",\n\"interventions\": [{\"type\": \"Placebo\", \"name\": \"Saline\nPlacebo\", \"dosage\": \"0.15-0.60 ml\", \"schedule\": \"Twice-weekly\nclinic visits for 8 weeks\"}]\n}\n],\n\"treatment_level_information\": [{\n\"product_name\": \"PMA (Proprietary Antigen Preparation)\",\n\"dose\": \"The dosage (dilution) of PMA will be the same for all\nsubjects and will begin with vial number #5 [...].\",\n\"administration\": \"Sub-cutaneous administration using [...].\",\n\"restrictions\": \"All patients are required to remain in the\nclinic under close observation by the study nurse for a period of\n30 minutes following their injection [...].\",\n\"modifications\": \"Subjects that develop local swelling greater\nthan 50mm will have their next dose decreased by one step [...].\"\n}]\n}\n8\n"}, {"page": 9, "text": "2.2.3\nSchedule of events\nElements from this category include visit number, visit time, and procedures to\nbe performed at each visit (see Listing 4 for an example).\nListing 4: Example json output for schedule of events.\n{\n\"schedule_of_events\": [\n{\n\"visit_number\": 1, \"visit_time\": \"Week 1\", \"procedures\": [\n{\"procedure_name\": \"Discussion and Recruitment\"},\n{\"procedure_name\": \"Informed consent\"},\n...,\n{\"procedure_name\": \"Blood Draw\"}\n]\n},\n{\n\"visit_number\": 2, \"visit_time\": \"Week 3 to 11\",\n\"procedures\": [\n{\"procedure_name\": \"Administer immunotherapy/safety\nf/u\"},\n...,\n{\"procedure_name\": \"AE assessment\"}\n]\n},\n{\n\"visit_number\": 18, \"visit_time\": \"Week 14\", \"procedures\": [\n{\"procedure_name\": \"Blood Draw\"},\n...,\n{\"procedure_name\": \"AE assessment\"}\n]\n}\n]\n}\n2.2.4\nInclusion/exclusion criteria\nElements from this category include inclusion criteria and exclusion criteria (see\nListing 5 for an example).\nListing 5: Example json output for inclusion/exclusion criteria.\n{\n\"inclusion\": [\n\"The study subject is currently experiencing active symptoms and\nsigns of moderate to severe PAR with or without SAR component\ndefined as score of at least 28 on the RQLQ and 6 on the TNSS\nscale. Scores will be determined on the first and second visit\n(after the 14 day medication washout period). Both scores exceeding\ntheir threshold on either day will qualify.\",\n9\n"}, {"page": 10, "text": "\"Skin test strongly positive (wheal at least 5mm diameter and\n2mm greater than negative control) to at least 6 of 48 allergy skin\nprick tests including at least one species of dust mite.\",\n\"The subject is a male or a non-pregnant, non-lactating female\nbetween the ages of 18 and 65.\"\n],\n\"exclusion\": [\n\"The subject has received any investigational compound within 30\ndays prior to screening.\",\n...,\n\"Subject has a history of cancer, other than squamous cell or\nbasal cell carcinoma of the skin that has not been in full\nremission for at least 5 years prior to Screening. (A history of\ntreated CIN I, II, or CIN III [cervical intraepithelial neoplasia]\nis allowed.)\"\n]\n}\n2.2.5\nAdverse event definitions\nElements from this category include adverse event (AE) and serious adverse\nevent (SAE) definitions, severity grading, relationship to study treatment, re-\nporting requirements (timeframes, data collection, contacts), safety monitoring\nand management (plan, discontinuation criteria, emergency procedures), and\nspecific AE information (expected AEs, potential risks, concomitant medication\nrestrictions, special population considerations) (see Listing 6 for an example).\nListing 6: Example json output for adverse event definitions.\n{\n\"ae_definitions_and_classifications\": {\n\"ae_definition\": \"An AE is defined as any untoward medical\noccurrence in a clinical investigation subject administered a drug;\n[...]\",\n\"sae_definition\": \"An SAE is defined as any untoward medical\noccurrence that at any dose results in death, is life-threatening,\nrequires inpatient hospitalization or prolongation of existing\nhospitalization, [...].\",\n\"severity_grading\": \"Mild: The event is transient and easily\ntolerated by the subject. Moderate: [...]\",\n\"ae_relationship\": \"Related: An AE that follows a reasonable\ntemporal sequence from administration of the immunotherapy. Not\nRelated: [...].\"\n},\n\"reporting_requirements\": {\n\"reporting_timeframes\": \"A sponsor SAE form must be completed and\nsigned by the investigator immediately or within 24 hours of first\nonset or notification of the event.\",\n\"ae_data_collection_requirements\": \"Event term, start and stop\ndate, severity, [...].\",\n10\n"}, {"page": 11, "text": "\"reporting_contacts\": \"***\"\n},\n\"safety_monitoring_and_management\": {\n\"safety_monitoring_plan\": \"All subjects experiencing AEs must be\nmonitored until the symptoms subside [...].\",\n\"discontinuation_criteria\": \"The subject has experienced a PTE or\nAE that requires early termination because [...].\",\n\"emergency_procedures\": \"\"\n},\n\"specific_ae_information\": {\n\"expected_aes\": \"Typical but uncommon side effects have included\nmild-moderate local swelling.\",\n\"potential_risks\": \"The most common side effect from allergen\nimmunotherapy [...] There is a very small chance of having a\nserious allergic reaction.\",\n\"concomitant_medication_restrictions\": \"Immunotherapy that is not\npart of the investigation, [...].\",\n\"special_population_considerations\": \"If female, the subject is\npregnant or lactating or intending to become pregnant before [...].\"\n}\n}\n2.2.6\nSite requirements\nElements from this category include site equipment, certifications, sample han-\ndling requirements as well as investigational product (IP) storage conditions (see\nListing 7 for an example).\nListing 7: Example json output for site requirements.\n{\n\"site_requirements\": [{\n\"equipment\": [\"refrigerator\", \"sterile syringe\", \"27G 1/2 inch\nsyringe\", \"temperature log\"],\n\"certifications\": [\"GCP certification\"],\n\"sample_handling\": {\n\"sample_description\": \"Blood samples collected for\nimmunoglobulin level measurement, including specific IgE, IgG4, and\npossibly IgG3.\",\n\"processing_timeline\": \"Blood will be drawn at the start and\ncompletion of the study. Serum collected at the screening visit\nwill be stored for later analysis. Blood will be redrawn at the\nfinal visit (week 14).\",\n\"storage_conditions\": \"refrigerated at 4 degrees Celsius\",\n\"transport_requirements\": \"All samples will be collected in\naccordance with acceptable laboratory procedures.\",\n\"access_control\": \"The investigator or designee is\nresponsible for transcribing or attaching laboratory results to the\nCRF. The investigator will maintain a copy of the laboratory\naccreditation and the reference ranges for the laboratory used.\",\n11\n"}, {"page": 12, "text": "\"chain_of_custody\": \"Maintain current records of the\nreceipt, administration, and disposition of sponsor-supplied\nimmunotherapy, and return all unused sponsor-supplied study\nsolutions to the sponsor.\"\n}\n}],\n\"ip_storage\": [{\n\"product_name\": \"Investigational Immunotherapy Solutions and\nPlacebo Saline Solutions\",\n\"temperature_requirements\": \"Refrigerated at 4 degrees Celsius\",\n\"environmental_conditions\": \"Stored in an appropriate,\nlimited-access, secure place\",\n\"storage_location\": \"Original container until dispensed\",\n\"access_restrictions\": \"Limited-access, secure place\",\n\"accountability_tracking\": \"A daily temperature log of the drug\nstorage area must be maintained every working day\"\n}]\n}\n2.3\nData extraction approaches\nWe use following three data extraction approaches:\n2.3.1\nStandalone LLM extraction\nWe use a standalone LLM approach as a baseline for comparison. We use a\nlarge generation LLM (gpt-4o [30]), with similarly engineered, but larger, less\nspecific, prompts encompassing the elements from the categories from section\n2.2. Therefore, for each data element category, we have a single prompt that\nasks the LLM to extract all elements from that category, and the LLM API is\nprovided with the entire protocol document text in its context.\nNote that ignoring scalability, security and compliance considerations, this\nbenchmark approach is reminiscent of a research team member uploading the\nstudy protocol to the equivalent publicly available general-purpose AI-assistance\nservice, and submitting carefully engineered prompts for the six data categories.\n2.3.2\nRAG extraction\nWe use the RAG approach mentioned in section 1, featuring a more specialized\nretrieval and prompting strategy. Therefore, each data element or small group\nof closely related data elements (e.g. ’protocol id’, ’protocol date’ and ’protocol\nversion’) is the target of a dedicated RAG chain call, and the LLM API is then\nprovided with only the relevant chunks retrieved from the protocol document as\nthe context alongside a element-tailored prompt.\nMoreover, in this setup,\n• we use a vector embedding model (Cohere English v3 Embeddings [31]) to\nvectorize the protocol document chunks,\n12\n"}, {"page": 13, "text": "• we employ methods of context aware chunking and chunk reunion based\non section headers in addition to document ordering in the retrieval steps\nas decribed below,\n• we use, among others, gpt-4o and Claude generation LLMs,\n• we source standard methods and orchestrate our RAG process through\nthe LangChain framework [32] for improved abstraction, observability and\nportability.\nThe results in Section 3.1 reflect a recent RAG configuration using Claude\nSonnet 4.5 as the generation model, while the controlled experiment (Section 2.6)\nused an earlier configuration with gpt-4o mini that was available at the time of\ndata collection.\nRecent research in RAG has demonstrated that chunking strategy significantly\nimpacts retrieval quality and generation accuracy [33]. For clinical trial protocols\nthat have hierarchical organization and cross-referenced sections, naive chunking\napproaches often break apart related information or mix unrelated content\ntogether. The main challenge is maintaining a balance between providing enough\ncontext and ensuring relevance, as large chunks include unrelated information\nwhile small ones lose essential context [34].\nTo address these challenges, we implemented three complementary techniques.\nFirst, context-aware chunking identifies and preserves hierarchical document\nstructure (section headers, subsections, numbered lists), ensuring chunks retain\nsemantic coherence and metadata about its position in a hierarchy. Second, with\nautomatic chunk reunion, split portions of oversized sections are programmatically\nreunited during retrieval.\nThird, document ordering sorts results by their\noriginal sequence, maintaining the logical flow and cross-referential integrity.\nThese methods combined ensure that the LLM receives contextually complete,\nstructurally coherent, and logically ordered information for each query.\n2.3.3\nSchedule of Events table detection and extraction\nSoE information presents unique extraction challenges due to its typical presenta-\ntion in complex tabular formats with intricate spatial relationships between visits,\nprocedures, and timepoints. We address this through a specialized two-stage\napproach: table detection (transformer-based) followed by multimodal LLM\ninformation extraction (native vision capability). This decomposition is deemed\nnecessary because clinical protocol tables often span multiple pages, exhibit di-\nverse layouts across protocols, and embed critical scheduling information within\ndense visual structures that resist purely text-based parsing methods.\nFor table detection, we employ the Microsoft Table Transformer [35], an\nobject detection model trained on tables from scientific articles. Multimodal\nLLMs have demonstrated effective visual cognition capabilities [36]. We then\nsubmit the identified table pages (converted to images) to a multimodal LLM\nwith SoE-specific prompting instructions, attempting to maintain context across\nmulti-page tables and tie procedures to visits. In our final configuration (RAG\n13\n"}, {"page": 14, "text": "1), we use Claude Opus 4.5 for this task, while our earlier configuration (RAG 2)\nused gpt-4o. This multimodal approach proved to be effective for SoE extraction,\nmostly preserving the spatial layout and visual cues that encode hierarchical\nrelationships in table structures while it was regularly lost or inaccurate in\ntext-only representations.\n2.4\nEvaluation\nSince comparing hundreds of semi-structured fields from each protocol to their\nground truths is resource-intensive, non-scalable, error-prone and involves hard-\nto-manage variability across humans, we adopt scripted procedures leveraging\nLLMs (gpt-4o) to compare the extracted semi-structured content to its ground\ntruth.\nEvaluations are done at the same level of granularity as the RAG chain calls,\nso for each data extraction RAG chain call, the output json object is evaluated\nindependently of others. Therefore, the majority of the jsons evaluated contain\nonly a few data elements, if not just one. This allows for more robust and\nexplainable evaluation and enables comparisons on constrained data elements.\nThe prompting used for evaluation of those components is general, focused\non judging the completeness and presicision of the data instead of the style or\nstructure. For instance, we instruct the LLM-as-judge to not be sensitive to the\nuse of abbreviations, to consider \"age >= 60\" as equivalent to \"Age 60 or older\",\n\"Merck & Co., Inc.\" as equivalent to \"Merck\", \"10.000\" as equivalent to \"10\",\n\"10,000\" as equivalent to \"10000\", etc. Moreover, we instruct the LLM-as-judge\nto not penalize for extra information that does not contradict the ground truth,\nmisrepresent information or introduce safety risks. This is intended to account\nfor different levels of verbosity and detail in the extracted information across\nhumans and LLMs, but ensuring that potential LLM hallucinations are penalized.\nA score from 0 to 5 (values 0, 1, 2, 3, 3.5, 4, 4.5 and 5) is assigned based on\nhow closely the submitted json reflects the entire information from the ground\ntruth json, with 5 meaning that the submitted json recovers 100% of the ground\ntruth, 4.5 meaning that it recovers at least 90% of the ground truth json to a\nhuman reader, ..., and 0 meaning that it recovers less than 20% of the ground\ntruth.\nIn order to obtain a score for a given data element category (e.g. general\ninformation, interventions, etc.), predefined weights are used in a weighted\naverage of the scores of all the individual jsons evaluated for that category.\nThis allows us to compare the performance of different extraction approaches\non each category while ensuring that, for example, primary endpoints and\nobjectives are weighted higher than their secondary counterparts (e.g. in the\ngeneral information category, primary_objectives have a weight of 0.25 while\nsecondary_objectives have a weight of 0.02). See Table 7 in Section A for the\ncomplete list.\nWe manually created an evaluation benchmark dataset of 35 scored pairs of\njson elements. This benchmark dataset was used to refine the base evaluation\nprompt and ensure that the LLM-as-judge scores are aligned with human expert\n14\n"}, {"page": 15, "text": "judgments. The final evaluation prompt was measured to have mean absolute\nerror under 0.07 while showing no significant bias.\nThis evaluation methodology is used to compare both the standalone LLM\nto the RAG approach and the AI-assisted tasks to the manual tasks from section\n2.6.\nListing 8: Example comparison of adverse event definition information.\n# Ground truth:\n{\"ae_definition\": \"An AE is defined as any untoward medical\noccurrence in a clinical investigation subject administered a drug;\nit does not necessarily have to have a causal relationship with\nthis treatment.\"}\n# AI output:\n{\"ae_definition\": \"An AE is defined as any untoward medical\noccurrence in a clinical investigation subject administered a drug;\nit does not necessarily have to have a causal relationship with\nthis treatment. An AE can therefore be any unfavorable and\nunintended sign (e.g., a clinically significant abnormal laboratory\nfinding), symptom, or disease temporally associated with the use of\na drug whether or not it is considered related to the drug.\"}\n# Evaluation output:\n{\"score\": 5,\n\"rationale\": \"The submitted json recovers 100% of the ground truth\njson to a human reader. The definition of an AE in the submitted\njson includes the exact wording of the ground truth json and adds\nadditional information that is consistent with the ground truth.\nThe extra information provided does not contradict the ground truth\nand is relevant to the context of defining an AE. Therefore, the\nmeaning is preserved, and the additional details enhance\nunderstanding without introducing any discrepancies.\"}\nListing 9: Example comparison of disease or condition information.\n#Ground truth:\n{\"disease_or_condition\": \"perennial allergic rhinitis\"}\n# AI output:\n{\"disease_or_condition\": \"allergic rhinitis\"}\n# Evaluation output:\n{\"score\": 4,\n\"rationale\": \"The submitted json specifies \"allergic rhinitis\" while\nthe ground truth specifies \"perennial allergic rhinitis\". The term\n\"perennial\" indicates that the allergic rhinitis is year-round, as\nopposed to seasonal. While \"allergic rhinitis\" is a broader\ncategory that includes both perennial and seasonal types, the\nomission of \"perennial\" means that the submitted json does not\n15\n"}, {"page": 16, "text": "fully capture the specific condition described in the ground truth.\nHowever, the core condition of \"allergic rhinitis\" is still\npresent, which means the submitted json recovers a significant\nportion of the ground truth information, but not completely.\nTherefore, it scores a 4, as it recovers at least 80% of the ground\ntruth information.\"}\nListing 10: Example comparison of country information.\n# ground truth:\n{\"countries\": [\n{\"country_name\": \"USA\", \"number_of_sites\": 1}\n],\n\"number_of_sites\": 1\n}\n# AI output:\n{\"countries\": [],\n\"number_of_sites\": 1}\n# Evaluation output:\n{\"score\": 2,\n\"rationale\": \"The submitted json captures the ’number_of_sites’\ncorrectly, which matches the ground truth. However, it fails to\ninclude the ’countries’ information, which is a significant part of\nthe ground truth json. The ground truth specifies that there is one\nsite in the USA, but the submitted json has an empty list for\n’countries’. This omission means that the submitted json recovers\nless than 40% of the ground truth information, as it lacks the\ngeographical context provided by the ’countries’ field. Therefore,\nthe score is 2.\"}\n2.5\nData annotation\nEstablishing a complete and accurate ground truth at the scale required for this\npaper represents a significant challenge. The density of medical, spread of related\ninformation across sections and subjectivity around which element components\nare essential or optional are such that even expert reviewers are prone to oversight\nand fatigue. Consequently, a small group of expert annotators cannot guarantee\na truly exhaustive ground truth while increasing their headcount, even if it could\nimprove completeness, is prohibitively very labor-intensive and fundamentally\nunscalable.\nTo achieve the scale and breadth required to allow for proper performance\nassessment, we draw upon recent methodologies that integrate LLMs into the\nannotation process to augment human capabilities. For example, recent work\ndemonstrated the efficacy of \"multi-LLM consensus\" combined with human\nreview to achieve scalable content annotation [26].\nSimilarly, human-LLM\ncollaboration, specifically where humans verify LLM-generated labels rather\n16\n"}, {"page": 17, "text": "than annotating from scratch, significantly improves efficiency without sacrificing\naccuracy [27]. Recent examples show an application where LLM labeling can\nmatch or exceed human agreement with “gold standard” ground truth [28]. These\nall support the idea that LLMs can be used within high-quality annotation and\nadjudication procedures, often being comparable to trained humans, especially\nwhen guided with optimized prompts and calibration.\nGuided by these findings, we designed a hybrid human-AI set of anno-\ntations that is submitted to an independent LLM-based adjudication layer.\nLow-confidence adjudications are sent to human experts for final review, as\nwell as a random sample of data elements used as quality control the LLM\nadjudication process itself. This hybrid approach aims to balance the scalability\nand speed of LLMs with the domain expertise and critical judgment of human\nreviewers, ultimately yielding a high-quality ground truth dataset.\n2.5.1\nAnnotation candidates\nFirst, we use a human-annotated full dataset from the 23 protocols (covering\nall data elements of all the data categories). We generate two more candidate\nannotations datasets leveraging different generative models, one from OpenAI\n(gpt-4o), the other one from Anthropic (Claude 3).\n2.5.2\nLLM-Based adjudication of annotation candidates\nWe employ a state-of-the-art LLM (gpt-5) to serve as the primary adjudication\nengine. This model acts as an independent evaluator, synthesizing information\nfrom:\n• the original protocol document (full PDF text provided),\n• the extraction instructions used by RAG chains for each data element and\n• the 3 candidate annotations, presented in a random order, without reference\nto their provenance.\nThe instructions provided to the LLM adjudicator emphasize generating\n• a final annotation in strict adherence to the information found in the\nprotocol and the predefined instructions and schema for each data element.\nThis final json output can be either of the submitted candidate annotations,\nor a new annotation synthesized by the LLM adjudicator.\n• a justification for each adjudication decision with clear rationales grounded\nin specific sections of the protocol text, and\n• a confidence score (0-100) reflecting the model’s certainty in its adjudication\ndecision.\n17\n"}, {"page": 18, "text": "2.5.3\nHuman-in-the-Loop validation\nTo ensure the reliability of this ground truth, we implement a twofold human\nexpert review strategy involving two independent expert adjudicators.\nUncertainty sampling is used to mitigate errors in complex cases, utilizing\nthe confidence score generated by the LLM as a filter for selection. We identify\nthe 40 data elements with the lowest confidence scores across the dataset. These\nflagged elements are split between the two human adjudicators for manual review.\nWe observed that human adjudication confirmed the LLM’s decision in 87% (N\n= 35) of these low-confidence cases, suggesting that the model remains robust,\naligned with its confidence scores, in areas of lower certainty.\nQuality control leveraging random sampling is used to establish a baseline\nfor the overall adjudication quality. A subset of 24 randomly selected data\nelements (exclusive of the low-confidence set) is reviewed by both of the two\nhuman adjudicators. In this random subset, human adjudication confirmed the\nLLM’s output in 100% (N = 24) of cases.\nBased on both these high concordance rates, we estimate that this hybrid\nannotation methodology yields a ground truth accuracy comparable to fully\nmanual double-blind annotation.\n2.6\nControlled Experiment\nThe main objective of this controlled experiment is to compare the effectiveness\nand accuracy of the AI-assisted approach to manual extraction by CRCs for one\ngiven RAG configuration.\n2.6.1\nExperiment design\nIn this experiment, data extraction tasks based on the specifications from section\n2.2 are randomly assigned to a group of CRCs to simulate a clinical trial site\nworkflow. A total of 13 CRCs with varying levels of experience (2 with < 1 year,\n9 with 1-3 years, 1 with 4-6 years of experience) were selected, consented and\nwere provided with (video and textual) instructions as well as examples of the\nexpected output formats for each data element.\nIn an online survey platform, after optional demographic/experience/interest\nquestions (gender, age group, experience as CRC, degree of comfort with new\nsoftware or technologies, enthusiasm for new software or technologies) and they\nwere presented with two tasks based on two different protocols chosen randomly\nfrom the 23 available (section 2.1). For each task, the CRC had to extract 4\nrandomly selected data element categories from the 6 described in section 2.2,\nand enter the values into provided spreadsheets where each data element value\nhad its own cell.\nIn one of the two tasks, the CRC was provided with the RAG process AI-\nextracted data pre-filled in the spreadsheet, and was asked to review, complete\nand correct it as needed (AI-assisted task). In the other task, the CRC had to\nfill in the spreadsheet manually from scratch (unassisted task). The order of\n18\n"}, {"page": 19, "text": "tasks (unassisted vs AI-assisted), as in the case of protocols and data element\ncategories, the order in which they were presented was randomized to avoid bias.\nIt is important to note that the AI-extracted data pre-filled in the spreadsheets\nwas generated using a RAG configuration having gpt-4o-mini as the generation\nmodel available at the time of the experiment, not the later Claude Sonnet\n4.5-based system that achieved the best evaluations in 3.1.\nAfter each task, CRCs self-reported time-to-completion and two ratings\nevaluating how mentally demanding the task was and how much effort they had\nto invest. After completing both tasks, they were asked 5 questions to assess\ntheir overall satisfaction and experience with the AI assistance (see Table 6).\n3\nResults\n3.1\nSpecialized RAG vs standalone LLM performance\nOverall, on information extraction into the semi-structured data models, in\nthe context of gpt-4o, our RAG process achieved an average accuracy of 69.6%\nacross the six categories from 2.2, comparing favourably to publicly available\nLLMs with fine-tuned prompting that obtained an average accuracy of 62.6%.\nHowever, later configurations involving Claude Sonnet 4.5 and Claude Opus\n4.5 and further optimizations (e.g. prompting, retrieval) achieved an average\naccuracy of 87.8%.\nTable 2: Category-wise RAG vs standalone LLM score comparison\nCategory\nRAG 1\nRAG 2\nStandalone\nN\nN\n(RAGs)\n(Standalone)\nAdverse Events\n92.3%\n78.0%\n59.5%\n23\n22\nGeneral Information\n83.5%\n76.1%\n78.6%\n23\n22\nInclusion/Exclusion\n84.7%\n77.6%\n90.6%\n23\n22\nInterventions\n92.8%\n68.6%\n53.1%\n23\n22\nSite Information\n81.2%\n46.2%\n38.6%\n23\n22\nSchedule of Events\n92.7%\n71.3%\n55.2%\n22\n21\nAverage (weighted)\n87.8%\n69.6%\n62.6%\n–\n–\nNotes: RAG 1: Claude Sonnet 4.5 for generation LLM and Claude Opus 4.5 for SoE,\nRAG 2: gpt-4o mini for generation LLM and gpt-4o for SoE (used in 2.6), Standalone:\ngpt-4o. N denotes the number of evaluated protocols per category.\nOf 23 selected protocols, 1 exceeded the standalone LLM’s context limit\nand was excluded from standalone analysis, while another lacked a clear sched-\nule of events and was excluded from that category. The results demonstrate\ndistinct performance patterns across categories. For general information and\ninclusion/exclusion, both approaches achieved relatively close performance. For\ninclusion/exclusion, we attribute the lower performance of RAG to the length of\n19\n"}, {"page": 20, "text": "the eligibililty criteria lists that would require a larger context window for its\nembedding model (limited to 512 tokens in the above configurations).\nHowever, RAG’s specialized and focused approach substantially surpassed\nstandalone LLM performance on more complex extraction tasks like adverse\nevents, site information and interventions. Since information for these categories\nis typically spread across different sections of the protocol, the specialized RAG\nsystem’s ability to retrieve and synthesize relevant content from multiple locations\nprovided a significant advantage. Moreover, our specialized SoE extraction\napproach demonstrated our approach’s advantage.\nAn important consideration when interpreting these category-specific results\nis that the subjectivity level of ground truth extractions varies across element\ntypes.\nCertain categories, such as inclusion and exclusion criteria, general\nprotocol information, and schedule of events have relatively low-variability\nrepresentations (e.g. an exact copy of a protocol excerpt). Other categories like\nadverse event information, intervention details, and site-specific information, can\nbe validly represented in multiple ways depending on the level of granularity,\norganization, and relevance chosen in the annotations. This phenomenon of\nground truth variability in specialized domains has been documented in semantics\ndata evaluation [37], notably in domains where expert-authored references cannot\nexhaustively capture all valid authorities and lines of reasoning [38]. In such\ncontext, evaluation metrics may underestimate extraction performance when\nresponses use valid but non-standard representations or do not capture all valid\nnuances of various relevance. Hence, some share of performance differences may\nbe explained by ground truth variability rather than solely extraction capability,\nespecially that the annotation process (2.5) is aimed at capturing a very high\nlevel of detail.\n3.1.1\nObservations and Examples\nAcross the various data element categories, several patterns emerged that high-\nlight strengths of the RAG approach over the standalone LLM method.\nContext confusion mitigation\nThe non-RAG approach frequently suffered\nfrom context confusion when processing lengthy protocols. In one protocol\n(NCT01742117), when extracting adverse event definitions, the non-RAG system\nincorrectly focused on bleeding event classifications:\nStandalone LLM output: “All bleed events that require some form\nof intervention (e.g. doctor’s visit, ER visit, hospitalization) will be\ncaptured, recorded, and classified as major, minor, or minimal.”\nIn contrast, the RAG system accurately retrieved and extracted the actual\nadverse event definition:\nRAG output: “An adverse event (AE) in the context of this study\nis defined as any problem or event which, in the opinion of the local\ninvestigator, was unanticipated, places subjects or others at a greater\n20\n"}, {"page": 21, "text": "risk of harm than was previously known or recognized, and was\npossibly related to the research procedures...”\nComprehensive Information Extraction\nThe RAG system’s targeted re-\ntrieval capability enabled more thorough extraction of complex, structured\ninformation. When identifying expected adverse events (in NCT02971956), the\nnon-RAG approach provided only generic statements:\nStandalone LLM output: “Adverse events (both non-serious and\nserious) associated with pembrolizumab exposure may represent an\nimmunologic etiology.”\nThe RAG system, however, retrieved and structured comprehensive lists of\nspecific expected events:\nStandalone LLM output: “[ \"Pneumonitis\", \"Colitis\", \"Endocrine\ndisorders (e.g., Adrenal Insufficiency, Hyperthyroidism, Hypophysitis,\nHypopituitarism, Hypothyroidism, Thyroid disorder, Thyroiditis)\",\n\"Hematologic disorders (e.g., Thrombotic Thrombocytopenic Pur-\npura, Autoimmune hemolytic anemia, Aplastic anemia...)\", ... ]“\nThe non-RAG system’s performance declined notably in protocols exceeding\ntypical token limits, where relevant information became diluted within extensive\nbackground content, where critical details are often scattered across multiple\nprotocol sections. The RAG system’s ability to retrieve targeted sections ad-\ndressing specific queries allowed it to maintain better performance regardless of\noverall document length.\n3.2\nControlled experiment results, AI-assisted vs manual\ndata extraction\nEach of the 13 CRCs was assigned two protocols: one with AI-assisted extraction\n(spreadsheet pre-filled with AI-generated content for review and correction) and\none with manual extraction (starting from an empty spreadsheet). For each\nprotocol, CRCs extracted four randomly selected data element categories from\nthe six described in Section 2.2.\nUnder the original design, this would yield 104 completed extraction tasks\n(13 CRCs × 2 protocols × 4 categories). However, the final dataset comprised\n97 tasks due to two factors: (1) four CRCs completed only three of the four\nassigned categories for one of their protocols (4 tasks), and (2) one protocol\nlacked a schedule of events table, necessitating the exclusion of schedule of events\nextraction tasks for the three CRCs assigned to that protocol (3 tasks), see Table\n3.\n3.2.1\nEfficiency of data extraction\nOverall, AI-assisted tasks were completed faster than manual tasks (see Figure 2).\nA one-sided paired (within subject, repeated measurement) t-test shows that\n21\n"}, {"page": 22, "text": "Table 3: Distribution of data extraction subtasks by type\nCategory\nAI-Assisted\nManual\nTotal\nAdverse Events\n10\n9\n19\nGeneral Information\n8\n11\n19\nInclusion/Exclusion\n7\n5\n12\nInterventions\n11\n10\n21\nSites\n7\n7\n14\nSchedule of Events\n7\n5\n12\nTotal\n50\n47\n97\nthe difference is statistically significant (p=0.008), see Table 4.\nTable 4: Comparison of task completion times between AI-assisted and manual\nprotocol abstraction tasks.\nCondition\nMedian Time (min)\nMean Time (min)\nAI-Assisted\n90.00\n120.67\nManual\n150.00\n167.67\nThis translates to an average time reduction of 47 minutes per protocol and\na median reduction of 60 minutes (40% improvement). The effect size indicates\na medium-to-large practical impact on workflow efficiency.\n3.2.2\nQuality of data extraction\nAI assistance significantly improved the accuracy and completeness of protocol\nabstractions.\nThe item-weighted average score was substantially higher for\nAI-assisted tasks compared to non-AI tasks (see table 5 for reporting average\nper item for two tasks, Welch’s independent groups t-test: t = 2.62, p < 0.05).\nThis represents a medium effect size (Hedges’ g = 0.53), indicating practical\nsignificance beyond statistical significance.\nWe emphasize that the above comparison involves a RAG system with gpt-4o\nmini as the generation model, not the most recent configuration from 3.1 which\ntends to produce more complete outputs. This, combined to the very high level\nof details from the ground truth obtained through 2.5, typically higher than\nhuman annotation on most categories, can explain the overall lower scores above\nrelative to the best configurations from section 3.1.\n3.2.3\nUser satisfaction and preferences\nFollowing completion of both the AI-assisted and manual extraction tasks, CRC\nparticipants were asked to complete a short survey assessing their perceptions\nof AI assistance.\nTwelve of the thirteen participants completed the survey.\n22\n"}, {"page": 23, "text": "Figure 2: Comparison of task completion times between non-AI and AI-assisted\nprotocol abstraction tasks. Box plot shows median completion time, interquartile\nrange, and outliers (circles) for both conditions. Non-AI tasks required substan-\ntially longer completion times compared to AI-assisted tasks.\nTable 5: Comparison of evaluation scores by item.\nAI Assisted\nUnassisted\nCategory\nScore\nScore (/100)\nCount\nScore\nScore (/100)\nCount\nae\n3.8155\n76.31\n10\n2.2751\n45.50\n9\ngen\n4.0062\n80.12\n8\n3.7100\n74.20\n11\nie\n4.7142\n94.28\n7\n4.8500\n97.00\n5\ninter\n3.3636\n67.27\n11\n2.7750\n55.50\n10\nsite\n2.5000\n50.00\n7\n2.4642\n49.28\n7\nsoe\n3.4285\n68.57\n7\n3.3000\n66.00\n5\nThe questions focused on perceived helpfulness, satisfaction relative to manual\nextraction, likelihood of future use, perceived cognitive effort, and overall task\ndifficulty. Table 6 summarizes the response distributions for each question.\nOverall, survey responses indicated a positive perception of AI assistance, with\nmost participants reporting it as helpful, preferable to manual extraction, and\neasier to use.\n23\n"}, {"page": 24, "text": "Figure 3: Distribution and comparison of item-weighted accuracy scores between\nAI-assisted and unassisted conditions. (A) Histogram showing count distribution\nof item-weighted scores , with unassisted scores (blue) and unassisted scores\n(orange) overlaid.\n(B) Box plot displaying median, quartiles, and range of\nscores for both conditions. (C) Count distribution by rounded score values on\n0-5 scale, comparing frequencies between AI and non-AI groups. (D) Kernel\ndensity estimation curves showing smoothed probability distributions for both\nconditions.\n3.2.4\nOther observations and examples\nAnalysis of individual protocol abstractions revealed three distinct error patterns\nthat contributed to reduced performance in unassisted tasks:\nEmpty fields\n: In unassisted cases, participants frequently left complex fields\nblank when information was challenging to locate within lengthy protocols. For\nexample, when extracting safety monitoring plans, unassisted participants often\nprovided no response, while AI-assisted participants extracted comprehensive\ndetails:\n# unassisted\n{\"safety_monitoring_plan\" = {}}\n# ai assisted, on same protocol\n{\"safety_monitoring_plan\" = {\n\"The safety of inhaled treprostinil will be evaluated by\ncomparing parameters such as AEs, oxygenation, pulmonary function,\n24\n"}, {"page": 25, "text": "Table 6: Response distribution for user satisfaction and preference survey (N =\n12).\nQuestion\nResponse distribution\nHow helpful did you find the AI assistance?\nExtremely helpful (4)\nVery helpful (4)\nSomewhat helpful (3)\nNot very helpful (1)\nNot at all helpful (0)\nHow satisfied are you with AI assistance\ncompared to manual extraction?\nMuch more satisfied (8)\nSlightly more satisfied (3)\nAbout the same (1)\nSlightly less satisfied (0)\nMuch less satisfied (0)\nHow likely would you be to choose AI as-\nsistance again for similar tasks?\nVery likely (5)\nLikely (5)\nNeutral (2)\nUnlikely (0)\nVery unlikely (0)\nDid AI assistance reduce the amount of\nthinking or decision-making required?\nYes, significantly (5)\nYes, somewhat (6)\nNot really (0)\nNot at all (1)\nIt made it more difficult (0)\nDid AI assistance make the task easier or\nharder compared to manual extraction?\nMuch easier (8)\nSlightly easier (3)\nAbout the same (1)\nSlightly harder (0)\nMuch harder (0)\nclinical laboratory parameters, vital signs, 12-Lead ECG,\nhospitalizations due to cardiopulmonary indications, and\nexacerbations of underlying lung disease between the two treatment\ngroups. All AEs will be assigned a MedDRA preferred term and system\norgan class for reporting purposes. The Data Monitoring Committee\n(DMC) will oversee safety monitoring, meeting regularly as outlined\nin the DMC charter.\",\n\"Safety monitoring procedures and frequency\": \"Interim analyses\nfor safety data will be performed at the request of the DMC. The\nDMC will meet regularly to monitor the safety of the study, with\nmeetings occurring as outlined in the DMC charter. Study monitors\nwill periodically contact the site and conduct on-site visits to\nconfirm ethical treatment of subjects, assess study progress,\nreview data collected, conduct source document verification, verify\n25\n"}, {"page": 26, "text": "drug accountability, and identify any issues requiring resolution.\"\n}}\nAnother example of this phenomenon:\n# unassisted:\n{\"product_name\": \"Pembrolizumab\", \"dose\": \"200 mg\",\n\"administration\": \"every 3 weeks\", \"restrictions\": \"\"}\n# restriction is clearly missed here, however on ai assisted:\n{\"product_name\": \"Pembrolizumab\", \"dose\": \"200 mg\",\n\"administration\": \"IV infusion\", \"restrictions\": \"Stop infusion for\nGrade 3 reactions; no subsequent dosing if severe reactions occur.\"}\nNon-informative responses\n: Unassisted tasks sometimes provided generic\nresponses that failed to capture specific required information. In one case in-\nvolving drug storage requirements, the response repeated identical boilerplate\nlanguage (“Clinical supplies must be stored in a secure, limited-access location\nunder the storage conditions specified on the label”) across three distinct fields\n(temperature, environmental conditions, and storage location). The AI-assisted\nversion provided specific details: temperature requirements (2–8°C), environmen-\ntal conditions (protect from light, do not shake), and storage specifications to\nthe participant helping them to find the right answer.\n# unassisted:\n{\"product_name\": \"Pembrolizumab\",\n\"temperature_requirements\": \"Clinical supplies must be stored in a\nsecure, limited-access location under the storage conditions\nspecified on the label.\",\n\"environmental_conditions\": \"Clinical supplies must be stored in a\nsecure, limited-access location under the storage conditions\nspecified on the label.\",\n\"storage_location\": \"Clinical supplies must be stored in a secure,\nlimited-access location under the storage conditions specified on\nthe label.\"}\n# AI-assisted, product code is added, specific temperature and\nstorage location is added:\n{\"product_name\": \"Pembrolizumab (MK-3475)\",\n\"temperature_requirements\": \"2-8 C (36-46 F)\",\n\"environmental_conditions\": \"Store in original box to protect from\nlight; do not shake\",\n\"storage_location\": \"Secure, limited-access location\"\n}\nIncorrect Values:\n: Sometimes direct transcription errors occurred in unas-\nsisted tasks, including incorrect product names (TPI 28 vs TPI 287) and trun-\ncated NCT identifiers (NCT0218782 vs NCT02187822), which could have signifi-\ncant implications for protocol accuracy.\n26\n"}, {"page": 27, "text": "# unassisted:\n{\"treatment_level_information\": [{\"product\\_name\": \"TPI 28\", ...}]}\n# AI-assisted:\n{\"treatment_level_information\": [{\"product\\_name\": \"TPI 287\", ...}]}\n...\n# unassisted:\n{\"study_nct_id\": \"NCT0218782\"}\n# AI-assisted:\n{\"study_nct_id\": \"NCT02187822\"}\n4\nDiscussion\nThis work provides evidence that clinical-research-specific RAG methods, paired\nwith a SoE-focused vision approach, can extract protocol information efficiently\nand with high quality across diverse therapeutic areas and document styles.\nIn direct comparisons on 23 publicly available protocols, our specialized RAG\nreliably matched or exceeded standalone LLM prompting, with the largest gains\nobserved in categories where relevant details are scattered across multiple sections\n(adverse events, interventions, site requirements) and in tabular SoE content.\nIn our controlled experiment with CRCs, AI assistance did not only signifi-\ncantly reduce time-to-completion, but increased accuracy, while being largely\nperceived as helpful and lowering cognitive load.\nThese findings suggest immediate practical value for clinical operations. Stan-\ndardized, machine-readable abstractions of protocol content enable downstream\nautomation and decision support for feasibility, start-up, site activation, and\nongoing monitoring. By attaching transparent, chunk-level references to each\nextracted element, the approach supports auditability and human review, fa-\ncilitating traceability and reproducibility. As institutions modernize clinical\nresearch infrastructure, similar upstream protocol intelligence could serve as a\nhub from which study information is propagated across systems (e.g. CTMS,\nfeasibility tools, EDC), reducing manual effort and variability.\nImportant limitations remain, however, among them:\n• our dataset, while spanning multiple therapeutic areas, is modest and\nfocuses on public protocols with heterogeneous formatting and variable\nquality,\n• our evaluation methodology, leveraging hybrid human-AI ground truth\ngeneration and LLM-as-judge with human quality control, prioritizes scal-\nability but may inherit model biases. Real-world validation against expert\nadjudication (e.g. data edits and validation from clinical experts) better\nreflecting real-world conditions and scale in a clinical context is required\nto corroborate true performance,\n• results may not fully generalize to certain trials, complex adaptive de-\nsigns, or highly specialized domains without further tailored schemas and\n27\n"}, {"page": 28, "text": "prompts.\nFuture work should include real-world pilot deployments at clinical sites, data\nelement confidence/risk scoring (more generally, robust performance and safety\nmonitoring), expansion of semi-structured schemas to cover further operational\nneeds, deeper alignment with emerging data standards (e.g. CDISC USDM),\nintegration with existing site systems (e.g., CTMS, IRB platforms), protocol\namendment handling, RAG pipeline hyperparameter and prompt tuning.\nGovernance and safety are essential for clinical site adoption. For that purpose,\nour foundational designs and methods ensure privacy preservation across all\ndata handling stages, maintain comprehensive audit trails and compliance with\nhealthcare standards, involves a complementary process providing continuous\nautomated error monitoring and inspection in addition to embedded human\napproval at key checkpoints to mitigate hallucinations and propagation of AI-\ngenerated errors. Clear user roles and responsibilities provided through proper\ntraining (e.g. what is expected from the automation and expert judgment) will\nalso help teams adopt AI assistance safely and confidently.\n5\nConclusion\nThe quantified improvements measured in this work (higher extraction accuracy,\nreduced completion times and strong user preference) support the practical\nvalue of integrating clinical-trial-specific RAG-based protocol extraction into\nresearch workflows. By combining targeted retrieval, transparent referencing,\nand structured outputs with expert review, research teams can scale protocol\ncomprehension and operational initiation of trials while maintaining rigor and\naccountability. With responsible deployment and continued validation, these\nmethods show promise in helping to address the growing complexity of clinical\ntrials and unlocking further efficiencies across the trial lifecycle.\nReferences\n[1] L. M. Friedman, C. D. Furberg, D. L. DeMets, Fundamentals of Clinical\nTrials, 5th Edition, Springer, 2015.\n[2] C. T. Jones, P. M. Jester, M. Fitz-Gerald, Issues in research management:\nProtocol challenges in the era of complexity, Research Practitioner 14 (3)\n(2013) 122–127.\n[3] F. Varse, L. Janani, Y. Moradi, M. Solaymani-Dodaran, H. R. Baradaran,\nS. Rimaz, Challenges in the design, conduction, analysis, and reporting of\nrandomized clinical trial studies: A systematic review, Medical Journal of\nthe Islamic Republic of Iran 33 (1) (2019) 37.\n[4] K. A. Getz, et al., The impact of protocol amendments on clinical trial\nperformance and cost, Therapeutic Innovation & Regulatory Science 52 (5)\n(2018) 577–586.\n28\n"}, {"page": 29, "text": "[5] D. Gryaznov, et al., Reporting quality of clinical trial protocols: a repeated\ncross-sectional study about the adherence to spirit recommendations in\nswitzerland, canada and germany (aspire-scage), BMJ Open 12 (2022)\ne053417.\n[6] K. A. Getz, et al., New benchmarks on protocol amendment practices, trends\nand their impact on clinical trial performance, Therapeutic Innovation &\nRegulatory Science 58 (3) (2024) 539–548.\n[7] S. Datta, et al., Autocriteria: a generalizable clinical trial eligibility crite-\nria extraction system powered by large language models, Journal of the\nAmerican Medical Informatics Association 31 (2) (2024) 375–385.\n[8] M. Kramer, Extraction of schedules of activities tables from clinical trial\nprotocols, https://github.com/markkramerus/publications/blob/ma\nin/2-Extraction%20of%20SoA%20Tables%20from%20PDFs.pdf (2025).\n[9] M. Kargren, J. April, G. Clark, J. Mackinnon, A. Nathoo, E. Theron,\nUnlocking new efficiencies: How structured content authoring is streamlin-\ning the production of clinical documents for the pharmaceutical industry,\nMedical Writing 32 (3) (2023) 32–37.\n[10] T. Georgieff, Navigating toward a digital clinical trial protocol, Applied\nClinical Trials 32 (12) (2023).\n[11] A. Vadakin, R. D. Kush, Cdisc standards and innovations, Clinical Evalua-\ntion 40 (Suppl. 31) (2012) 217–228.\n[12] M. Maleki, S. A. Ghahari, Clinical trials protocol authoring using llms,\narXiv, https://arxiv.org/html/2404.05044v2 (2024).\n[13] R. Babaeipour, F. Charest, M. Wright, Ai-assisted protocol complexity\nestimation for improved clinical trial workflowsIn preparation.\n[14] X. Liu, et al., Clinical trial information extraction with bert, in: IEEE\n9th International Conference on Healthcare Informatics (ICHI), 2021, pp.\n505–506.\n[15] Snorkel AI, Augmenting the clinical trial design information extraction,\nBlog, https://snorkel.ai/blog/augmenting-the-clinical-trial-d\nesign-information-extraction/ (2022).\n[16] I. C. Wiest, et al., A software pipeline for medical information extraction\nwith large language models, open source and suitable for oncology, npj\nPrecision Oncology 9 (2025) 313.\n[17] P. Hosseini, I. Castro, I. Ghinassi, M. Purver, Efficient solutions for an\nintriguing failure of llms: Long context window does not mean llms can\nanalyze long sequences flawlessly, arXiv (2024). arXiv:2408.01866, doi:\n10.48550/arXiv.2408.01866.\nURL https://doi.org/10.48550/arXiv.2408.01866\n29\n"}, {"page": 30, "text": "[18] N. F. Liu, et al., Lost in the middle: How language models use long contexts,\nTransactions of the Association for Computational Linguistics 12 (2024)\n157–173.\n[19] P. Lewis, et al., Retrieval-augmented generation for knowledge-intensive nlp\ntasks, in: Advances in Neural Information Processing Systems (NeurIPS),\n2020, pp. 9459–9474.\n[20] P. Rajpurkar, et al., Ai in health and medicine, Nature Medicine 28 (1)\n(2022) 31–38.\n[21] Journal of Society for Clinical Data Management, Representing clinical study\nschedule of activities as fhir resources: Required characteristic attributes,\nhttps://www.jscdm.org/article/id/266/ (2025).\n[22] D. Ferrés, H. Saggion, F. Ronzano, À. Bravo, Pdfdigest: an adaptable\nlayout-aware pdf-to-xml textual content extractor for scientific articles, in:\nProceedings of the Eleventh International Conference on Language Resources\nand Evaluation (LREC-2018), Miyazaki, Japan, 2018, pp. 1896–1901.\n[23] X. Zhong, E. ShafieiBavani, A. Jimeno Yepes, Image-based table recognition:\nData, model, and evaluation, in: Computer Vision – ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart XXI, Springer-Verlag, Berlin, Heidelberg, 2020, pp. 564–580. doi:\n10.1007/978-3-030-58589-1_34.\nURL https://doi.org/10.1007/978-3-030-58589-1_34\n[24] J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, et al., A survey on llm-as-\na-judge, arXiv (2024). arXiv:2411.15594.\nURL https://arxiv.org/abs/2411.15594\n[25] E. Croxford, Y. Gao, E. First, N. Pellegrino, M. Schnier, J. Caskey, et al.,\nAutomating evaluation of ai text generation in healthcare with a large\nlanguage model (llm)-as-a-judge, medRxiv [Preprint] (2025). doi:10.110\n1/2025.04.22.25326219.\nURL https://pmc.ncbi.nlm.nih.gov/articles/PMC12045442/\n[26] M. Yuan, J. Chen, Z. Xing, G. Mohammadi, A. Quigley, A case study of\nscalable content annotation using multi-llm consensus and human review,\narXiv (2025). arXiv:2503.17620.\nURL https://arxiv.org/pdf/2503.17620\n[27] X. Wang, H. Kim, S. Rahman, K. Mitra, Z. Miao, Human-llm collaborative\nannotation through effective verification of llm labels, in: Proceedings of\nthe 2024 CHI Conference on Human Factors in Computing Systems (CHI\n’24), Association for Computing Machinery, New York, NY, USA, 2024, pp.\n1–21, article 303, 1–21.\n30\n"}, {"page": 31, "text": "[28] P. Thomas, S. Spielman, N. Craswell, B. Mitra, Large language models\ncan accurately predict searcher preferences, in: Proceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR ’24, Association for Computing Machinery,\nNew York, NY, USA, 2024, p. 1930–1940. doi:10.1145/3626772.3657707.\nURL https://doi.org/10.1145/3626772.3657707\n[29] National Library of Medicine, Clinicaltrials.gov, https://clinicaltrials\n.gov/, n.d.\n[30] OpenAI, Gpt-4o, july 2025 (2025).\n[31] Cohere, Cohere embeddings, july 2025 (2025).\n[32] H. Chase, et al., Langchain: Building applications with llms through com-\nposability, GitHub, https://github.com/langchain-ai/langchain\n(2022).\n[33] X. Wang, Z. Wang, X. Gao, F. Zhang, Y. Wu, Z. Xu, T. Shi, Z. Wang, S. Li,\nQ. Qian, R. Yin, C. Lv, X. Zheng, X. Huang, Searching for best practices\nin retrieval-augmented generation, in: Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing (EMNLP), 2024, pp.\n17753–17788.\nURL https://aclanthology.org/2024.emnlp-main.981/\n[34] S. Li, L. Stenzel, C. Eickhoff, S. A. Bahrainian, Enhancing retrieval-\naugmented generation: A study of best practices, in: Proceedings of the\n31st International Conference on Computational Linguistics (COLING),\n2025, pp. 6682–6698.\nURL https://aclanthology.org/2025.coling-main.449/\n[35] B. Smock, R. Pesala, R. Abraham, Pubtables-1m: Towards comprehen-\nsive table extraction from unstructured documents, in: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022, pp. 4634–4642.\n[36] L. M. Schulze Buschoff, E. Akata, M. Bethge, et al., Visual cognition in\nmultimodal large language models, Nature Machine Intelligence 7 (2025)\n96–106. doi:10.1038/s42256-024-00963-y.\n[37] L. Aroyo, C. Welty, Truth is a lie: Crowd truth and the seven myths of\nhuman annotation, AI Magazine 36 (1) (2015) 15–24. doi:10.1609/aimag.\nv36i1.2564.\nURL https://ojs.aaai.org/aimagazine/index.php/aimagazine/art\nicle/view/2564\n[38] F. Yu, N. Seedat, D. Herrmannova, F. Schilder, J. R. Schwarz, Beyond\npointwise scores: Decomposed criteria-based evaluation of llm responses,\nin: Proceedings of the 2025 Conference on Empirical Methods in Natural\n31\n"}, {"page": 32, "text": "Language Processing: Industry Track, 2025, pp. 1931–1954. doi:10.18653\n/v1/2025.emnlp-industry.136.\nURL https://aclanthology.org/2025.emnlp-industry.136/\n32\n"}, {"page": 33, "text": "33\n"}, {"page": 34, "text": "A\nAppendices\nA.1\nSupplementary tables and figures\nTable 7: Element weights by data element category for weighted evaluation\nscoring. Weights within each category sum to 1.0.\nCategory\nElement\nWeight\ngen\nStudy NCT ID\n0.15\ngen\nStudy Title\n0.08\ngen\nProtocol ID, Version, Date\n0.07\ngen\nSponsor Name\n0.05\ngen\nPhase\n0.10\ngen\nTherapeutic Area\n0.01\ngen\nDisease/Condition\n0.03\ngen\nAllocation\n0.02\ngen\nMasking\n0.02\ngen\nTarget Enrollment\n0.00\ngen\nCountries and Sites\n0.00\ngen\nTimeline (Start/End/Duration)\n0.00\ngen\nPrimary Objectives\n0.25\ngen\nSecondary Objectives\n0.02\ngen\nPrimary/Secondary Endpoints\n0.20\nie\nInclusion Criteria\n0.50\nie\nExclusion Criteria\n0.50\nie\nDemographics\n0.00\nie\nWashout Period\n0.00\nae\nAE Definition\n0.15\nae\nSAE Definition\n0.20\nae\nSeverity Grading\n0.05\nae\nAE Relationship Assessment\n0.02\nae\nReporting Timeframes\n0.12\nae\nData Collection Requirements\n0.08\nae\nReporting Contacts\n0.02\nae\nSafety Monitoring Plan\n0.08\nae\nDiscontinuation Criteria\n0.12\nae\nEmergency Procedures\n0.05\nae\nExpected AEs\n0.04\nae\nPotential Risks\n0.03\nae\nConcomitant Medication Restrictions\n0.03\nae\nSpecial Population Considerations\n0.01\ninter\nIntervention Arms\n0.50\ninter\nTreatment-Level Information\n0.50\nsite\nEquipment\n0.20\nsite\nCertifications\n0.10\nsite\nSample Handling\n0.35\nsite\nIP Storage\n0.35\nsoe\nSchedule of Events\n1.00\n34\n"}, {"page": 35, "text": "Listing 11: Example comparison showing subtle data variation.\n# AI output:\n\"exclusion\": [\n\"The subject has received any investigational compound within 30\ndays prior to screening.\",\n\"OR\",\n\"The subject has received allergen immunotherapy or SLIT in a\nprevious clinical study or as a therapeutic agent within the past\ntwo years.\",\n# p.13\n]\n# annotation:\n\"exclusion\": [\n\"The subject has received any investigational compound within 30\ndays prior to screening.\",\n\"OR\",\n\"The subject has received multi antigen immunotherapy or SLIT in a\nprevious clinical study or as a therapeutic agent within the past\ntwo years.\",\n# p.26\n]\nA.2\nAuthor declarations\nAll procedures involving human participants were conducted in accordance with\nnational research ethical standards, privacy rights of human subjects have been\nobserved and informed consent was obtained for experimentation with human\nsubjects.\nDuring the preparation of this work the authors used Google Gemini En-\nterprise and GitHub Copilot Enterprise in order to assist linguistic review and\nenhancement. After using this tool/service, the authors reviewed and edited the\ncontent as needed and take full responsibility for the content of the published\narticle.\nAuthors report financial support provided by Banting Health AI Inc. as well\nas an employment relationship with Banting Health AI Inc.\n35\n"}]}